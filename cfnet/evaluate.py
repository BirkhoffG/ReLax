# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_evaluate.ipynb (unless otherwise specified).

__all__ = ['CFExplanationResults', 'generate_cf_results', 'generate_cf_results_local_exp', 'generate_cf_results_cfnet',
           'compute_predictive_acc', 'compute_validity', 'compute_proximity', 'get_runtime', 'metrics2fn',
           'evaluate_cfs', 'DEFAULT_METRICS', 'benchmark_cfs']

# Cell
from .import_essentials import *
from .train import train_model, TensorboardLogger
from .datasets import TabularDataModule
from .interfaces import BaseCFExplanationModule, LocalCFExplanationModule
from copy import deepcopy

# Cell
@dataclass
class CFExplanationResults:
    cf_name: str        # cf method's name
    dataset_name: str   # dataset name
    X: jnp.DeviceArray  # input
    y: jnp.DeviceArray  # label
    cfs: jnp.DeviceArray # generated cf explanation of `X`
    total_time: float   # total runtime
    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray] # predict function

# Cell
def generate_cf_results(
    cf_module: BaseCFExplanationModule,
    dm: TabularDataModule,
    pred_fn: Optional[Callable[[jnp.DeviceArray], jnp.DeviceArray]] = None,
    params: Optional[hk.Params] = None, # params of `cf_module`
    rng_key: Optional[random.PRNGKey] = None
) -> CFExplanationResults:
    # validate arguments
    if (pred_fn is None) and (params is None) and (rng_key is None):
        raise ValueError("A valid `pred_fn: Callable[jnp.DeviceArray], jnp.DeviceArray]` or `params: hk.Params` needs to be passed.")
    # prepare
    X, y = dm.test_dataset[:]
    cf_module.update_cat_info(dm)
    # generate cfs
    current_time = time.time()
    if pred_fn:
        cfs = cf_module.generate_cfs(X, pred_fn)
    else:
        cfs = cf_module.generate_cfs(X, params, rng_key)
        pred_fn = lambda x: cf_module.predict(deepcopy(params), rng_key, x)
    total_time = time.time() - current_time

    return CFExplanationResults(
        X=X, y=y, cfs=cfs, total_time=total_time,
        pred_fn=pred_fn,
        cf_name=cf_module.name, dataset_name=dm.data_name
    )

# Cell
def generate_cf_results_local_exp(
    cf_module: LocalCFExplanationModule,
    dm: TabularDataModule,
    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]
) -> CFExplanationResults:
    return generate_cf_results(cf_module, dm, pred_fn=pred_fn)

def generate_cf_results_cfnet(
    cf_module: LocalCFExplanationModule,
    dm: TabularDataModule,
    params: Optional[hk.Params] = None, # params of `cf_module`
    rng_key: Optional[random.PRNGKey] = None
) -> CFExplanationResults:
    return generate_cf_results(cf_module, dm, params=params, rng_key=rng_key)

# Cell
def _compute_acc(x: jnp.ndarray, y: jnp.ndarray):
    return jnp.sum(x == y) / len(x)

# Cell
def compute_predictive_acc(cf_results: CFExplanationResults):
    pred_fn = cf_results.pred_fn
    y_pred = pred_fn(cf_results.X).reshape(-1, 1)
    label = cf_results.y.reshape(-1, 1)
    return _compute_acc(jnp.round(y_pred), label).item()

def compute_validity(cf_results: CFExplanationResults):
    pred_fn = cf_results.pred_fn
    y_pred = pred_fn(cf_results.X).reshape(-1, 1).round()
    y_prime = 1 - y_pred
    cf_y = pred_fn(cf_results.cfs).reshape(-1, 1).round()
    return _compute_acc(y_prime, cf_y).item()

def compute_proximity(cf_results: CFExplanationResults):
    return jnp.abs(cf_results.X - cf_results.cfs).sum(axis=1).mean().item()

def get_runtime(cf_results: CFExplanationResults):
    return cf_results.total_time

# Cell
metrics2fn = {
    "acc": compute_predictive_acc,
    "validity": compute_validity,
    "proximity": compute_proximity,
    "runtime": get_runtime
}

# Cell
DEFAULT_METRICS = ['acc', 'validity', 'proximity']

def evaluate_cfs(cf_results: CFExplanationResults,
                 metrics: Optional[List[str]] = None,
                 return_dict: bool = True,
                 return_df: bool = False):
    cf_name = cf_results.cf_name
    result_dict = {
        cf_name: dict()
    }
    if metrics is None:
        metrics = DEFAULT_METRICS

    for metric in metrics:
        result_dict[cf_name][metric] = metrics2fn[metric](cf_results)
    result_df = pd.DataFrame.from_dict(result_dict, orient='index')
    if return_dict and return_df:
        return (result_dict, result_df)
    elif return_dict or return_df:
        return result_df if return_df else result_dict

# Cell
def benchmark_cfs(cf_results_list: Iterable[CFExplanationResults],
                  metrics: Optional[List[str]] = None):
    dfs = [
        evaluate_cfs(cf_results=cf_results, metrics=metrics, return_dict=False, return_df=True)
            for cf_results in cf_results_list
    ]
    return pd.concat(dfs)