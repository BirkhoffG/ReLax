# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_methods.vanilla.ipynb (unless otherwise specified).

__all__ = ['binary_cross_entropy', 'VanillaCFConfig', 'VanillaCF']

# Cell
from ..import_essentials import *
from ..interfaces import BaseCFExplanationModule, LocalCFExplanationModule
from ..datasets import TabularDataModule
from ..training_module import grad_update, cat_normalize
from ..utils import check_cat_info, validate_configs


# Cell
def binary_cross_entropy(y_pred: chex.Array, y: chex.Array) -> chex.Array:
    return -(y * jnp.log(y_pred) + (1 - y) * jnp.log(1 - y_pred))

# Cell
class VanillaCFConfig(BaseParser):
    n_steps: int = 1000
    lr: float = 0.001


# Cell
class VanillaCF(LocalCFExplanationModule):
    name = "VanillaCF"

    def __init__(self,
                 pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],
                 configs: Union[Dict[str, Any], VanillaCFConfig],
                 data_module: Optional[TabularDataModule] = None):
        self.pred_fn = pred_fn
        self.configs = validate_configs(configs, VanillaCFConfig)
        if data_module:
            self.update_cat_info(data_module)

    def _loss_fn_1(self,
                   cf_y: jnp.ndarray,
                   y_prime: jnp.ndarray) -> jnp.ndarray:
        return jnp.mean(binary_cross_entropy(y_pred=cf_y, y=y_prime))

    def _loss_fn_2(self,
                   x: jnp.ndarray,
                   cf: jnp.ndarray,) -> jnp.ndarray:
        return jnp.mean(optax.l2_loss(cf, x))

    def generate_cf(self,
                    x: jnp.ndarray) -> jnp.ndarray:

        pred_fn = self.pred_fn

        def loss_fn(cf: jnp.ndarray,
                    x: jnp.ndarray,
                    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]) -> jnp.ndarray:
            y_pred = pred_fn(x)
            y_prime = 1. - y_pred
            cf_y = pred_fn(cf)
            return self._loss_fn_1(cf_y, y_prime) + 0.5 * self._loss_fn_2(x, cf)

        @jax.jit
        def gen_cf_step(x: jnp.ndarray, cf: jnp.ndarray, opt_state: optax.OptState):
            cf_grads = jax.grad(loss_fn)(cf, x, pred_fn)
            cf, opt_state = grad_update(cf_grads, cf, opt_state, opt)
            cf = cat_normalize(
                cf, cat_arrays=self.cat_arrays, cat_idx=self.cat_idx, hard=False)
            return cf, opt_state

        x = x.reshape(1, -1)
        cf = jnp.array(x, copy=True)
        opt = optax.rmsprop(self.configs.lr)
        opt_state = opt.init(cf)
        for _ in tqdm(range(self.configs.n_steps)):
            cf, opt_state = gen_cf_step(x, cf, opt_state)

        cf = cat_normalize(
            cf, cat_arrays=self.cat_arrays, cat_idx=self.cat_idx, hard=True)
        return cf.reshape(-1)

    @check_cat_info
    def generate_cfs(self,
                      X: jnp.array,
                      is_parallel: bool = False) -> jnp.ndarray:
        def _generate_cf(x: jnp.ndarray) -> jnp.ndarray:
            return self.generate_cf(x)
        return jax.vmap(_generate_cf)(X) if not is_parallel else jax.pmap(_generate_cf)(X)