# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_nets.ipynb (unless otherwise specified).

__all__ = ['DenseBlock', 'MLP', 'PredictiveMLP', 'PredictivConvNet', 'CounterNetMLP', 'CounterNetConv']

# Cell
from .import_essentials import *
from .utils import validate_configs, sigmoid

# Cell
class DenseBlock(hk.Module):
    def __init__(self,
                output_size: int,
                dropout_rate: float = 0.3,
                name: Optional[str] = None):
        super().__init__(name=name)
        self.output_size = output_size
        self.dropout_rate = dropout_rate

    def __call__(self,
                x: jnp.ndarray,
                is_training: bool = True) -> jnp.ndarray:
        dropout_rate = self.dropout_rate if is_training else 0.0
        # he_uniform
        w_init = hk.initializers.VarianceScaling(2.0, 'fan_in', 'uniform')
        x = hk.Linear(self.output_size, w_init=w_init)(x)
        x = jax.nn.leaky_relu(x)
        x = hk.dropout(hk.next_rng_key(), dropout_rate, x)
        return x

# Cell
class MLP(hk.Module):
    def __init__(self,
                sizes: List[int],
                dropout_rate: float = 0.3,
                name: Optional[str] = None):
        super().__init__(name=name)
        self.sizes = sizes
        self.dropout_rate = dropout_rate

    def __call__(self,
                x: jnp.ndarray,
                is_training: bool = True) -> jnp.ndarray:
        for size in self.sizes:
            x = DenseBlock(size, self.dropout_rate)(x, is_training)
        return x

# Internal Cell
class PredictiveMLPConfigs(BaseParser):
    sizes: List[int]
    dropout_rate: float = 0.3

# Cell
class PredictiveMLP(hk.Module):
    def __init__(
        self,
        m_config: Dict[str, Any],
        name: Optional[str] = None
    ):
        super().__init__(name=name)
        self.configs = validate_configs(m_config, PredictiveMLPConfigs) #PredictiveModelConfigs(**m_config)

    def __call__(
        self,
        x: jnp.ndarray,
        is_training: bool = True
    ) -> jnp.ndarray:
        x = MLP(sizes=self.configs.sizes, dropout_rate=self.configs.dropout_rate)(x, is_training)
        x = hk.Linear(1)(x)
        x = sigmoid(x)
        return x

# Cell
class PredictivConvNet(hk.Module):
    def __init__(
        self, name = None
    ):
        super().__init__(name=name)

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        x = hk.Sequential([
            hk.Conv2D(output_channels=32, kernel_shape=(3, 3), padding="SAME"),
            jax.nn.leaky_relu,
            hk.Conv2D(output_channels=64, kernel_shape=(3, 3), padding="SAME"),
            jax.nn.leaky_relu,
            hk.Flatten(),
            hk.Linear(256),
            jax.nn.leaky_relu,
            hk.Linear(1),
        ])(x)
        return sigmoid(x)


# Internal Cell
class CounterNetMLPConfigs(BaseParser):
    enc_sizes: List[int]
    dec_sizes: List[int]
    exp_sizes: List[int]
    dropout_rate: float = 0.3

# Cell
class CounterNetMLP(hk.Module):

    def __init__(self,
                m_config: Dict[str, Any],
                name: Optional[str] = None):
        super().__init__(name=name)
        self.configs = validate_configs(m_config, CounterNetMLPConfigs)

    def __call__(self,
                x: jnp.ndarray,
                is_training: bool = True) -> jnp.ndarray:
        input_shape = x.shape[-1]
        # encoder
        z = MLP(self.configs.enc_sizes, self.configs.dropout_rate, name="Encoder")(x, is_training)

        # prediction
        pred = MLP(self.configs.dec_sizes, self.configs.dropout_rate, name="Predictor")(z, is_training)
        y_hat = hk.Linear(1, name='Predictor')(pred)
        y_hat = sigmoid(y_hat)

        # explain
        z_exp = jnp.concatenate((z, pred), axis=-1)
        cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name="Explainer")(z_exp, is_training)
        cf = hk.Linear(input_shape, name='Explainer')(cf)
        return y_hat, cf

# Internal Cell
class ConvExplainer(hk.Module):
    def __init__(
        self,
        flaten_shape,
        z_shape_3d,
        name: Optional[str] = None
    ):
        self.flatten_shape = flaten_shape
        self.z_shape_3d = z_shape_3d
        super().__init__(name=name)

    def __call__(self, x):
        x = hk.Linear(self.flatten_shape[-1], name="Explainer")(x)
        x = x.reshape(self.z_shape_3d)
        x = jax.nn.leaky_relu(x)
        x = hk.Conv2DTranspose(output_channels=4, kernel_shape=(3, 3), padding='SAME')(x)
        x = jax.nn.leaky_relu(x)
        x = hk.Conv2DTranspose(output_channels=1, kernel_shape=(3, 3), padding='SAME')(x)
        x = jnp.tanh(x)
        return x

# Cell
class CounterNetConv(hk.Module):
    def __init__(
        self,
        name: Optional[str] = None
    ):
        super().__init__(name=name)

    def __call__(self,
                x: jnp.ndarray,
                is_training: bool = True) -> jnp.ndarray:
        x = jnp.expand_dims(x, axis=-1)
        # encoder
        z = hk.Sequential([
            hk.Conv2D(output_channels=4, kernel_shape=(3, 3), padding="SAME"),
            jax.nn.leaky_relu,
            hk.Conv2D(output_channels=16, kernel_shape=(3, 3), padding="SAME"),
            jax.nn.leaky_relu,
        ], name='Encoder')(x)
        z_shape_3d = z.shape
        z = hk.Flatten()(z)
        z_shape_flattened = z.shape

        # prediction
        pred = hk.Sequential([
            hk.Linear(50),
            jax.nn.leaky_relu,
        ], name='Predictor')(z)
        y_hat = hk.Linear(1, name='Predictor')(pred)
        y_hat = sigmoid(y_hat)

        # explain
        z_exp = jnp.concatenate((z, pred), axis=-1)
        # z_exp = hk.Linear(z_shape_flattened[-1], name="Explainer")(z_exp)
        # z_exp = z_exp.reshape(z_shape_3d)

        # cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name="Explainer")(z_exp)
        cf = ConvExplainer(z_shape_flattened, z_shape_3d, name='Explainer')(z_exp)
        return y_hat, cf.squeeze()