# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% ../nbs/01_data.ipynb 3
from __future__ import annotations
from .utils import load_json, validate_configs, cat_normalize, get_config
from .base import *
from .data_utils import *
import jax
from jax import numpy as jnp, random as jrand, lax, Array
import pandas as pd
import numpy as np
# from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
# from sklearn.base import TransformerMixin
# from sklearn.utils.validation import check_is_fitted, NotFittedError
from urllib.request import urlretrieve
# from relax.data.loader import Dataset, ArrayDataset, DataLoader, DataloaderBackends
from pydantic.fields import ModelField, Field
from typing import List, Dict, Union, Optional, Tuple, Callable, Any, Iterable

# %% auto 0
__all__ = ['BaseDataModule', 'DataModuleConfig', 'DataModule', 'load_data']

# %% ../nbs/01_data.ipynb 6
class BaseDataModule(BaseModule):
    """DataModule Interface"""

    def prepare(self, *args, **kwargs):
        """Prepare data for training"""
        raise NotImplementedError
        
    def apply_constraints(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> Array:
        raise NotImplementedError
    
    def apply_regularization(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> float:
        raise NotImplementedError

# %% ../nbs/01_data.ipynb 8
class DataModuleConfig(BaseConfig):
    """Configurator of `TabularDataModule`."""

    data_dir: str = Field(description="The directory of dataset.")
    data_name: str = Field(description="The name of `DataModule`.")
    continous_cols: List[str] = Field([], description="Continuous features/columns in the data.")
    discret_cols: List[str] = Field([], description="Categorical features/columns in the data.")
    imutable_cols: List[str] = Field([], description="Immutable features/columns in the data.")
    continuous_transformation: str = Field('minmax', description="Transformation for continuous features.")
    discret_transformation: str = Field('ohe', description="Transformation for categorical features.")
    sample_frac: Optional[float] = Field(
        None, description="Sample fraction of the data. Default to use the entire data.", ge=0., le=1.0
    )
    train_indices: List[int] = Field([], description="Indices of training data.")
    test_indices: List[int] = Field([], description="Indices of testing data.")
    
    # normalizer: Optional[str] = Field(
    #     default_factory=lambda: MinMaxScaler(),
    #     description="Sklearn scalar for continuous features. Can be unfitted, fitted, or None. "
    #     "If not fitted, the `TabularDataModule` will fit using the training data. If fitted, no fitting will be applied. "
    #     "If `None`, no transformation will be applied. Default to `MinMaxScaler()`."
    # )
    # encoder: Optional[str] = Field(
    #     default_factory=lambda: OneHotEncoder(sparse=False),
    #     description="Fitted encoder for categorical features. Can be unfitted, fitted, or None. "
    #     "If not fitted, the `TabularDataModule` will fit using the training data. If fitted, no fitting will be applied. "
    #     "If `None`, no transformation will be applied. Default to `OneHotEncoder(sparse=False)`."
    # )

    def shuffle(self, data: Array, test_size: float, seed: int = None):
        """Shuffle data with a seed."""
        if seed is None:
            seed = get_config().global_seed
        key = jrand.PRNGKey(seed)
        total_length = data.shape[0]
        train_length = int((1 - test_size) * total_length)
        self.train_indices = jrand.permutation(key, total_length)[:train_length].tolist()
        self.test_indices = jrand.permutation(key, total_length)[train_length:].tolist()

# %% ../nbs/01_data.ipynb 10
class DataModule(BaseDataModule):
    def __init__(
        self, 
        config, 
        data: pd.DataFrame = None,
        features: List[Feature] = None,
        label: Feature = None,
    ):
        config = validate_configs(config, DataModuleConfig)
        if data is None:
            data = pd.read_csv(config.data_dir)
        self._data = data
        features = self.convert_to_features(config, data, features)
        label = self.convert_to_label(config, data, label)
        self.prepare(features, label)
        config.shuffle(self.xs, test_size=0.25)
        super().__init__(config, name=config.data_name)

    def convert_to_features(
        self, 
        config: DataModuleConfig, 
        data: pd.DataFrame, 
        features: list[Feature] = None
    ):
        to_feature = lambda col, data, is_continuous: Feature(
                name=col, data=data[col].to_numpy().reshape(-1, 1),
                transformation=config.continuous_transformation if is_continuous else config.discret_transformation,
                is_immutable=col in config.imutable_cols
            )

        if features is not None:
            return features
        
        cont_features = [to_feature(col, data, True) for col in config.continous_cols]
        cat_features = [to_feature(col, data, False) for col in config.discret_cols]
        return cont_features + cat_features        
        
    def convert_to_label(self, config: DataModuleConfig, data: pd.DataFrame, label: Feature = None):
        if label is not None:
            return label
        
        label_col = data.columns[-1]
        return Feature(
            name=label_col, data=data[label_col].to_numpy().reshape(-1, 1),
            transformation='identity',
            is_immutable=label_col in config.imutable_cols
        )
        
    def prepare(self, features, label):
        if features is not None and label is not None:
            self._features = FeaturesList(features)
            self._label = label
        elif features is None:
            raise ValueError("Features cannot be None.")
        elif label is None:
            raise ValueError("Label cannot be None.")
    
    @property
    def data(self) -> pd.DataFrame:
        return self._data
    
    @property
    def xs(self) -> Array:
        return self._features.transformed_data
    
    @property
    def ys(self) -> Array:
        return self._label.transformed_data

    @property
    def dataset(self) -> Tuple[Array, Array]:
        return (self.xs, self.ys)
    
    def _get_data(self, indices):
        if isinstance(indices, list):
            indices = jnp.array(indices)
        return (self.xs[indices], self.ys[indices])
        
    def __getitem__(self, name: str):
        if name == 'train':
            return self._get_data(self.config.train_indices)
        elif name in ['valid', 'test']:
            return self._get_data(self.config.test_indices)
        else:
            raise ValueError(f"Unknown data name: {name}. Should be one of ['train', 'valid', 'test']")
        
    def apply_constraints(self, x: Array, cf: Array, hard: bool = False) -> Array:
        return self._features.apply_constraints(x, cf, hard)

# %% ../nbs/01_data.ipynb 13
DEFAULT_DATA_CONFIGS = {
    'adult': {
        'data' :'assets/adult/data.csv',
        'conf' :'assets/adult/configs.json',
        'model' :'assets/adult/model'
    },
    'heloc': {
        'data': 'assets/heloc/data.csv',
        'conf': 'assets/heloc/configs.json',
        'model' :'assets/heloc/model'
    },
    'oulad': {
        'data': 'assets/oulad/data.csv',
        'conf': 'assets/oulad/configs.json',
        'model' :'assets/oulad/model'
    },
    'credit': {
        'data': 'assets/credit/data.csv',
        'conf': 'assets/credit/configs.json',
        'model' :'assets/credit/model'
    },
    'cancer': {
        'data': 'assets/cancer/data.csv',
        'conf': 'assets/cancer/configs.json',
        'model' :'assets/cancer/model'
    },
    'student_performance': {
        'data': 'assets/student_performance/data.csv',
        'conf': 'assets/student_performance/configs.json',
        'model' :'assets/student_performance/model'
    },
    'titanic': {
        'data': 'assets/titanic/data.csv',
        'conf': 'assets/titanic/configs.json',
        'model' :'assets/titanic/model'
    },
    'german': {
        'data': 'assets/german/data.csv',
        'conf': 'assets/german/configs.json',
        'model' :'assets/german/model'
    },
    'spam': {
        'data': 'assets/spam/data.csv',
        'conf': 'assets/spam/configs.json',
        'model' :'assets/spam/model'
    },
    'ozone': {
        'data': 'assets/ozone/data.csv',
        'conf': 'assets/ozone/configs.json',
        'model' :'assets/ozone/model'
    },
    'qsar': {
        'data': 'assets/qsar/data.csv',
        'conf': 'assets/qsar/configs.json',
        'model' :'assets/qsar/model'
    },
    'bioresponse': {
        'data': 'assets/bioresponse/data.csv',
        'conf': 'assets/bioresponse/configs.json',
        'model' :'assets/bioresponse/model'
    },
    'churn': {
        'data': 'assets/churn/data.csv',
        'conf': 'assets/churn/configs.json',
        'model' :'assets/churn/model'
    },
    'road': {
        'data': 'assets/road/data.csv',
        'conf': 'assets/road/configs.json',
        'model' :'assets/road/model'
    }
}

# %% ../nbs/01_data.ipynb 17
def _validate_dataname(data_name: str):
    if data_name not in DEFAULT_DATA_CONFIGS.keys():
        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA_CONFIGS.keys()}, '
            f'but got data_name={data_name}.')

# %% ../nbs/01_data.ipynb 18
def load_data(
    data_name: str, # The name of data
    return_config: bool = False, # Return `data_config `or not
    data_configs: dict = None # Data configs to override default configuration
) -> TabularDataModule | Tuple[TabularDataModule, TabularDataModuleConfigs]: 
    """High-level util function for loading `data` and `data_config`."""
    
    _validate_dataname(data_name)

    # get data/config urls
    _data_path = DEFAULT_DATA_CONFIGS[data_name]['data']
    _conf_path = DEFAULT_DATA_CONFIGS[data_name]['conf']
    
    data_url = f"https://github.com/BirkhoffG/ReLax/raw/master/{_data_path}"
    conf_url = f"https://github.com/BirkhoffG/ReLax/raw/master/{_conf_path}"

    # create new dir
    data_dir = Path(os.getcwd()) / "cf_data" / data_name
    if not data_dir.exists():
        os.makedirs(data_dir)
    data_path = data_dir / 'data.csv'
    conf_path = data_dir / 'configs.json'

    # download data/configs
    if not data_path.is_file():
        urlretrieve(data_url, data_path)    
    if not conf_path.is_file():
        urlretrieve(conf_url, conf_path)

    # read config
    config = load_json(conf_path)['data_configs']
    config['data_dir'] = str(data_path)

    if not (data_configs is None):
        config.update(data_configs)

    config = TabularDataModuleConfigs(**config)
    data_module = TabularDataModule(config)

    if return_config:
        return data_module, config
    else:
        return data_module

