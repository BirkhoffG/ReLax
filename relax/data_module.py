# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% ../nbs/01_data.ipynb 3
from __future__ import annotations
from .utils import load_json, validate_configs, cat_normalize, get_config
from .base import *
from .data_utils import *
import jax
from jax import numpy as jnp, random as jrand, lax, Array
import pandas as pd
import numpy as np
from pathlib import Path
import json, os, shutil
# from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
# from sklearn.base import TransformerMixin
# from sklearn.utils.validation import check_is_fitted, NotFittedError
from urllib.request import urlretrieve
# from relax.data.loader import Dataset, ArrayDataset, DataLoader, DataloaderBackends
from pydantic.fields import ModelField, Field
from typing import List, Dict, Union, Optional, Tuple, Callable, Any, Iterable

# %% auto 0
__all__ = ['BaseDataModule', 'DataModuleConfig', 'DataModule', 'download_data_module_files', 'load_data']

# %% ../nbs/01_data.ipynb 6
class BaseDataModule(BaseModule):
    """DataModule Interface"""

    def prepare(self, *args, **kwargs):
        """Prepare data for training"""
        raise NotImplementedError
        
    def apply_constraints(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> Array:
        raise NotImplementedError
    
    def apply_regularization(self, x: Array, cf: Array, hard: bool = False, **kwargs) -> float:
        raise NotImplementedError

# %% ../nbs/01_data.ipynb 8
class DataModuleConfig(BaseConfig):
    """Configurator of `TabularDataModule`."""

    data_dir: str = Field(description="The directory of dataset.")
    data_name: str = Field(description="The name of `DataModule`.")
    continous_cols: List[str] = Field([], description="Continuous features/columns in the data.")
    discret_cols: List[str] = Field([], description="Categorical features/columns in the data.")
    imutable_cols: List[str] = Field([], description="Immutable features/columns in the data.")
    continuous_transformation: str = Field('minmax', description="Transformation for continuous features.")
    discret_transformation: str = Field('ohe', description="Transformation for categorical features.")
    sample_frac: Optional[float] = Field(
        None, description="Sample fraction of the data. Default to use the entire data.", ge=0., le=1.0
    )
    train_indices: List[int] = Field([], description="Indices of training data.")
    test_indices: List[int] = Field([], description="Indices of testing data.")
    
    # normalizer: Optional[str] = Field(
    #     default_factory=lambda: MinMaxScaler(),
    #     description="Sklearn scalar for continuous features. Can be unfitted, fitted, or None. "
    #     "If not fitted, the `TabularDataModule` will fit using the training data. If fitted, no fitting will be applied. "
    #     "If `None`, no transformation will be applied. Default to `MinMaxScaler()`."
    # )
    # encoder: Optional[str] = Field(
    #     default_factory=lambda: OneHotEncoder(sparse=False),
    #     description="Fitted encoder for categorical features. Can be unfitted, fitted, or None. "
    #     "If not fitted, the `TabularDataModule` will fit using the training data. If fitted, no fitting will be applied. "
    #     "If `None`, no transformation will be applied. Default to `OneHotEncoder(sparse=False)`."
    # )

    def shuffle(self, data: Array, test_size: float, seed: int = None):
        """Shuffle data with a seed."""
        if seed is None:
            seed = get_config().global_seed
        key = jrand.PRNGKey(seed)
        total_length = data.shape[0]
        train_length = int((1 - test_size) * total_length)
        if len(self.train_indices) == 0:
            self.train_indices = jrand.permutation(key, total_length)[:train_length].tolist()
        if len(self.test_indices) == 0:
            self.test_indices = jrand.permutation(key, total_length)[train_length:].tolist()

# %% ../nbs/01_data.ipynb 10
class DataModule(BaseDataModule):
    def __init__(
        self, 
        config, 
        data: pd.DataFrame = None,
        features: List[Feature] = None,
        label: Feature = None,
    ):
        config = validate_configs(config, DataModuleConfig)
        if data is None:
            data = pd.read_csv(config.data_dir)
        self._data = data
        features = self.convert_to_features(config, data, features)
        label = self.convert_to_label(config, data, label)
        self.prepare(features, label)
        config.shuffle(self.xs, test_size=0.25)
        super().__init__(config, name=config.data_name)
    
    def save(self, path):
        path = Path(path)
        if not path.exists():
            path.mkdir(parents=True)
        self._features.save(path / 'features')
        self._label.save(path / 'label')
        with open(path / "config.json", "w") as f:
            json.dump(self.config.dict(), f)

    @classmethod
    def load_from_path(cls, path, config=None):
        path = Path(path)
        if config is None:
            config = load_json(path / 'config.json')
        features = FeaturesList.load_from_path(path / 'features')
        label = FeaturesList.load_from_path(path / 'label')
        return cls(config, features=features, label=label)

    def convert_to_features(
        self, 
        config: DataModuleConfig, 
        data: pd.DataFrame, 
        features: list[Feature] = None
    ):
        to_feature = lambda col, data, is_continuous: Feature(
            name=col, data=data[col].to_numpy().reshape(-1, 1),
            transformation=config.continuous_transformation if is_continuous else config.discret_transformation,
            is_immutable=col in config.imutable_cols
        )

        if features is not None:
            return features
        
        cont_features = [to_feature(col, data, True) for col in config.continous_cols]
        cat_features = [to_feature(col, data, False) for col in config.discret_cols]
        return cont_features + cat_features        
        
    def convert_to_label(self, config: DataModuleConfig, data: pd.DataFrame, label: Feature = None):
        if label is not None:
            return label
        
        label_col = data.columns[-1]
        return Feature(
            name=label_col, data=data[label_col].to_numpy().reshape(-1, 1),
            transformation='identity',
            is_immutable=label_col in config.imutable_cols
        )
        
    def prepare(self, features, label):
        if features is not None and label is not None:
            self._features = FeaturesList(features)
            self._label = FeaturesList(label)
        elif features is None:
            raise ValueError("Features cannot be None.")
        elif label is None:
            raise ValueError("Label cannot be None.")
    
    @property
    def data(self) -> pd.DataFrame:
        return self._data
    
    @property
    def xs(self) -> Array:
        return self._features.transformed_data
    
    @property
    def ys(self) -> Array:
        return self._label.transformed_data

    @property
    def dataset(self) -> Tuple[Array, Array]:
        return (self.xs, self.ys)
    
    def _get_data(self, indices):
        if isinstance(indices, list):
            indices = jnp.array(indices)
        return (self.xs[indices], self.ys[indices])
        
    def __getitem__(self, name: str):
        if name == 'train':
            return self._get_data(self.config.train_indices)
        elif name in ['valid', 'test']:
            return self._get_data(self.config.test_indices)
        else:
            raise ValueError(f"Unknown data name: {name}. Should be one of ['train', 'valid', 'test']")
        
    def apply_constraints(self, x: Array, cf: Array, hard: bool = False) -> Array:
        return self._features.apply_constraints(x, cf, hard)

# %% ../nbs/01_data.ipynb 13
DEFAULT_DATA = [
    'adult',
    'heloc',
    'oulad',
    'credit',
    'cancer',
    'student_performance',
    'titanic',
    'german',
    'spam',
    'ozone',
    'qsar',
    'bioresponse',
    'churn',
    'road'
 ]

DEFAULT_DATA_CONFIGS = { 
    data: { 
        'data': f"{data}/data", 'model': f"{data}/model",
    } for data in DEFAULT_DATA
}

# %% ../nbs/01_data.ipynb 17
def _validate_dataname(data_name: str):
    if data_name not in DEFAULT_DATA:
        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA}, '
            f'but got data_name={data_name}.')

# %% ../nbs/01_data.ipynb 18
def download_data_module_files(
    data_name: str, # The name of data
    data_parent_dir: Path, # The directory to save data.
    download_original_data: bool = False, # Download original data or not
):
    files = [
        "features/data.npy", "features/treedef.json",
        "label/data.npy", "label/treedef.json",
        "config.json"
    ]
    if download_original_data:
        files.append("data.csv")
    for f in files:
        url = f"https://github.com/BirkhoffG/ReLax-Assets/raw/master/{data_name}/data/{f}"
        f_path = data_parent_dir / f'{data_name}/data' / f
        os.makedirs(f_path.parent, exist_ok=True)
        if not f_path.is_file(): urlretrieve(url, f_path)


def load_data(
    data_name: str, # The name of data
    return_config: bool = False, # Return `data_config `or not
    data_configs: dict = None, # Data configs to override default configuration
    download_original_data: bool = False, # Download original data or not
) -> DataModule | Tuple[DataModule, DataModuleConfig]: 
    """High-level util function for loading `data` and `data_config`."""
    
    _validate_dataname(data_name)

    # create new dir
    data_parent_dir = Path(os.getcwd()) / "relax-assets"
    if not data_parent_dir.exists():
        os.makedirs(data_parent_dir, exist_ok=True)
    # download files
    download_data_module_files(
        data_name, data_parent_dir, 
        download_original_data=download_original_data
    )

    # read config
    conf_path = data_parent_dir / f'{data_name}/data/config.json'
    config = load_json(conf_path)

    if not (data_configs is None):
        config.update(data_configs)

    config = DataModuleConfig(**config)
    data_dir = data_parent_dir / f'{data_name}/data'
    data_module = DataModule.load_from_path(data_dir, config=config)

    if return_config:
        return data_module, config
    else:
        return data_module

