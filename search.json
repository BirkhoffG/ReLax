[
  {
    "objectID": "data.scm.html",
    "href": "data.scm.html",
    "title": "Causal Graphical Models",
    "section": "",
    "text": "https://github.com/amirhk/recourse/blob/master/distributions.py\n\nsource\n\n\n\nCLASS relax.data.scm.BaseDistribution (name)\n\nBase class for all distributions.\n\nsource\n\n\n\n\nCLASS relax.data.scm.MixtureOfGaussians (probs, means, vars)\n\nMixture of Gaussians distribution.\n\nsource\n\n\n\n\nCLASS relax.data.scm.Normal (mean, var)\n\nNormal distribution."
  },
  {
    "objectID": "data.scm.html#distributions",
    "href": "data.scm.html#distributions",
    "title": "Causal Graphical Models",
    "section": "",
    "text": "https://github.com/amirhk/recourse/blob/master/distributions.py\n\nsource\n\n\n\nCLASS relax.data.scm.BaseDistribution (name)\n\nBase class for all distributions.\n\nsource\n\n\n\n\nCLASS relax.data.scm.MixtureOfGaussians (probs, means, vars)\n\nMixture of Gaussians distribution.\n\nsource\n\n\n\n\nCLASS relax.data.scm.Normal (mean, var)\n\nNormal distribution."
  },
  {
    "objectID": "data.scm.html#load-scm",
    "href": "data.scm.html#load-scm",
    "title": "Causal Graphical Models",
    "section": "Load SCM",
    "text": "Load SCM\nLoad scm structural equations.\n\nsource\n\nSANITY_3_LIN\n\nrelax.data.scm.sanity_3_lin ()"
  },
  {
    "objectID": "data.scm.html#causal-model",
    "href": "data.scm.html#causal-model",
    "title": "Causal Graphical Models",
    "section": "Causal Model",
    "text": "Causal Model\n\nsource\n\nCAUSALMODEL\n\nCLASS relax.data.scm.CausalModel (scm_class)\n\nClass with topological methods given a structural causal model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nscm_class (str) ‚Äì Name of the structural causal model.\n\n\n\nCredit goes to the CARLA implementation.\n\nscm = CausalModel('sanity_3_lin')\n\n\nassert scm.get_children('x1') == {'x2', 'x3'}\nassert scm.get_parents('x3') == ['x1', 'x2']\nassert scm.get_ancestors('x3') == ['x1', 'x2']\nassert scm.get_descendants('x1') == ['x2', 'x3']\nassert scm.get_non_descendants('x1') == set()"
  },
  {
    "objectID": "data.scm.html#generate-synthethic-data",
    "href": "data.scm.html#generate-synthethic-data",
    "title": "Causal Graphical Models",
    "section": "Generate synthethic Data",
    "text": "Generate synthethic Data\nAdapted from Carla."
  },
  {
    "objectID": "data.scm.html#experimental-data-module",
    "href": "data.scm.html#experimental-data-module",
    "title": "Causal Graphical Models",
    "section": "(Experimental) Data Module",
    "text": "(Experimental) Data Module\n\nscm = CausalModel('sanity_3_lin')\ndf_endogenous, df_exogenous = _create_synthetic_data(scm, 1000)\nd_config = TabularDataModuleConfigs(\n    data_dir=\".\",\n    data_name='sanity_3_lin',\n    continous_cols=scm._continuous,\n    discret_cols=scm._categorical,\n)\n\n\ndm = TabularDataModule(d_config, data=df_endogenous)\n\n\nsetattr(dm, 'scm', scm)\nsetattr(dm, 'exogenous', df_exogenous)\n\n\nassert isinstance(dm.scm, CausalModel)\nassert isinstance(dm.exogenous, pd.DataFrame)"
  },
  {
    "objectID": "logger.html",
    "href": "logger.html",
    "title": "Logger",
    "section": "",
    "text": "source\n\nTENSORBOARDLOGGER\n\nCLASS relax.logger.TensorboardLogger (log_dir, name, on_step=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nlogger = TensorboardLogger('log', name='debug')\nlogger.save_hyperparams({'lr': 0.01})\nlogger.log_dict({'train/train_loss_1': 0.1, 'epoch': 0})\nlogger.log_dict({'train/train_loss_1': 0.1})\nlogger.log_dict({'train/train_loss_1': 0.05})\n\nlogger.log_dict({'train/train_loss_1': 0.07, 'epoch': 1})\nlogger.log_dict({'train/train_loss_1': 0.05, })\nlogger.log_dict({'train/train_loss_1': 0.05, 'epoch': 2})\n\nlogger.close()"
  },
  {
    "objectID": "learning.html",
    "href": "learning.html",
    "title": "Training",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "learning.html#examples",
    "href": "learning.html#examples",
    "title": "Training",
    "section": "Examples",
    "text": "Examples\nA siimple example to train a predictive model.\n\nfrom relax.data import TabularDataModule, load_data\nfrom relax.module import PredictiveTrainingModule, PredictiveModelConfigs\n\n\ndatamodule = load_data('adult')\n\nparams, opt_state = train_model(\n    PredictiveTrainingModule({'sizes': [50, 10, 50], 'lr': 0.003}), \n    datamodule, t_configs={\n        'n_epochs': 10, 'batch_size': 256, 'monitor_metrics': 'val/val_loss'\n    }\n)\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:00&lt;00:00, 377.38batch/s, train/train_loss_1=0.0706]"
  },
  {
    "objectID": "training_module.html",
    "href": "training_module.html",
    "title": "Module",
    "section": "",
    "text": "Networks are haiku.module, which define model architectures.\n\nsource\n\n\n\nCLASS relax.module.BaseNetwork ()\n\nBaseNetwork needs a is_training argument\n\nsource\n\n\n\n\nCLASS relax.module.DenseBlock (output_size, dropout_rate=0.3, name=None)\n\nA DenseBlock consists of a dense layer, followed by Leaky Relu and a dropout layer.\n\n\n\n\n\n\nParameters:\n\n\n\n\noutput_size (int) ‚Äì Output dimensionality.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (str | None, default=None) ‚Äì Name of the Module\n\n\n\n\nsource\n\n\n\n\nCLASS relax.module.MLP (sizes, dropout_rate=0.3, name=None)\n\nA MLP consists of a list of DenseBlock layers.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsizes (Iterable[int]) ‚Äì Sequence of layer sizes.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (str | None, default=None) ‚Äì Name of the Module"
  },
  {
    "objectID": "training_module.html#networks",
    "href": "training_module.html#networks",
    "title": "Module",
    "section": "",
    "text": "Networks are haiku.module, which define model architectures.\n\nsource\n\n\n\nCLASS relax.module.BaseNetwork ()\n\nBaseNetwork needs a is_training argument\n\nsource\n\n\n\n\nCLASS relax.module.DenseBlock (output_size, dropout_rate=0.3, name=None)\n\nA DenseBlock consists of a dense layer, followed by Leaky Relu and a dropout layer.\n\n\n\n\n\n\nParameters:\n\n\n\n\noutput_size (int) ‚Äì Output dimensionality.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (str | None, default=None) ‚Äì Name of the Module\n\n\n\n\nsource\n\n\n\n\nCLASS relax.module.MLP (sizes, dropout_rate=0.3, name=None)\n\nA MLP consists of a list of DenseBlock layers.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsizes (Iterable[int]) ‚Äì Sequence of layer sizes.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (str | None, default=None) ‚Äì Name of the Module"
  },
  {
    "objectID": "training_module.html#predictive-model",
    "href": "training_module.html#predictive-model",
    "title": "Module",
    "section": "Predictive Model",
    "text": "Predictive Model\n\nsource\n\nPREDICTIVEMODEL\n\nCLASS relax.module.PredictiveModel (sizes, dropout_rate=0.3, name=None)\n\nA basic predictive model for binary classification.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsizes (List[int]) ‚Äì Sequence of layer sizes.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (Optional[str], default=None) ‚Äì Name of the module.\n\n\n\nUse make_hk_module to create a haiku.Transformed model.\n\nfrom relax.utils import make_hk_module\n\n\nnet = make_hk_module(PredictiveModel, sizes=[50, 20, 10], dropout_rate=0.3)\n\nWe make some random data.\n\nkey = hk.PRNGSequence(42)\nxs = random.normal(next(key), (1000, 10))\n\nWe can then initalize the model\n\nparams = net.init(next(key), xs, is_training=True)\n\nWe can view model‚Äôs structure via jax.tree_map.\n\njax.tree_map(lambda x: x.shape, params)\n\n{'predictive_model/linear': {'b': (1,), 'w': (10, 1)},\n 'predictive_model/mlp/dense_block/linear': {'b': (50,), 'w': (10, 50)},\n 'predictive_model/mlp/dense_block_1/linear': {'b': (20,), 'w': (50, 20)},\n 'predictive_model/mlp/dense_block_2/linear': {'b': (10,), 'w': (20, 10)}}\n\n\nModel output is produced via apply function.\n\ny = net.apply(params, next(key), xs, is_training=True)\n\nFor more usage of haiku.module, please refer to Haiku documentation."
  },
  {
    "objectID": "training_module.html#training-modules-api",
    "href": "training_module.html#training-modules-api",
    "title": "Module",
    "section": "Training Modules API",
    "text": "Training Modules API\n\nsource\n\nBASETRAININGMODULE\n\nCLASS relax.module.BaseTrainingModule ()\n\nHelper class that provides a standard way to create an ABC using inheritance."
  },
  {
    "objectID": "training_module.html#predictive-training-module",
    "href": "training_module.html#predictive-training-module",
    "title": "Module",
    "section": "Predictive Training Module",
    "text": "Predictive Training Module\n\nsource\n\nPREDICTIVETRAININGMODULECONFIGS\n\nCLASS relax.module.PredictiveTrainingModuleConfigs (lr, sizes, dropout_rate=0.3)\n\nConfigurator of PredictiveTrainingModule.\n\n\n\n\n\n\nParameters:\n\n\n\n\nlr (float) ‚Äì Learning rate.\nsizes (List[int]) ‚Äì Sequence of layer sizes.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate\n\n\n\n\nsource\n\n\nPREDICTIVETRAININGMODULE\n\nCLASS relax.module.PredictiveTrainingModule (m_configs)\n\nA training module for predictive models."
  },
  {
    "objectID": "tutorials/contribution.html",
    "href": "tutorials/contribution.html",
    "title": "Contribute",
    "section": "",
    "text": "This library uses nbdev for development. We love great flexibility offered by jupyter notebook, and nbdev in addressing limitations of using Notebook in developing large-scale projects (e.g., sync between notebooks and python modules, documentations).\nHere, we only cover basis of our development procedure. For an in-depth use of nbdev, please refer to the nbdev tutorial. Following links are particularly useful:"
  },
  {
    "objectID": "tutorials/contribution.html#set-up-the-working-environment",
    "href": "tutorials/contribution.html#set-up-the-working-environment",
    "title": "Contribute",
    "section": "Set up the working environment",
    "text": "Set up the working environment\nRefer to installation guidance for installing ReLax. For running ReLax in CPU, you should\npip install jax-relax\nNext, install Quarto for the documentation system. See nbdev docs for more details.\nnbdev_install_quarto\nNext, install hooks for cleaning Jupyter Notebooks.\nnbdev_install_hooks"
  },
  {
    "objectID": "tutorials/contribution.html#write-code-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-code-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Code in Jupyter Notebook",
    "text": "Write Code in Jupyter Notebook\nNote that nbdev provides a best practice guidline to writing code in Jupyter Notebooks. Here, we present some of the most important steps.\n\nExport Cell to Python Module\n#| export marks code cells (in Notebook; .ipynb) to be exported to Python Module (.py). By default, this cell will be exported to the file defined in #| default_exp file_name (usually presented upfront).\nFor example, the below function will be exported to the Python module.\n#| export\ndef func(args):\n    ...\nWe can also specify files to be exported.\n#| export file_name.py\ndef func(args):\n    ...\nFor private functions/objects, we can use #| exporti. In this way, the code will still be exported to the file, but not included in __all__.\nMore about directives.\n\n\nTwo-way Sync between Notebooks (.ipynb) and Python Code (.py)\nTo update code written in Jupyter Notebook to Python Module (i.e., .ipynb -&gt; .py)\nnbdev_export\nTo sync code updated in Python Module back to Jupyter Notebook (i.e., .py -&gt; .ipynb)\nnbdev_update\n\n\n\n\n\n\nWarning\n\n\n\nIf you write a new function/object in .py, nbdev_update will not include this function in __all__. The best practice is to write functions/objects in Jupyter Notebook, and debug in Python Module (via IDE).\n\n\n\n\nCode Style\nReLax follows the black code style. See black‚Äôs code style document."
  },
  {
    "objectID": "tutorials/contribution.html#write-test-cases-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-test-cases-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Test Cases in Jupyter Notebook",
    "text": "Write Test Cases in Jupyter Notebook\nIt is desirable to write some unit tests for each function and object. nbdev recommends to write test cases after implementing a feature. A normal cell is considered for testing.\nFor example, let‚Äôs consider a function which adds up all the inputs:\n\ndef add_numbers(*args):\n    return sum(args)\n\nTo test this function, we write unit tests via assert.\n\n# check correctness\nassert add_numbers(1, 2, 3) == 6\n# check types\nassert type(add_numbers(1, 2, 3)) == int\nassert type(add_numbers(1., 2, 3)) == float\n\n\n\n\n\n\n\nNote\n\n\n\nNote that all the test cases should be quickly run. If a cell takes a long time to run (e.g., model training), mark the cell as #| eval: false to skip this cell."
  },
  {
    "objectID": "tutorials/contribution.html#write-documentations-in-jupyter-notebook",
    "href": "tutorials/contribution.html#write-documentations-in-jupyter-notebook",
    "title": "Contribute",
    "section": "Write Documentations in Jupyter Notebook",
    "text": "Write Documentations in Jupyter Notebook\n\nDoc string\nTo write documentations in nbdev, it is recommended to\n\nuse simple type annotations\ndescribe each arguments with short comments\nprovide code examples and explanations in separate cells\n\n\n\n\n\n\n\nTip\n\n\n\nUnion typing is introduced after Python 3.10. For Python 3.7 - 3.9 users, you should\nfrom __future__ import annotations\n\n\n\ndef validate_configs(\n    configs: dict|BaseParser, # A configuration of the model/data.\n    config_cls: BaseParser # The desired configuration class.\n) -&gt; BaseParser:\n    \"\"\"return a valid configuration object.\"\"\"\n    ...\n\nnbdev will automatically render the documentation:\n\n\nsource\n\nvalidate_configs\n\n validate_configs (configs:Union[dict,pydantic.main.BaseModel],\n                   config_cls:pydantic.main.BaseModel)\n\nreturn a valid configuration object.\n\n\n\n\nType\nDetails\n\n\n\n\nconfigs\ndict | BaseParser\nA configuration of the model/data.\n\n\nconfig_cls\nBaseParser\nThe desired configuration class.\n\n\nReturns\nBaseParser\n\n\n\n\n\n\nNext, we elaborate the use of this function with more descriptions and code examples.\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nvalidate_configs(configs, LearningConfigs)\n\nLearningConfigs(lr=0.01)\n\n\n\n\n\nCallout\nWe can also use callout for clear documentations.\n:::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\nwhich renders:\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, warning, important, tip, and caution."
  },
  {
    "objectID": "tutorials/contribution.html#preparing-a-code-commit",
    "href": "tutorials/contribution.html#preparing-a-code-commit",
    "title": "Contribute",
    "section": "Preparing a Code Commit",
    "text": "Preparing a Code Commit\nPreview the documentation system\nnbdev_preview\nIf everything is in your satisfaction, prepare code before commit to GitHub\nnbdev_prepare"
  },
  {
    "objectID": "tutorials/contribution.html#summary",
    "href": "tutorials/contribution.html#summary",
    "title": "Contribute",
    "section": "Summary",
    "text": "Summary\n\nInstall all required packages based on installation guidance\nInstall the git hook nbdev_install_hooks\nWrite code in Jupyter Notebooks; add approprate directives, e.g., #| export\nWrite tests after the code in the Notebooks; test the code via nbdev_test\nWrite documents directly in the Notebooks; preview the docs nbdev_preview\nPrepare changes with nbdev_prepare\nCreate pull requests and push changes to GitHub"
  },
  {
    "objectID": "tutorials/install.html",
    "href": "tutorials/install.html",
    "title": "Installation",
    "section": "",
    "text": "Tip\n\n\n\nTL;DR: Install ReLax via the Python Package Index:\npip install jax-relax"
  },
  {
    "objectID": "tutorials/install.html#installing-relax",
    "href": "tutorials/install.html#installing-relax",
    "title": "Installation",
    "section": "Installing ReLax",
    "text": "Installing ReLax\n\n\n\n\n\n\nNote\n\n\n\nWe suggest to create a new environment when using ReLax.\nIf you are using conda, you can create a new environment by:\nconda create -n relax python=3.8 -y\nconda activate relax\n\n\nReLax is built on top of Jax. You should also check the official installation guide from the Jax team.\n\nRunning on CPU\nIf you only need to run relax on CPU, you can simply install via pip or clone the GitHub project.\nInstallation via PyPI:\npip install --upgrade pip\npip install --upgrade jax-relax\nEditable Install:\ngit clone https://github.com/BirkhoffG/relax.git\npip install -e relax\n\n\nRunning on GPU or TPU\n\n\n\n\n\n\nWarning\n\n\n\nWe do not run continuous integration (CI) for GPU and TPU environments. So ReLax might fail unexpectedly when running on GPU/TPU. If you encounter issues when running on GPU/TPU, please report to us.\n\n\nIf you wish to run relax on GPU or TPU, please first install this library via pip install jax-relax.\nThen, you should install the right GPU or TPU version of Jax by following steps in the install guidelines.\n\n\nContributor of relax\nSee nbdev installation."
  },
  {
    "objectID": "tutorials/getting_started.html",
    "href": "tutorials/getting_started.html",
    "title": "Getting started",
    "section": "",
    "text": "This tutorial aims at introducing basics about ReLax, and how to use ReLax to generate counterfactual (or recourse) explanations for jax-based implementations of ML models.\nIn particular, we will cover the following things in this tutorial:"
  },
  {
    "objectID": "tutorials/getting_started.html#preparation",
    "href": "tutorials/getting_started.html#preparation",
    "title": "Getting started",
    "section": "Preparation",
    "text": "Preparation\nWe assume that you have already installed ReLax. If not, follow the steps in this installation tutorial, or just enter pip install jax-relax.\nWe also want to import some libraries for this tutorial.\n\nimport jax"
  },
  {
    "objectID": "tutorials/getting_started.html#load-dataset-with-tabulardatamodule",
    "href": "tutorials/getting_started.html#load-dataset-with-tabulardatamodule",
    "title": "Getting started",
    "section": "Load Dataset with TabularDataModule",
    "text": "Load Dataset with TabularDataModule\nTabularDataModule is a python class which modularizes tabular dataset loading. TabularDataModule loads a .csv file from the directory by specifying the following attributes:\n\ndata_name is the name of your dataset.\ndata_dir should contain the relative path of the directory where your dataset is located.\ncontinous_cols specifies a list of feature names representing all the continuous/numeric features in our dataset.\ndiscret_cols specifies a list of feature names representing all discrete features in our dataset. By default, all discrete features are converted via one-hot encoding for training purposes.\nimutable_cols specifies a list of feature names that represent immutable features that we do not wish to change in the generated recourse.\n\n\nfrom relax.data import TabularDataModuleConfigs, TabularDataModule, load_data\n\nFor example, to load the adult dataset, we can specify the TabularDataModuleConfigs as\n\ndata_config = TabularDataModuleConfigs(\n    # The name of this dataset is \"adult\"\n    data_name=\"adult\",\n    # The data file is located in `../assets/data/s_adult.csv`.\n    data_dir=\"../assets/data/s_adult.csv\",\n    # Contains 2 features with continuous variables\n    continous_cols=[\"age\",\"hours_per_week\"],\n    # Contains 6 features with categorical (discrete) variables\n    discret_cols=[\"workclass\",\"education\",\"marital_status\",\"occupation\",\"race\",\"gender\"],\n    # Contains 2 features that we do not wish to change\n    imutable_cols=[\"race\", \"gender\"]\n)\n\nWe can then pass data_configs to the TabularDataModule.\n\ndatamodule = TabularDataModule(data_config)\n\nAlternatively, we can also specify this config via a dictionary.\n\n# This approach is equivalent to using `TabularDataModuleConfigs`\ndata_config_dict = {\n    \"data_name\": \"adult\",\n    \"data_dir\": \"../assets/data/s_adult.csv\",\n    \"continous_cols\": [\"age\",\"hours_per_week\"],\n    \"discret_cols\": [\"workclass\",\"education\",\"marital_status\",\"occupation\",\"race\",\"gender\"],\n    \"imutable_cols\": [\"race\",\"gender\"]\n}\ndatamodule = TabularDataModule(data_config_dict)\n\nFor datasets supported by ReLax, we can simply call load_data:\n\n# This is equivalent to specifying configs for `TabularDataModule`\ndatamodule = load_data('adult')\n\nFor more usage of loading datasets in ReLax, check out the data module documentation."
  },
  {
    "objectID": "tutorials/getting_started.html#train-the-classifier",
    "href": "tutorials/getting_started.html#train-the-classifier",
    "title": "Getting started",
    "section": "Train the Classifier",
    "text": "Train the Classifier\nFor the purpose of exposing full functionality of the framework, we will train the model using the built-in functions in ReLax, which uses haiku for building neural network blocks. However, the recourse algorithms in ReLax can generate explanations for all jax-based framework (e.g., flax, haiku, vanilla jax).\n\n\n\n\n\n\nWarning\n\n\n\nThe recourse algorithms in ReLax currently only supports binary classification. The output of the classifier must be a probability score (bounded by [0, 1]). Future support for multi-class classification is planned.\n\n\nTraining a classifier using the built-in functions in ReLax is very simple. We will first specify the classifier. The classifier is called PredictiveTrainingModule, which specifies the model structure, and the optimization procedure (e.g., it specifies the loss function for optimizing the model). Next, we use train_model to train the model on TabularDataModule.\n\nDefine the Model\n\nfrom relax.module import PredictiveTrainingModuleConfigs, PredictiveTrainingModule\n\nDefining¬†PredictiveTrainingModule¬†is similar to defining¬†TabularDataModule. We first specify the configurator as PredictiveTrainingModuleConfigs, and pass this configurator to PredictiveTrainingModule.\n\nmodel_config = PredictiveTrainingModuleConfigs(\n    lr=0.01, # Learning rate\n    sizes=[50, 10, 50], # The sizes of the hidden layers\n    dropout_rate=0.3 # Dropout rate\n)\n\n# specify the predictive model\nmodule = PredictiveTrainingModule(model_config)\n\n\n\n\n\n\n\nNote\n\n\n\nThe training step for each batch is specified in PredictiveTrainingModule. Essentially, it will compute the binary cross-entropy loss for each batch, and apply backpropagation (via adam) to update parameters of the model.\n\n\n\n\nTrain the Model\n\nfrom relax.trainer import TrainingConfigs, train_model\n\nTo train PredictiveTrainingModule for the entire dataset (specified in TabularDataModule), we can simply call train_model:\n\ntrainer_config = TrainingConfigs(\n    n_epochs=10, # Number of epochs\n    batch_size=256, # Batch size\n    monitor_metrics='val/val_loss', # The metric to monitor\n    logger_name='pred' # The name of the logger\n)\n\n# train the model\nparams, opt_state = train_model(\n    module, datamodule, trainer_config\n)\n\nWARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n2023-04-24 10:46:46.661007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n2023-04-24 10:46:46.661163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n2023-04-24 10:46:46.661171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:00&lt;00:00, 218.13batch/s, train/train_loss_1=0.0634]\n\n\n\n\nMake Predictions\nWe can directly use module.pred_fn for making the predictions.\n\npred_fn = module.pred_fn"
  },
  {
    "objectID": "tutorials/getting_started.html#generate-counterfactual-explanations",
    "href": "tutorials/getting_started.html#generate-counterfactual-explanations",
    "title": "Getting started",
    "section": "Generate Counterfactual Explanations",
    "text": "Generate Counterfactual Explanations\nNow, it is time to use ReLax to generate counterfactual explanations (or recourse).\n\nfrom relax.methods import VanillaCF, VanillaCFConfig\n\nWe use VanillaCF (a very popular recourse generation algorithm) as an example for this tutorial. Defining VanillaCF is similar to defining TabularDataModule and PredictiveTrainingModule.\n\ncf_config = VanillaCFConfig(\n    n_steps=1000, # Number of steps\n    lr=0.001 # Learning rate\n)\ncf_exp = VanillaCF(cf_config)\n\nGenerate counterfactual examples.\n\nfrom relax.evaluate import generate_cf_explanations\n\n\ncf_results = generate_cf_explanations(\n    cf_exp, datamodule, pred_fn, \n    pred_fn_args={\n        'params': params, 'rng_key': jax.random.PRNGKey(0)\n    }\n)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:10&lt;00:00, 96.49it/s]"
  },
  {
    "objectID": "tutorials/getting_started.html#benchmark-the-counterfactual-method",
    "href": "tutorials/getting_started.html#benchmark-the-counterfactual-method",
    "title": "Getting started",
    "section": "Benchmark the Counterfactual Method",
    "text": "Benchmark the Counterfactual Method\nAfter we obtain the counterfactual results, we can use benchmark_cfs to evaluate the accuracy, validity, and proximity of the counterfactual example.\n\nfrom relax.evaluate import benchmark_cfs\n\n\nbenchmark_cfs([cf_results])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nVanillaCF\n0.829751\n0.999754\n7.896814"
  },
  {
    "objectID": "tutorials/forktable.html",
    "href": "tutorials/forktable.html",
    "title": "Benchmarking to 10M Dataset",
    "section": "",
    "text": "from relax.import_essentials import *\nfrom relax.data import *\nfrom relax.module import *\nimport datasets as hfds\nfrom relax.trainer import train_model, TrainingConfigs\nfrom relax.utils import *\nfrom relax._ckpt_manager import load_checkpoint, save_checkpoint\n\n\nds = hfds.load_dataset(\"birkhoffg/folktables-acs-income\")\n\nFound cached dataset parquet (/home/birk/.cache/huggingface/datasets/birkhoffg___parquet/birkhoffg--folktables-acs-income-bc190711a423bf3e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\n\n\n\ndef hfds_to_dm(\n    dataset: hfds.Dataset, \n    configs: TabularDataModuleConfigs\n) -&gt; TabularDataModule:\n    train_df = dataset[\"train\"].to_pandas()\n    test_df = dataset[\"test\"].to_pandas()\n    df = pd.concat([train_df, test_df])\n    if \"__index_level_0__\" in df.columns:\n        df = df.drop(columns=[\"__index_level_0__\"])\n    print('df is loaded')\n    dm = TabularDataModule(configs, df)\n    return dm\n\n\nconfigs = TabularDataModuleConfigs(\n    data_dir='',\n    data_name='forktable',\n    continous_cols=['AGEP', 'OCCP', 'POBP', 'RELP', 'WKHP'],\n    discret_cols=['COW', 'SCHL', 'MAR', 'SEX', 'RAC1P', 'STATE', 'YEAR'],\n    # sample_frac=0.1\n)\n\n\ndm = hfds_to_dm(ds, configs)\n\ndf is loaded\n\n\n/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\n\nmodule = PredictiveTrainingModule({\n    'lr': 1e-3,\n    'sizes': [110, 110, 50, 10],\n    'dropout': 0.3,\n})\n\n\nparams, _ = train_model(\n    module, dm, TrainingConfigs(\n        n_epochs=10, batch_size=256, monitor_metrics='val/val_accuracy',\n        max_n_checkpoints=1\n    )\n)\n\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28694/28694 [02:57&lt;00:00, 161.59batch/s, train/train_loss_1=0.0623]\n\n\n\ntrain_X, train_y = dm.train_dataset[:]\ntest_X, test_y = dm.test_dataset[:]\n\n\ny_pred = module.pred_fn(test_X, params, jrand.PRNGKey(0)).round()\n(y_pred == test_y).mean()"
  },
  {
    "objectID": "data.module.html",
    "href": "data.module.html",
    "title": "Data Module",
    "section": "",
    "text": "High-level interfaces for DataModule. Docs to be added.\n\nsource\n\n\n\nCLASS relax.data.module.BaseDataModule ()\n\nDataModule Interface"
  },
  {
    "objectID": "data.module.html#data-module-interfaces",
    "href": "data.module.html#data-module-interfaces",
    "title": "Data Module",
    "section": "",
    "text": "High-level interfaces for DataModule. Docs to be added.\n\nsource\n\n\n\nCLASS relax.data.module.BaseDataModule ()\n\nDataModule Interface"
  },
  {
    "objectID": "data.module.html#tabula-data-module",
    "href": "data.module.html#tabula-data-module",
    "title": "Data Module",
    "section": "Tabula Data Module",
    "text": "Tabula Data Module\nDataModule for processing tabular data.\n\nsource\n\nTRANSFORMERMIXINTYPE\n\nCLASS relax.data.module.TransformerMixinType ()\n\nMixin class for all transformers in scikit-learn.\nIf :term:get_feature_names_out is defined, then BaseEstimator will automatically wrap transform and fit_transform to follow the set_output API. See the :ref:developer_api_set_output for details.\n:class:base.OneToOneFeatureMixin and :class:base.ClassNamePrefixFeaturesOutMixin are helpful mixins for defining :term:get_feature_names_out.\n\nsource\n\n\nTABULARDATAMODULECONFIGS\n\nCLASS relax.data.module.TabularDataModuleConfigs (data_dir, data_name, continous_cols=[], discret_cols=[], imutable_cols=[], normalizer=None, encoder=None, sample_frac=None, backend=‚Äòjax‚Äô)\n\nConfigurator of TabularDataModule.\n\n\n\n\n\n\nParameters:\n\n\n\n\ndata_dir (str) ‚Äì The directory of dataset.\ndata_name (str) ‚Äì The name of TabularDataModule.\ncontinous_cols (List[str], default=[]) ‚Äì Continuous features/columns in the data.\ndiscret_cols (List[str], default=[]) ‚Äì Categorical features/columns in the data.\nimutable_cols (List[str], default=[]) ‚Äì Immutable features/columns in the data.\nnormalizer (Optional[TransformerMixinType]) ‚Äì Sklearn scalar for continuous features. Can be unfitted, fitted, or None. If not fitted, the TabularDataModule will fit using the training data. If fitted, no fitting will be applied. If None, no transformation will be applied. Default to MinMaxScaler().\nencoder (Optional[TransformerMixinType]) ‚Äì Fitted encoder for categorical features. Can be unfitted, fitted, or None. If not fitted, the TabularDataModule will fit using the training data. If fitted, no fitting will be applied. If None, no transformation will be applied. Default to OneHotEncoder(sparse=False).\nsample_frac (Optional[float]) ‚Äì Sample fraction of the data. Default to use the entire data.\nbackend (str, default=jax) ‚Äì Dataloader backend. Currently supports: [‚Äòjax‚Äô, ‚Äòpytorch‚Äô]\n\n\n\nAn example configurator of the adult dataset:\n\nconfigs_dict = {\n    \"data_dir\": \"assets/data/s_adult.csv\",\n    \"data_name\": \"adult\",\n    \"continous_cols\": [\"age\", \"hours_per_week\"],\n    \"discret_cols\": [\"workclass\", \"education\", \"marital_status\",\"occupation\"],\n    \"imutable_cols\": [\"age\", \"workclass\", \"marital_status\"],\n    \"normalizer\": MinMaxScaler(),\n    \"encoder\": OneHotEncoder(sparse=False),\n    \"sample_frac\": 0.1,\n    \"backend\": \"jax\"\n}\nconfigs = TabularDataModuleConfigs(**configs_dict)\n\n\nsource\n\n\nTABULARDATAMODULE\n\nCLASS relax.data.module.TabularDataModule (data_config, data=None)\n\nDataModule for tabular data\n\n\n\n\n\n\nParameters:\n\n\n\n\ndata_config (dict | TabularDataModuleConfigs) ‚Äì Configurator of TabularDataModule\ndata (pd.DataFrame, default=None) ‚Äì Data in pd.DataFrame. If data is None, the DataModule will load data from data_dir.\n\n\n\nTo load TabularDataModule from TabularDataModuleConfigs,\n\nconfigs = TabularDataModuleConfigs(\n    data_name='adult',\n    data_dir='assets/data/s_adult.csv',\n    continous_cols=['age', 'hours_per_week'],\n    discret_cols=['workclass', 'education', 'marital_status', 'occupation'],\n    imutable_cols=['age', 'workclass', 'marital_status'],\n    sample_frac=0.1\n)\n\ndm = TabularDataModule(configs)\n\nWe can also explicitly pass a pd.DataFrame to TabularDataModule. In this case, TabularDataModule will use the passed pd.DataFrame, instead of loading data from data_dir in TabularDataModuleConfigs.\n\ndf = pd.read_csv('assets/data/s_adult.csv')[:1000]\ndm = TabularDataModule(configs, data=df)\nassert len(dm.data) == 1000 # dm contains `df`\n\n\nsource\n\n\nTABULARDATAMODULE.DATA\n\nrelax.data.module.TabularDataModule.data ()\n\nLoaded data in pd.DataFrame.\nTabulaDataModule loads either a csv file (specified in data_dir in data_config), or directly passes a DataFrame (specified as data). Either way, this data needs to satisfy following conditions:\n\nIt requires the target column (i.e., the labels) to be the last column of the DataFrame, and the rest columns are features. This target column needs to be binary-valued (i.e., it is either 0 or 1).\n\nIn the belowing example, income is the target column, and the rest columns are features.\n\nIt requires continous_cols and discret_cols in data_config to be subsets of data.columns.\nIt only use columns specified in continous_cols and discret_cols.\n\nIt loads continous_cols first, then discret_cols.\n\n\n\ndm.data.head()\n\n\n\n\n\n\n\n\nage\nhours_per_week\nworkclass\neducation\nmarital_status\noccupation\nincome\n\n\n\n\n0\n42.0\n45.0\nPrivate\nHS-grad\nMarried\nBlue-Collar\n1\n\n\n1\n32.0\n40.0\nSelf-Employed\nSome-college\nMarried\nBlue-Collar\n0\n\n\n2\n35.0\n40.0\nPrivate\nAssoc\nSingle\nWhite-Collar\n1\n\n\n3\n36.0\n40.0\nPrivate\nHS-grad\nSingle\nBlue-Collar\n0\n\n\n4\n57.0\n35.0\nPrivate\nSchool\nMarried\nService\n0\n\n\n\n\n\n\n\n\nsource\n\n\nTABULARDATAMODULE.TRANSFORM\n\nrelax.data.module.TabularDataModule.transform (data)\n\nTransform data into numerical representations.\n\n\n\n\n\n\nParameters:\n\n\n\n\ndata (pd.DataFrame) ‚Äì Data to be transformed to numpy.ndarray\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(Tuple[np.ndarray, np.ndarray]) ‚Äì Return (X, y)\n\n\nBy default, we transform continuous features via MinMaxScaler, and discrete features via OneHotEncoding.\nA tabular data point x is encoded as x = [\\underbrace{x_{0}, x_{1}, ..., x_{m}}_{\\text{cont features}},\n\\underbrace{x_{m+1}^{c=1},..., x_{m+p}^{c=1}}_{\\text{cat feature} (1)}, ...,\n\\underbrace{x_{k-q}^{c=i},..., x_{k}^{^{c=i}}}_{\\text{cat feature} (i)}]\n\ndf = dm.data.head()\nX, y = dm.transform(df)\n\nassert isinstance(X, np.ndarray)\nassert isinstance(y, np.ndarray)\nassert y.shape == (len(X), 1)\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nTABULARDATAMODULE.INVERSE_TRANSFORM\n\nrelax.data.module.TabularDataModule.inverse_transform (x, y=None)\n\nScaled back into pd.DataFrame.\n\n\n\n\n\n\nParameters:\n\n\n\n\nx (jnp.DeviceArray) ‚Äì The transformed input to be scaled back\ny (jnp.DeviceArray, default=None) ‚Äì The transformed label to be scaled back. If None, the target columns will not be scaled back.\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(pd.DataFrame) ‚Äì Transformed pd.DataFrame.\n\n\nTabularDataModule.inverse_transform scales numerical representations back to the original DataFrame.\n\ndm.inverse_transform(X, y)\n\n\n\n\n\n\n\n\nage\nhours_per_week\nworkclass\neducation\nmarital_status\noccupation\nincome\n\n\n\n\n0\n42.0\n45.0\nPrivate\nHS-grad\nMarried\nBlue-Collar\n1\n\n\n1\n32.0\n40.0\nSelf-Employed\nSome-college\nMarried\nBlue-Collar\n0\n\n\n2\n35.0\n40.0\nPrivate\nAssoc\nSingle\nWhite-Collar\n1\n\n\n3\n36.0\n40.0\nPrivate\nHS-grad\nSingle\nBlue-Collar\n0\n\n\n4\n57.0\n35.0\nPrivate\nSchool\nMarried\nService\n0\n\n\n\n\n\n\n\nIf y is not passed, it will only scale back X.\n\ndm.inverse_transform(X)\n\n\n\n\n\n\n\n\nage\nhours_per_week\nworkclass\neducation\nmarital_status\noccupation\n\n\n\n\n0\n42.0\n45.0\nPrivate\nHS-grad\nMarried\nBlue-Collar\n\n\n1\n32.0\n40.0\nSelf-Employed\nSome-college\nMarried\nBlue-Collar\n\n\n2\n35.0\n40.0\nPrivate\nAssoc\nSingle\nWhite-Collar\n\n\n3\n36.0\n40.0\nPrivate\nHS-grad\nSingle\nBlue-Collar\n\n\n4\n57.0\n35.0\nPrivate\nSchool\nMarried\nService\n\n\n\n\n\n\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nTABULARDATAMODULE.APPLY_CONSTRAINTS\n\nrelax.data.module.TabularDataModule.apply_constraints (x, cf, hard=False)\n\nApply categorical normalization and immutability constraints\n\n\n\n\n\n\nParameters:\n\n\n\n\nx (jnp.DeviceArray) ‚Äì input\ncf (jnp.DeviceArray) ‚Äì Unnormalized counterfactuals\nhard (bool, default=False) ‚Äì Apply hard constraints or not\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(jnp.DeviceArray)\n\n\nTabularDataModule.apply_constraints does two things:\n\nIt ensures that generated counterfactuals respect the one-hot encoding format (i.e., \\sum_{p \\to q} x^{c=i}_{p} = 1).\nIt ensures the immutability constraints (i.e., immutable features defined in imutable_cols will not be changed).\n\n\nx, y = next(iter(dm.test_dataloader(batch_size=128)))\n# unnormalized counterfactuals\ncf = random.normal(\n    random.PRNGKey(0), x.shape\n)\n# normalized counterfactuals\ncf_normed = dm.apply_constraints(x, cf)\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nTABULARDATAMODULE.APPLY_REGULARIZATION\n\nrelax.data.module.TabularDataModule.apply_regularization (x, cf)\n\nApply categorical constraints by adding regularization terms\n\n\n\n\n\n\nParameters:\n\n\n\n\nx (jnp.DeviceArray) ‚Äì Input\ncf (jnp.DeviceArray) ‚Äì Unnormalized counterfactuals\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(float) ‚Äì Return regularization loss\n\n\n\nx, y = next(iter(dm.test_dataloader(batch_size=128)))\n# unnormalized counterfactuals\ncf = random.normal(\n    random.PRNGKey(0), x.shape\n)\n# normalized counterfactuals\ncf_normed = dm.apply_constraints(x, cf)\n\n\nsource\n\n\nSAMPLE\n\nrelax.data.module.sample (datamodule, frac=1.0)"
  },
  {
    "objectID": "data.module.html#load-data",
    "href": "data.module.html#load-data",
    "title": "Data Module",
    "section": "Load Data",
    "text": "Load Data\n\nsource\n\nLOAD_DATA\n\nrelax.data.module.load_data (data_name, return_config=False, data_configs=None)\n\nHigh-level util function for loading data and data_config.\n\n\n\n\n\n\nParameters:\n\n\n\n\ndata_name (str) ‚Äì The name of data\nreturn_config (bool, default=False) ‚Äì Return data_configor not\ndata_configs (dict, default=None) ‚Äì Data configs to override default configuration\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(TabularDataModule | Tuple[TabularDataModule, TabularDataModuleConfigs])\n\n\nload_data easily loads example datasets by passing the data_name. For example, you can load the adult as:\n\ndm = load_data(data_name = 'adult')\n\nUnderlying, load_data loads the default data_configs. To access this data_configs,\n\ndm, data_configs = load_data(data_name = 'adult', return_config=True)\n\nIf you want to override some of the data configs, you can pass it as an auxillary argumenet in data_configs. For example, if you want to use only 10% of the data, you can\n\ndm = load_data(\n    data_name = 'adult', data_configs={'sample_frac': 0.1}\n)\n\n\nSupported Datasets\nload_data currently supports following datasets:\n\n\n\n\n\n\n\n\n\n# Cont Features\n# Cat Features\n# of Data Points\n\n\n\n\nadult\n2\n6\n32561\n\n\nheloc\n21\n2\n10459\n\n\noulad\n23\n8\n32593\n\n\ncredit\n20\n3\n30000\n\n\ncancer\n30\n0\n569\n\n\nstudent_performance\n2\n14\n649\n\n\ntitanic\n2\n24\n891\n\n\ngerman\n7\n13\n1000\n\n\nspam\n57\n0\n4601\n\n\nozone\n72\n0\n2534\n\n\nqsar\n38\n3\n1055\n\n\nbioresponse\n1776\n0\n3751\n\n\nchurn\n3\n16\n7043\n\n\nroad\n29\n3\n111762\n\n\n\n\n\n\n\n\nDEFAULT_DATA_CONFIGS.keys()\n\ndict_keys(['adult', 'heloc', 'oulad', 'credit', 'cancer', 'student_performance', 'titanic', 'german', 'spam', 'ozone', 'qsar', 'bioresponse', 'churn', 'road'])"
  },
  {
    "objectID": "data.loader.html",
    "href": "data.loader.html",
    "title": "Dataloader",
    "section": "",
    "text": "source\n\n\n\nCLASS relax.data.loader.Dataset ()\n\nA pytorch-like abstract Dataset class.\n\nsource\n\n\n\n\nCLASS relax.data.loader.ArrayDataset (*arrays)\n\nDataset wrapping tensors.\n\nkey = random.PRNGKey(0)\nX = jax.random.normal(key, shape=(10, 10))\ny = jax.random.normal(key, shape=(10, ))\nds = ArrayDataset(X, y)\n\nassert len(ds) == 10\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nIndexing Dataset using ds[idx]\n\nx1, y1 = ds[1]\nassert jnp.array_equal(x1, X[1])\nassert jnp.array_equal(y1, y[1])"
  },
  {
    "objectID": "data.loader.html#dataset",
    "href": "data.loader.html#dataset",
    "title": "Dataloader",
    "section": "",
    "text": "source\n\n\n\nCLASS relax.data.loader.Dataset ()\n\nA pytorch-like abstract Dataset class.\n\nsource\n\n\n\n\nCLASS relax.data.loader.ArrayDataset (*arrays)\n\nDataset wrapping tensors.\n\nkey = random.PRNGKey(0)\nX = jax.random.normal(key, shape=(10, 10))\ny = jax.random.normal(key, shape=(10, ))\nds = ArrayDataset(X, y)\n\nassert len(ds) == 10\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nIndexing Dataset using ds[idx]\n\nx1, y1 = ds[1]\nassert jnp.array_equal(x1, X[1])\nassert jnp.array_equal(y1, y[1])"
  },
  {
    "objectID": "data.loader.html#dataloader",
    "href": "data.loader.html#dataloader",
    "title": "Dataloader",
    "section": "Dataloader",
    "text": "Dataloader\n\nsource\n\nBASEDATALOADER\n\nCLASS relax.data.loader.BaseDataLoader (dataset, backend, batch_size=1, shuffle=False, drop_last=False, **kwargs)\n\nDataloader Interface\n\n\n\n\n\n\nParameters:\n\n\n\n\ndataset\nbackend (str)\nbatch_size (int, default=1) ‚Äì Batch size\nshuffle (bool, default=False) ‚Äì If true, dataloader shuffles before sampling each batch\ndrop_last (bool, default=False) ‚Äì Drop last batches or not\nkwargs\n\n\n\n\nsource\n\n\nJAXDATALOADER\n\nCLASS relax.data.loader.JaxDataloader (dataset, backend=‚Äòjax‚Äô, batch_size=1, shuffle=False, drop_last=False, **kwargs)\n\nDataloder in vanilla Jax\n\n\n\n\n\n\nParameters:\n\n\n\n\ndataset (Dataset)\nbackend (str, default=jax) ‚Äì Position argument\nbatch_size (int, default=1) ‚Äì Batch size\nshuffle (bool, default=False) ‚Äì If true, dataloader shuffles before sampling each batch\ndrop_last (bool, default=False) ‚Äì Drop last batches or not\nkwargs\n\n\n\n\nsource\n\n\nTORCHDATALOADER\n\nCLASS relax.data.loader.TorchDataloader (dataset, backend=‚Äòpytorch‚Äô, batch_size=1, shuffle=False, num_workers=0, drop_last=False, **kwargs)\n\nUse Pytorch to load batches. It requires pytorch to be installed.\n\n\n\n\n\n\nParameters:\n\n\n\n\ndataset (Dataset)\nbackend (str, default=pytorch) ‚Äì positional argument\nbatch_size (int, default=1) ‚Äì batch size\nshuffle (bool, default=False) ‚Äì if true, dataloader shuffles before sampling each batch\nnum_workers (int, default=0) ‚Äì number of workers\ndrop_last (bool, default=False) ‚Äì drop last batch or not\nkwargs"
  },
  {
    "objectID": "data.loader.html#main-dataloader-class",
    "href": "data.loader.html#main-dataloader-class",
    "title": "Dataloader",
    "section": "Main Dataloader Class",
    "text": "Main Dataloader Class\n\nsource\n\nDATALOADERBACKENDS\n\nCLASS relax.data.loader.DataloaderBackends (jax=&lt;class ‚Äòmain.JaxDataloader‚Äô&gt;, pytorch=&lt;class ‚Äòmain.TorchDataloader‚Äô&gt;, tensorflow=None, merlin=None)\n\n\nsource\n\n\n_DISPATCH_DATALOADER\n\nrelax.data.loader._dispatch_dataloader (backend)\n\nReturn Dataloader class based on given backend\n\n\n\n\n\n\nParameters:\n\n\n\n\nbackend (str) ‚Äì dataloader backend\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(BaseDataLoader)\n\n\n\nassert _dispatch_dataloader('jax') == JaxDataloader\nassert _dispatch_dataloader('pytorch') == TorchDataloader\n\n\nsource\n\n\nDATALOADER\n\nCLASS relax.data.loader.DataLoader (dataset, backend, batch_size=1, shuffle=False, num_workers=0, drop_last=False, **kwargs)\n\nMain Dataloader class to load Numpy data batches\n\n\n\n\n\n\nParameters:\n\n\n\n\ndataset (Dataset)\nbackend (str) ‚Äì Dataloader backend; Currently supports ['jax', 'pytorch']\nbatch_size (int, default=1) ‚Äì batch size\nshuffle (bool, default=False) ‚Äì if true, dataloader shuffles before sampling each batch\nnum_workers (int, default=0) ‚Äì number of workers\ndrop_last (bool, default=False) ‚Äì drop last batches or not\nkwargs\n\n\n\n\nA Minimum Example of using Dataloader\nWe showcase how to use Dataloader for training a simple regression model.\n\nfrom sklearn.datasets import make_regression\n\n\nX, y = make_regression(n_samples=10000, n_features=20)\ndataset = ArrayDataset(X, y.reshape(-1, 1))\nkeys = hk.PRNGSequence(0)\n\nDefine loss, step, train:\n\ndef loss(w, x, y):\n    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n\ndef step(w, x, y):\n    lr = 0.1\n    grad = jax.grad(loss)(w, x, y)\n    w -= lr * grad\n    return w\n\ndef train(dataloader: DataLoader, key: random.PRNGKey):\n    w = jax.random.normal(key, shape=(1, 20))\n    n_epochs = 10\n    for _ in range(n_epochs):\n        for x, y in dataloader:\n            w = step(w, x, y)\n    return w\n\ndef eval(dataloader: DataLoader, w):\n    err = []\n    for x, y in dataloader:\n        err.append(loss(w, x, y))\n    return np.mean(err)\n\nTrain this linear regression model via DataLoaderJax:\n\ndataloader = DataLoader(\n    dataset, 'jax', batch_size=128, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\nassert np.allclose(eval(dataloader, w), 0.)\n\nTrain this linear regression model via DataLoaderPytorch:\n\ndataloader = DataLoader(\n    dataset, 'pytorch', batch_size=128, shuffle=True)\nw = train(dataloader, next(keys)).block_until_ready()\nassert np.allclose(eval(dataloader, w), 0.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReLax",
    "section": "",
    "text": "Overview | Installation | Tutorials | Documentation | Citing ReLax"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "ReLax",
    "section": "Overview",
    "text": "Overview\nReLax (Recourse Explanation Library in Jax) is a library built on top of jax to generate counterfactual and recourse explanations for Machine Learning algorithms. By leveraging vectorization though vmap/pmap and just-in-time compilation in jax (a high-performance auto-differentiation library). ReLax offers massive speed improvements in generating individual (or local) explanations for predictions made by Machine Learning algorithms.\nSome of the key features are as follows:\n\nüèÉ Fast recourse generation via jax.jit, jax.vmap/jax.pmap.\nüöÄ Accelerated over cpu, gpu, tpu.\nü™ì Comprehensive set of recourse methods implemented for benchmarking.\nüëê Customizable API to enable the building of entire modeling\nand interpretation pipelines for new recourse algorithms."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ReLax",
    "section": "Installation",
    "text": "Installation\nThe latest ReLax release can directly be installed from PyPI:\npip install jax-relax\nor installed directly from the repository:\npip install git+https://github.com/BirkhoffG/ReLax.git \nTo futher unleash the power of accelerators (i.e., GPU/TPU), we suggest to first install this library via pip install jax-relax. Then, follow steps in the official install guidelines to install the right version for GPU or TPU."
  },
  {
    "objectID": "index.html#an-example-of-using-relax",
    "href": "index.html#an-example-of-using-relax",
    "title": "ReLax",
    "section": "An Example of using ReLax",
    "text": "An Example of using ReLax\nSee Getting Started with ReLax."
  },
  {
    "objectID": "index.html#citing-relax",
    "href": "index.html#citing-relax",
    "title": "ReLax",
    "section": "Citing ReLax",
    "text": "Citing ReLax\nTo cite this repository:\n@software{relax2023github,\n  author = {Hangzhi Guo and Xinchang Xiong and Amulya Yadav},\n  title = {{R}e{L}ax: Recourse Explanation Library in Jax},\n  url = {http://github.com/birkhoffg/ReLax},\n  version = {0.1.0},\n  year = {2023},\n}"
  },
  {
    "objectID": "ckpt_manager.html",
    "href": "ckpt_manager.html",
    "title": "Checkpoint Manager",
    "section": "",
    "text": "LOAD_CHECKPOINT\n\nload_checkpoint (ckpt_dir)\n\n\n\n\nSAVE_CHECKPOINT\n\nsave_checkpoint (state, ckpt_dir)\n\n\n\n\nCHECKPOINTMANAGER\n\nCLASS CheckpointManager (log_dir, monitor_metrics, max_n_checkpoints=3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nExample\n\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule\n\n\nkey = hk.PRNGSequence(42)\nckpt_manager = CheckpointManager(\n    log_dir='log', \n    monitor_metrics='train/train_loss_1',\n    max_n_checkpoints=3\n)\ndm = load_data('adult')\nmodule = PredictiveTrainingModule({'lr': 0.01, 'sizes': [50, 10, 50]})\nparams, opt_state = module.init_net_opt(dm, next(key))\nlogs = {'train/train_loss_1': 0.1}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=1)\nlogs = {'train/train_loss_1': 0.2}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=2)\nlogs = {'train/train_loss_1': 0.15}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=3)\nlogs = {'train/train_loss_1': 0.05}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=4)\nlogs = {'train/train_loss_1': 0.14}\nckpt_manager.update_checkpoints(params, opt_state, logs, epochs=5)\nassert ckpt_manager.n_checkpoints == len(ckpt_manager.checkpoints)\nassert ckpt_manager.checkpoints.popitem(last=True)[0] == 0.14\n\nshutil.rmtree(Path('log/epoch=1'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=2'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=3'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=4'), ignore_errors=True)\nshutil.rmtree(Path('log/epoch=5'), ignore_errors=True)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "methods.base.html",
    "href": "methods.base.html",
    "title": "Interfaces",
    "section": "",
    "text": "source\n\nBASECFMODULE\n\nCLASS relax.methods.base.BaseCFModule ()\n\nBase CF Explanation Module.\n\nsource\n\nBASECFMODULE.NAME\n\nrelax.methods.base.BaseCFModule.name ()\n\nName of the CF Explanation Module.\n\nsource\n\n\nBASECFMODULE.DATA_MODULE\n\nrelax.methods.base.BaseCFModule.data_module ()\n\nBinded DataModule.\n\nsource\n\n\nBASECFMODULE.GENERATE_CFS\n\nrelax.methods.base.BaseCFModule.generate_cfs (X, pred_fn=None)\n\nAbstract method to generate counterfactuals\n.. deprecated:: 0.1.4\n\n\n\n\n\n\nParameters:\n\n\n\n\nX (jnp.ndarray) ‚Äì Input to be explained\npred_fn (Callable, default=None) ‚Äì Predictive function\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(jnp.ndarray) ‚Äì Generated counterfactuals\n\n\n\nsource\n\n\nBASECFMODULE.HOOK_DATA_MODULE\n\nrelax.methods.base.BaseCFModule.hook_data_module (data_module)\n\nBind TabularDataModule to self._data_module.\n\nsource\n\n\n\nBASEPARAMETRICCFMODULE\n\nCLASS relax.methods.base.BaseParametricCFModule ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\nBASEPARAMETRICCFMODULE.TRAIN\n\nrelax.methods.base.BaseParametricCFModule.train (datamodule, t_configs=None, pred_fn=None)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\ndatamodule (TabularDataModule) ‚Äì data module\nt_configs (TrainingConfigs | dict, default=None) ‚Äì training configs; see docs in TrainingConfigs\npred_fn (Callable, default=None) ‚Äì predictive function\n\n\n\n\nsource\n\n\n\nBASEPREDFNCFMODULE\n\nCLASS relax.methods.base.BasePredFnCFModule ()\n\nBase class of CF Module with a predictive module.\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\nBASEPREDFNCFMODULE.PRED_FN\n\nrelax.methods.base.BasePredFnCFModule.pred_fn (X)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nX (jnp.DeviceArray) ‚Äì input X\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(jnp.DeviceArray) ‚Äì prediction"
  },
  {
    "objectID": "methods/prototype.html",
    "href": "methods/prototype.html",
    "title": "Proto CF",
    "section": "",
    "text": "source\n\nPROTOCFCONFIG\n\nCLASS relax.methods.proto.ProtoCFConfig (n_steps=1000, lr=0.01, lambda_=0.01, ae_configs={‚Äòenc_sizes‚Äô: [50, 10], ‚Äòdec_sizes‚Äô: [10, 50], ‚Äòdropout_rate‚Äô: 0.3, ‚Äòlr‚Äô: 0.03})\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nPROTOCF\n\nCLASS relax.methods.proto.ProtoCF (configs=None)\n\nBase CF Explanation Module.\n\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule, PredictiveTrainingModuleConfigs, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\n\nLoad data:\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nTrain predictive model:\n\n# load model\nparams, training_module = load_pred_model('adult')\n\n# predict function\npred_fn = lambda x, params, key: training_module.forward(\n    params, key, x, is_training=False\n)\n\nDefine ProtoCF:\n\nprotocf = ProtoCF()\n\nGenerate explanations:\n\ncf_exp = generate_cf_explanations(\n    protocf, dm, pred_fn=pred_fn, \n    t_configs=dict(\n        n_epochs=5, batch_size=128\n    ), \n    pred_fn_args=dict(\n        params=params, key=random.PRNGKey(0)\n    )\n)\n\nEvaluate explanations:\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nProtoCF\n0.8241\n0.812308\n6.427959"
  },
  {
    "objectID": "methods/diverse.html",
    "href": "methods/diverse.html",
    "title": "Diverse CF",
    "section": "",
    "text": "source\n\nDIVERSECFCONFIG\n\nCLASS relax.methods.diverse.DiverseCFConfig (n_cfs=5, n_steps=1000, lr=0.01, lambda_=0.01, seed=42)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nDIVERSECF\n\nCLASS relax.methods.diverse.DiverseCF (configs=None)\n\nBase CF Explanation Module.\n\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule, PredictiveTrainingModuleConfigs, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\nfrom relax.trainer import train_model\n\nLoad data:\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n\n/home/birk/miniconda3/envs/nbdev2/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nTrain predictive model:\n\n# load model\nparams, training_module = load_pred_model('adult')\n\n# predict function\npred_fn = training_module.pred_fn\n\nDefine DiverseCF:\n\ndiversecf = DiverseCF()\n\nGenerate explanations:\n\ncf_exp = generate_cf_explanations(\n    diversecf, dm, pred_fn=pred_fn, \n    pred_fn_args=dict(\n        params=params, rng_key=random.PRNGKey(0)\n    )\n)\n\nEvaluate explanations:\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nDiverseCF\n0.8241\n0.393932\n1.913267"
  },
  {
    "objectID": "methods/sphere.html",
    "href": "methods/sphere.html",
    "title": "Growing Sphere",
    "section": "",
    "text": "source\n\nHYPER_SPHERE_COORDINDATES\n\nrelax.methods.sphere.hyper_sphere_coordindates (rng_key, x, n_samples, high, low, p_norm=2)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nrng_key (jrand.PRNGKey) ‚Äì Random number generator key\nx (Array) ‚Äì Input instance with only continuous features. Shape: (1, n_features)\nn_samples (int) ‚Äì Number of samples\nhigh (float) ‚Äì Upper bound\nlow (float) ‚Äì Lower bound\np_norm (int, default=2) ‚Äì Norm\n\n\n\n\nsource\n\n\nCAT_SAMPLE\n\nrelax.methods.sphere.cat_sample (rng_key, cat_array_sizes, n_samples)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nrng_key (jrand.PRNGKey) ‚Äì Random number generator key\ncat_array_sizes (List[int]) ‚Äì A list of the number of categories for each categorical feature\nn_samples (int) ‚Äì Number of samples to sample\n\n\n\n\nsource\n\n\nSAMPLE_CATEGORICAL\n\nrelax.methods.sphere.sample_categorical (rng_key, col_size, n_samples)\n\n\ncandidates = cat_sample(jrand.PRNGKey(0), [2, 3], 10)\nassert candidates.shape == (10, 5)\n# No categorical features\ncandidates = cat_sample(jrand.PRNGKey(0), [], 10)\nassert jnp.concatenate([jnp.ones((10, 5)), candidates], axis=1).shape == (10, 5)\n\n\nsource\n\n\nAPPLY_IMMUTABLE\n\nrelax.methods.sphere.apply_immutable (x, cf, immutable_idx)\n\n\nsource\n\n\nGSCONFIG\n\nCLASS relax.methods.sphere.GSConfig (seed=42, n_steps=100, n_samples=1000, step_size=0.05, p_norm=2)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nGROWINGSPHERE\n\nCLASS relax.methods.sphere.GrowingSphere (configs=None)\n\nBase CF Explanation Module.\n\nTest\n\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule, PredictiveTrainingModuleConfigs, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\nfrom relax.trainer import train_model\n\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n# dm = load_data('adult',)\n\n\n# load model\nparams, training_module = load_pred_model('adult')\n\n# predict function\n# pred_fn = lambda x: training_module.forward(params, x, is_training=False)\npred_fn = lambda x, params, key: training_module.forward(\n    params, key, x, is_training=False\n)\n\n\ngs = GrowingSphere({'n_steps': 50, 'n_samples': 100, 'step_size': 0.1})\n# gs.hook_data_module(dm)\n\n\ncf_exp = generate_cf_explanations(\n    gs, dm, pred_fn=pred_fn, \n    pred_fn_args=dict(\n        params=params, key=random.PRNGKey(0)\n    )\n)\nassert not np.array_equal(cf_exp.cfs[0], cf_exp.cfs[1])\n\n\n\n\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nGrowing Sphere\n0.8241\n1.0\n6.270596"
  },
  {
    "objectID": "methods/vanilla.html",
    "href": "methods/vanilla.html",
    "title": "Vanilla CF",
    "section": "",
    "text": "source\n\nVANILLACFCONFIG\n\nCLASS relax.methods.vanilla.VanillaCFConfig (n_steps=1000, lr=0.001, lambda_=0.01)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nVANILLACF\n\nCLASS relax.methods.vanilla.VanillaCF (configs=None)\n\nBase CF Explanation Module.\n\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule, PredictiveTrainingModuleConfigs, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\nfrom relax.trainer import train_model\n\nLoad data:\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nLoad predictive model:\n\n# load model\nparams, module = load_pred_model('adult')\n\n# predict function\npred_fn = lambda x, params, key: module.forward(\n    params, key, x, is_training=False\n)\n\nDefine VanillaCF:\n\nvanillacf = VanillaCF()\n\nGenerate explanations:\n\ncf_exp = generate_cf_explanations(\n    vanillacf, dm, pred_fn=pred_fn, \n    t_configs=dict(\n        n_epochs=5, batch_size=128\n    ), \n    pred_fn_args=dict(\n        params=params, key=random.PRNGKey(0)\n    )\n)\n\nEvaluate explanations:\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nVanillaCF\n0.8241\n0.891414\n6.703655"
  },
  {
    "objectID": "methods/counternet.html",
    "href": "methods/counternet.html",
    "title": "CounterNet",
    "section": "",
    "text": "source\n\n\n\nCLASS relax.methods.counternet.CounterNetModel (m_config, name=None)\n\nCounterNet Model\n\n\n\n\n\n\nParameters:\n\n\n\n\nm_config (Dict | CounterNetModelConfigs) ‚Äì Model configs which contain configs in CounterNetModelConfigs.\nname (str, default=None) ‚Äì Name of the module."
  },
  {
    "objectID": "methods/counternet.html#counternet-model",
    "href": "methods/counternet.html#counternet-model",
    "title": "CounterNet",
    "section": "",
    "text": "source\n\n\n\nCLASS relax.methods.counternet.CounterNetModel (m_config, name=None)\n\nCounterNet Model\n\n\n\n\n\n\nParameters:\n\n\n\n\nm_config (Dict | CounterNetModelConfigs) ‚Äì Model configs which contain configs in CounterNetModelConfigs.\nname (str, default=None) ‚Äì Name of the module."
  },
  {
    "objectID": "methods/counternet.html#counternet-training-module",
    "href": "methods/counternet.html#counternet-training-module",
    "title": "CounterNet",
    "section": "CounterNet Training Module",
    "text": "CounterNet Training Module\nDefine the CounterNetTrainingModule for training CounterNetModel.\n\nsource\n\nPARTITION_TRAINABLE_PARAMS\n\nrelax.methods.counternet.partition_trainable_params (params, trainable_name)\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nPROJECT_IMMUTABLE_FEATURES\n\nrelax.methods.counternet.project_immutable_features (x, cf, imutable_idx_list)\n\n\nsource\n\n\nCOUNTERNETTRAININGMODULECONFIGS\n\nCLASS relax.methods.counternet.CounterNetTrainingModuleConfigs (lr=0.003, lambda_1=1.0, lambda_2=0.2, lambda_3=0.1)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\nsource\n\n\nCOUNTERNETTRAININGMODULE\n\nCLASS relax.methods.counternet.CounterNetTrainingModule (m_configs)\n\nHelper class that provides a standard way to create an ABC using inheritance."
  },
  {
    "objectID": "methods/counternet.html#counternet-explanation-module",
    "href": "methods/counternet.html#counternet-explanation-module",
    "title": "CounterNet",
    "section": "CounterNet Explanation Module",
    "text": "CounterNet Explanation Module\n\n\n\nCounterNet architecture\n\n\nCounterNet consists of three objectives:\n\npredictive accuracy: the predictor network should output accurate predictions \\hat{y}_x;\ncounterfactual validity: CF examples x' produced by the CF generator network should be valid (e.g.¬†\\hat{y}_{x} + \\hat{y}_{x'}=1);\nminimizing cost of change: minimal modifications should be required to change input instance x to CF example x'.\n\nThe objective function of CounterNet:\n\n\\operatorname*{argmin}_{\\mathbf{\\theta}} \\frac{1}{N}\\sum\\nolimits_{i=1}^{N}\n    \\bigg[\n    \\lambda_1 \\cdot \\! \\underbrace{\\left(y_i- \\hat{y}_{x_i}\\right)^2}_{\\text{Prediction Loss}\\ (\\mathcal{L}_1)} +\n    \\;\\lambda_2 \\cdot \\;\\; \\underbrace{\\left(\\hat{y}_{x_i}- \\left(1 - \\hat{y}_{x_i'}\\right)\\right)^2}_{\\text{Validity Loss}\\ (\\mathcal{L}_2)} \\,+\n    \\;\\lambda_3 \\cdot \\!\\! \\underbrace{\\left(x_i- x'_i\\right)^2}_{\\text{Cost of change Loss}\\ (\\mathcal{L}_3)}\n    \\bigg]\n\nCounterNet applies two-stage gradient updates to CounterNetModel for each training_step (see CounterNetTrainingModule).\n\nThe first gradient update optimizes for predictive accuracy: \\theta^{(1)} = \\theta^{(0)} - \\nabla_{\\theta^{(0)}} (\\lambda_1 \\cdot \\mathcal{L}_1).\nThe second gradient update optimizes for generating CF explanation: \\theta^{(2)}_g = \\theta^{(1)}_g - \\nabla_{\\theta^{(1)}_g} (\\mathcal \\lambda_2 \\cdot \\mathcal{L}_2 + \\lambda_3 \\cdot \\mathcal{L}_3)\n\nThe design choice of this optimizing procedure is made due to improved convergence of the model, and improved adversarial robustness of the predictor network. The CounterNet paper elaborates the design choices.\n\nsource\n\nCOUNTERNETCONFIGS\n\nCLASS relax.methods.counternet.CounterNetConfigs (enc_sizes=[50, 10], dec_sizes=[10], exp_sizes=[50, 50], dropout_rate=0.3, lr=0.003, lambda_1=1.0, lambda_2=0.2, lambda_3=0.1)\n\nConfigurator of CounterNet.\n\n\n\n\n\n\nParameters:\n\n\n\n\nenc_sizes (List[int], default=[50, 10]) ‚Äì Sequence of layer sizes for encoder network.\ndec_sizes (List[int], default=[10]) ‚Äì Sequence of layer sizes for predictor.\nexp_sizes (List[int], default=[50, 50]) ‚Äì Sequence of layer sizes for CF generator.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nlr (float, default=0.003) ‚Äì Learning rate for training CounterNet.\nlambda_1 (float, default=1.0) ‚Äì \\lambda_1 for balancing the prediction loss \\mathcal{L}_1.\nlambda_2 (float, default=0.2) ‚Äì \\lambda_2 for balancing the prediction loss \\mathcal{L}_2.\nlambda_3 (float, default=0.1) ‚Äì \\lambda_3 for balancing the prediction loss \\mathcal{L}_3.\n\n\n\n\nsource\n\n\nCOUNTERNET\n\nCLASS relax.methods.counternet.CounterNet (m_configs=None)\n\nAPI for CounterNet Explanation Module.\n\n\n\n\n\n\nParameters:\n\n\n\n\nm_configs (dict | CounterNetConfigs, default=None) ‚Äì configurator of hyperparamters; see CounterNetConfigs\n\n\n\n\nBasic usage of CounterNet\nPrepare data:\n\nfrom relax.data import load_data\n\n\ndm = load_data(\"adult\", data_configs=dict(sample_frac=0.1))\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n\n\nDefine CounterNet:\n\ncounternet = CounterNet()\n\n\nassert isinstance(counternet, BaseParametricCFModule)\nassert isinstance(counternet, BaseCFModule)\nassert isinstance(counternet, BasePredFnCFModule)\nassert hasattr(counternet, 'pred_fn')\n\nTrain the model:\n\nt_configs = dict(n_epochs=1, batch_size=128)\ncounternet.train(dm, t_configs=t_configs)\n\nPredict labels\n\nX, y = dm.test_dataset[:]\ny_pred = counternet.pred_fn(X)\nassert y_pred.shape == (len(y), 1)\n\nGenerate a CF explanation for a given x.\n\nx, _ = dm.test_dataset[0]\ncf = counternet.generate_cf(x)\nassert x.shape == cf.shape\nassert cf.shape == (29,)\n\nGenerate CF explanations for given x.\n\nX, _ = dm.test_dataset[:]\ncfs = counternet.generate_cfs(X)\nassert X.shape == cfs.shape"
  },
  {
    "objectID": "methods/cchvae.html",
    "href": "methods/cchvae.html",
    "title": "CCHVAE",
    "section": "",
    "text": "source\n\nCCHVAECONFIGS\n\nCLASS relax.methods.cchvae.CCHVAEConfigs (enc_sizes=[20, 16, 14, 12], dec_sizes=[12, 14, 16, 20], encoded_size=5, lr=0.001, max_steps=100, n_search_samples=300, step_size=0.1, seed=0)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nenc_sizes (List[int], default=[20, 16, 14, 12]) ‚Äì Encoder hidden sizes\ndec_sizes (List[int], default=[12, 14, 16, 20]) ‚Äì Decoder hidden sizes\nencoded_size (int, default=5) ‚Äì Encoded size\nlr (float, default=0.001) ‚Äì Learning rate\nmax_steps (int, default=100) ‚Äì Max steps\nn_search_samples (int, default=300) ‚Äì Number of generated candidate counterfactuals.\nstep_size (float, default=0.1) ‚Äì Step size\nseed (int, default=0) ‚Äì Seed for random number generator\n\n\n\n\nsource\n\n\nCCHVAE\n\nCLASS relax.methods.cchvae.CCHVAE (configs=None)\n\nBase CF Explanation Module.\n\nTest\n\nfrom relax.module import PredictiveTrainingModule, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\n\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n\n\n# load model\nparams, training_module = load_pred_model('adult')\n\n# predict function\n# pred_fn = lambda x: training_module.forward(params, x, is_training=False)\npred_fn = lambda x, params, key: training_module.forward(\n    params, key, x, is_training=False\n)\n\n\ncchvae_test = CCHVAE()\ncchvae_test.train(dm)\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n  param = init(shape, dtype)\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/relax/_ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n  warnings.warn(\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00&lt;00:00, 493.63batch/s, train/loss=0.552]\n\n\n\ncf_exp = generate_cf_explanations(\n    cchvae_test, dm, pred_fn, pred_fn_args=dict(\n        params=params, key=random.PRNGKey(0)\n    )\n)\n\n\n\n\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nC-CHVAE\n0.8241\n1.0\n4.124321"
  },
  {
    "objectID": "methods/07_vaecf.html",
    "href": "methods/07_vaecf.html",
    "title": "VAECF",
    "section": "",
    "text": "# pred_prob = jrand.uniform(jrand.PRNGKey(0), (6, ))\n# pred_prob = jnp.array([0.3, 0.1, 0.8, 0.8, .99, .99])\npred_prob = jnp.array([.99, .99, 0.3, 0.1, 0.1, 0.1])\n\ny = jnp.array([1, 1, 0, 0, 0, 0])\ntarget = jnp.array([-1])\n\ntempt_1, tempt_0 = pred_prob[y == 1], pred_prob[y == 0]\nvalidity_loss_1 = hindge_embedding_loss(tempt_1 - (1. - tempt_1), target, 0.165) + \\\n    hindge_embedding_loss(1. - 2 * tempt_0, target, 0.165)\n\n\n# tempt_1 = hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165)\n# tempt_0 = hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165)\n# validity_loss = jnp.where(\n#     y == 1, tempt_1, tempt_0\n# )\n\ntempt_1 = jnp.where(\n    y == 1,\n    hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165, reduction=None),\n    0\n).sum() / y.sum()\ntempt_0 = jnp.where(\n    y == 0,\n    hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165, reduction=None),\n    0\n).sum() / (y.shape[0] - y.sum())\n# validity_loss = jnp.where(\n#     y == 1,\n#     hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165, reduction=None),\n#     hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165, reduction=None)\n# )\n# validity_loss_2 = jnp.sum(validity_loss)\nvalidity_loss_2 = tempt_1 + tempt_0\n\n\nhindge_embedding_loss(tempt_1 - (1. - tempt_1), target, 0.165)\n\nDeviceArray(1.165, dtype=float32)\n\n\n\nvalidity_loss_1, validity_loss_2\n\n(DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32))\n\n\n\nsource\n\nVAECFCONFIGS\n\nCLASS relax.methods.vaecf.VAECFConfigs (enc_sizes=[20, 16, 14, 12, 5], dec_sizes=[12, 14, 16, 20], dropout_rate=0.1, lr=0.001, mu_samples=50, validity_reg=42.0)\n\nConfigurator of VAECFModule.\n\n\n\n\n\n\nParameters:\n\n\n\n\nenc_sizes (List[int], default=[20, 16, 14, 12, 5]) ‚Äì Sequence of Encoder layer sizes.\ndec_sizes (List[int], default=[12, 14, 16, 20]) ‚Äì Sequence of Decoder layer sizes.\ndropout_rate (float, default=0.1) ‚Äì Dropout rate.\nlr (float, default=0.001) ‚Äì Learning rate.\nmu_samples (int, default=50) ‚Äì Number of samples for mu.\nvalidity_reg (float, default=42.0) ‚Äì Regularization for validity.\n\n\n\n\nsource\n\n\nVAECF\n\nCLASS relax.methods.vaecf.VAECF (m_config=None)\n\nBase CF Explanation Module.\n\nTest\n\nfrom relax.trainer import train_model\nfrom relax.data import load_data\nfrom relax.module import PredictiveTrainingModule, load_pred_model\nfrom relax.evaluate import _AuxPredFn, generate_cf_explanations, benchmark_cfs\n\n\ndm = load_data('adult', data_configs=dict(sample_frac=0.1))\n\n\n# load model\nparams, training_module = load_pred_model('adult')\n\n# predict function\npred_fn = training_module.pred_fn\n\n\nvaecf = VAECF()\n# vaecf.train(dm, t_config, pred_fn)\n\n\ncf_exp = generate_cf_explanations(\n    vaecf, dm, pred_fn, pred_fn_args=dict(\n        params=params, rng_key=random.PRNGKey(0)\n    )\n)\n\nVAECF contains parametric models. Starts training before generating explanations...\n\n\n/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/relax/_ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n  warnings.warn(\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00&lt;00:00, 81.43batch/s, train/loss=nan]    \n\n\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nC-CHVAE\n0.8241\n0.18241\n7.563876"
  },
  {
    "objectID": "methods/clue.html",
    "href": "methods/clue.html",
    "title": "CLUE",
    "section": "",
    "text": "source\n\nDECODER\n\nCLASS relax.methods.vaecf.Decoder (sizes, input_size, dropout=0.1)\n\nBase class for Haiku modules.\nA Haiku module is a lightweight container for variables and other modules. Modules typically define one or more ‚Äúforward‚Äù methods (e.g.¬†__call__) which apply operations combining user input and module parameters.\nModules must be initialized inside a :func:transform call.\nFor example:\n\n\n\nclass AddModule(hk.Module): ‚Ä¶ def call(self, x): ‚Ä¶ w = hk.get_parameter(‚Äúw‚Äù, [], init=jnp.ones) ‚Ä¶ return x + w\n\n\n\n\n\n\ndef forward_fn(x): ‚Ä¶ mod = AddModule() ‚Ä¶ return mod(x)\n\n\n\n\n\n\nforward = hk.transform(forward_fn) x = 1. rng = None params = forward.init(rng, x) print(forward.apply(params, None, x)) 2.0\n\n\n\n\nsource\n\n\nENCODER\n\nCLASS relax.methods.vaecf.Encoder (sizes, dropout=0.1)\n\nBase class for Haiku modules.\nA Haiku module is a lightweight container for variables and other modules. Modules typically define one or more ‚Äúforward‚Äù methods (e.g.¬†__call__) which apply operations combining user input and module parameters.\nModules must be initialized inside a :func:transform call.\nFor example:\n\n\n\nclass AddModule(hk.Module): ‚Ä¶ def call(self, x): ‚Ä¶ w = hk.get_parameter(‚Äúw‚Äù, [], init=jnp.ones) ‚Ä¶ return x + w\n\n\n\n\n\n\ndef forward_fn(x): ‚Ä¶ mod = AddModule() ‚Ä¶ return mod(x)\n\n\n\n\n\n\nforward = hk.transform(forward_fn) x = 1. rng = None params = forward.init(rng, x) print(forward.apply(params, None, x)) 2.0\n\n\n\n\nsource\n\n\nKL_DIVERGENCE\n\nrelax.methods.clue.kl_divergence (p, q, eps=7.62939453125e-06)\n\n\nsource\n\n\nVAEGAUSSCATCONFIGS\n\nCLASS relax.methods.clue.VAEGaussCatConfigs (lr=0.001, enc_sizes=[20, 16, 14, 12], dec_sizes=[12, 14, 16, 20], dropout_rate=0.1)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nlr (float, default=0.001) ‚Äì Learning rate.\nenc_sizes (List[int], default=[20, 16, 14, 12]) ‚Äì Sequence of Encoder layer sizes.\ndec_sizes (List[int], default=[12, 14, 16, 20]) ‚Äì Sequence of Decoder layer sizes.\ndropout_rate (float, default=0.1) ‚Äì Dropout rate.\n\n\n\n\nsource\n\n\nVAEGAUSSCAT\n\nCLASS relax.methods.clue.VAEGaussCat (m_configs=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nCLUECONFIGS\n\nCLASS relax.methods.clue.CLUEConfigs (enc_sizes=[20, 16, 14, 12], dec_sizes=[12, 14, 16, 20], encoded_size=5, lr=0.001, max_steps=500, step_size=0.01, vae_n_epochs=10, vae_batch_size=128, seed=0)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nenc_sizes (List[int], default=[20, 16, 14, 12]) ‚Äì Sequence of Encoder layer sizes.\ndec_sizes (List[int], default=[12, 14, 16, 20]) ‚Äì Sequence of Decoder layer sizes.\nencoded_size (int, default=5) ‚Äì Encoded size\nlr (float, default=0.001) ‚Äì Learning rate\nmax_steps (int, default=500) ‚Äì Max steps\nstep_size (float, default=0.01) ‚Äì Step size\nvae_n_epochs (int, default=10) ‚Äì Number of epochs for VAE\nvae_batch_size (int, default=128) ‚Äì Batch size for VAE\nseed (int, default=0) ‚Äì Seed for random number generator\n\n\n\n\nsource\n\n\nCLUE\n\nCLASS relax.methods.clue.CLUE (m_config=None)\n\nBase CF Explanation Module.\n\nTest\n\nfrom relax.module import PredictiveTrainingModule, load_pred_model\nfrom relax.evaluate import generate_cf_explanations, benchmark_cfs\n\n\ndata_name = 'cancer'\ndm = load_data(data_name) # ,) data_configs=dict(sample_frac=0.1))\n\n\n# load model\nparams, training_module = load_pred_model(data_name)\npred_fn = training_module.pred_fn\n\n\ndl = dm.train_dataloader(128)\nX, y = next(iter(dl))\n\n\nclue = CLUE()\nclue.train(dm)\n\n/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.8/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n  param = init(shape, dtype)\n/home/birk/code/ReLax/relax/_ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n  warnings.warn(\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;00:00, 125.86batch/s, train/loss=2.1]\n\n\n\nclue_test = CLUE()\nclue_test.params = clue.params\nclue_test.module = clue.module\n\n\ncf_exp = generate_cf_explanations(\n    clue_test, dm, pred_fn, pred_fn_args=dict(\n        params=params, rng_key=random.PRNGKey(0)\n    ), t_configs=dict(\n        n_epochs=5, batch_size=256\n    )\n)\n\n\n\n\n\nbenchmark_cfs([cf_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nbreast cancer\nCLUE\n0.909091\n0.608392\n9.972657"
  },
  {
    "objectID": "docs.html",
    "href": "docs.html",
    "title": "Docs",
    "section": "",
    "text": "source\n\nCALLOUTDOCUMENT\n\nCLASS relax.docs.CalloutDocument (tbl)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nCUSTOMIZEDMARKDOWNRENDERER\n\nCLASS relax.docs.CustomizedMarkdownRenderer (sym, name=None, title_level=3)\n\nDisplaying documents of functions, classes, haiku.module, and BaseParser.\nCustomizedMarkdownRenderer is the customized markdown render for the ReLax documentation site. We can use it to displaying documents of functions, classes, haiku.module, and BaseParser.\nWe can display documentations for functions:\n\ndef validate_config(\n    configs: Dict|BaseParser, # A configuration of the model/data.\n    config_cls: BaseParser # The desired configuration class.\n) -&gt; BaseParser:\n    \"\"\"Return a valid configuration object.\"\"\"\n    ...\n\nCustomizedMarkdownRenderer(validate_config)\n\n\n\nVALIDATE_CONFIG\n\nvalidate_config (configs, config_cls)\n\nReturn a valid configuration object.\n\n\n\n\n\n\nParameters:\n\n\n\n\nconfigs (Dict | BaseParser) ‚Äì A configuration of the model/data.\nconfig_cls (BaseParser) ‚Äì The desired configuration class.\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(BaseParser)\n\n\n\n\n\n\nWe can display documentations for classes:\n\nclass VanillaCF:\n    \"\"\"VanillaCF Explanation of the model.\"\"\"\n\n    def __init__(\n        self, \n        configs: Dict|BaseParser=None # A configuration of the model.\n    ): ...\nCustomizedMarkdownRenderer(VanillaCF)\n\n\nsource\n\nVANILLACF\n\nCLASS relax.methods.vanilla.VanillaCF (configs=None)\n\nVanillaCF Explanation of the model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nconfigs (Dict | BaseParser, default=None) ‚Äì A configuration of the model.\n\n\n\n\n\n\n\nWe can display documentations for BaseParser:\n\nclass VanillaCFConfig(BaseParser):\n    \"\"\"Configuration for the `Model`.\"\"\"\n\n    lr: float = Field(1e-3, description=\"Learning rate.\")\n    n_steps: int = Field(100, description=\"Number of iteration steps.\")\n\nCustomizedMarkdownRenderer(VanillaCFConfig)\n\n\nsource\n\nVANILLACFCONFIG\n\nCLASS relax.methods.vanilla.VanillaCFConfig (lr=0.001, n_steps=100)\n\nConfiguration for the Model.\n\n\n\n\n\n\nParameters:\n\n\n\n\nlr (float, default=0.001) ‚Äì Learning rate.\nn_steps (int, default=100) ‚Äì Number of iteration steps.\n\n\n\n\n\n\n\nclass Model(hk.Module):\n    \"\"\"A basic predictive model for binary classification.\"\"\"\n\n    def __init__(\n        self,\n        sizes: List[int], # Sequence of layer sizes.\n        dropout_rate: float = 0.3,  # Dropout rate.\n        name: Optional[str] = None,  # Name of the module.\n    ):\n        ...\n\nCustomizedMarkdownRenderer(Model)\n\n\n\nMODEL\n\nCLASS Model (sizes, dropout_rate=0.3, name=None)\n\nA basic predictive model for binary classification.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsizes (List[int]) ‚Äì Sequence of layer sizes.\ndropout_rate (float, default=0.3) ‚Äì Dropout rate.\nname (Optional[str], default=None) ‚Äì Name of the module."
  },
  {
    "objectID": "plotting.html",
    "href": "plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "plotting.html#example",
    "href": "plotting.html#example",
    "title": "Plotting",
    "section": "Example",
    "text": "Example\nWe first use VanillaCF to generate Explanation.\n\nfrom relax.utils import load_json\nfrom relax.module import PredictiveTrainingModule\nfrom relax.trainer import train_model\nfrom relax.evaluate import generate_cf_explanations\nfrom relax.methods import VanillaCF\n\n\n# load configs\nconfigs = load_json('assets/configs/data_configs/adult.json')\nm_configs = configs['mlp_configs']\ndata_configs = configs['data_configs']\nt_configs = dict(n_epochs=10, batch_size=256)\n\n# load data and model\ndm = TabularDataModule(data_configs)\nmodel = PredictiveTrainingModule(m_configs)\n\n# train predictive models\nparams, opt_state = train_model(model, dm, t_configs)\npred_fn = lambda x, params, prng_key: model.forward(params, prng_key, x, is_training=False)\n\n# generate explanations\nexp = generate_cf_explanations(\n    VanillaCF(), dm, pred_fn, \n    pred_fn_args=dict(params=params, prng_key=random.PRNGKey(0))\n)\n\nTo visualize individual explanation:\n\n# this visualize the differences between `exp.X[0]` and `exp.cfs[0]`\nfig = individual_plot(exp, idx=0)\n\n\n\n\nTo analyze the entire explanation distribution:\n\nfig = summary_plot(exp, sample_frac=0.01)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\n\n\n\n\n\nParameters:\n\n\n\n\nconfigs (dict | BaseParser) ‚Äì A configuration of the model/dataset.\nconfig_cls (BaseParser) ‚Äì The desired configuration class.\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(BaseParser)\n\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']\n\n\nsource\n\n\n\n\nrelax.utils.show_doc (sym)\n\nSame functionality as nbdev.show_doc, but provide additional support for BaseParser.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsym ‚Äì Symbol to document"
  },
  {
    "objectID": "utils.html#configurations",
    "href": "utils.html#configurations",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\nrelax.utils.validate_configs (configs, config_cls)\n\nreturn a valid configuration object.\n\n\n\n\n\n\nParameters:\n\n\n\n\nconfigs (dict | BaseParser) ‚Äì A configuration of the model/dataset.\nconfig_cls (BaseParser) ‚Äì The desired configuration class.\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(BaseParser)\n\n\nWe define a configuration object (which inherent BaseParser) to manage training/model/data configurations. validate_configs ensures to return the designated configuration object.\nFor example, we define a configuration object LearningConfigs:\n\nclass LearningConfigs(BaseParser):\n    lr: float\n\nA configuration can be LearningConfigs, or the raw data in dictionary.\n\nconfigs_dict = dict(lr=0.01)\n\nvalidate_configs will return a designated configuration object.\n\nconfigs = validate_configs(configs_dict, LearningConfigs)\nassert type(configs) == LearningConfigs\nassert configs.lr == configs_dict['lr']\n\n\nsource\n\n\n\n\nrelax.utils.show_doc (sym)\n\nSame functionality as nbdev.show_doc, but provide additional support for BaseParser.\n\n\n\n\n\n\nParameters:\n\n\n\n\nsym ‚Äì Symbol to document"
  },
  {
    "objectID": "utils.html#categorical-normalization",
    "href": "utils.html#categorical-normalization",
    "title": "Utils",
    "section": "Categorical normalization",
    "text": "Categorical normalization\n\nsource\n\nCAT_NORMALIZE\n\nrelax.utils.cat_normalize (cf, cat_arrays, cat_idx, hard=False)\n\nEnsure generated counterfactual explanations to respect one-hot encoding constraints.\n\n\n\n\n\n\nParameters:\n\n\n\n\ncf (jnp.ndarray) ‚Äì Unnormalized counterfactual explanations [n_samples, n_features]\ncat_arrays (List[List[str]]) ‚Äì A list of a list of each categorical feature name\ncat_idx (int) ‚Äì Index that starts categorical features\nhard (bool, default=False) ‚Äì If True, return one-hot vectors; If False, return probability normalized via softmax\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(jnp.ndarray)\n\n\nA tabular data point is encoded as x = [\\underbrace{x_{0}, x_{1}, ..., x_{m}}_{\\text{cont features}},\n\\underbrace{x_{m+1}^{c=1},..., x_{m+p}^{c=1}}_{\\text{cat feature (1)}}, ...,\n\\underbrace{x_{k-q}^{c=i},..., x_{k}^{^{c=i}}}_{\\text{cat feature (i)}}]\ncat_normalize ensures the generated cf that satisfy the categorical constraints, i.e., \\sum_j x^{c=i}_j=1, x^{c=i}_j &gt; 0, \\forall c=[1, ..., i].\ncat_idx is the index of the first categorical feature. In the above example, cat_idx is m+1.\nFor example, let‚Äôs define a valid input data point:\n\nx = np.array([\n    [1., .9, 'dog', 'gray'],\n    [.3, .3, 'cat', 'gray'],\n    [.7, .1, 'fish', 'red'],\n    [1., .6, 'dog', 'gray'],\n    [.1, .2, 'fish', 'yellow']\n])\n\nWe encode the categorical features via the OneHotEncoder in sklearn.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ncat_idx = 2\nohe = OneHotEncoder(sparse=False)\nx_cat = ohe.fit_transform(x[:, cat_idx:])\nx_cont = x[:, :cat_idx].astype(float)\nx_transformed = np.concatenate(\n    (x_cont, x_cat), axis=1\n)\n\nIf hard=True, the categorical features are in one-hot format.\n\ncfs = np.random.randn(*x_transformed.shape)\ncfs = cat_normalize(cfs, ohe.categories_, \n    cat_idx=cat_idx, hard=True)\ncfs[:1]\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nArray([[-1.1198508 , -0.57364744,  0.        ,  1.        ,  0.        ,\n         1.        ,  0.        ,  0.        ]], dtype=float32)\n\n\nIf hard=False, the categorical features are normalized via softmax function.\n\ncfs = np.random.randn(*x_transformed.shape)\ncfs = cat_normalize(cfs, ohe.categories_, \n    cat_idx=cat_idx, hard=False)\nn_cat_feats = len(ohe.categories_)\n\nassert (cfs[:, cat_idx:].sum(axis=1) - n_cat_feats * jnp.ones(len(cfs))).sum() &lt; 1e-6"
  },
  {
    "objectID": "utils.html#vectorization-utils",
    "href": "utils.html#vectorization-utils",
    "title": "Utils",
    "section": "Vectorization Utils",
    "text": "Vectorization Utils\n\nsource\n\nAUTO_RESHAPING\n\nrelax.utils.auto_reshaping (reshape_argname)\n\nDecorator to automatically reshape function‚Äôs input into (1, k), and out to input‚Äôs shape.\nThis decorator ensures that the specified input argument and output of a function are in the same shape. This is particularly useful when using jax.vamp.\n\n@auto_reshaping('x')\ndef f_vmap(x): return x * jnp.ones((10,))\nassert vmap(f_vmap)(jnp.ones((10, 10))).shape == (10, 10)"
  },
  {
    "objectID": "utils.html#training-utils",
    "href": "utils.html#training-utils",
    "title": "Utils",
    "section": "Training Utils",
    "text": "Training Utils\n\nsource\n\nMAKE_MODEL\n\nrelax.utils.make_model (m_configs, model)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nm_configs (Dict[str, Any])\nmodel (hk.Module) ‚Äì model configs\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(hk.Transformed)\n\n\n\nsource\n\n\nMAKE_HK_MODULE\n\nrelax.utils.make_hk_module (module, *args, **kargs)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nmodule (hk.Module) ‚Äì haiku module\nargs\nkargs\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(hk.Transformed) ‚Äì haiku module arguments haiku module arguments\n\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nINIT_NET_OPT\n\nrelax.utils.init_net_opt (net, opt, X, key)\n\n\nsource\n\n\nGRAD_UPDATE\n\nrelax.utils.grad_update (grads, params, opt_state, opt)\n\n\nsource\n\n\nCHECK_CAT_INFO\n\nrelax.utils.check_cat_info (method)"
  },
  {
    "objectID": "utils.html#helper-functions",
    "href": "utils.html#helper-functions",
    "title": "Utils",
    "section": "Helper functions",
    "text": "Helper functions\n\nsource\n\nLOAD_JSON\n\nrelax.utils.load_json (f_name)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\nf_name (str)\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(Dict[str, Any]) ‚Äì file name"
  },
  {
    "objectID": "utils.html#loss-functions",
    "href": "utils.html#loss-functions",
    "title": "Utils",
    "section": "Loss Functions",
    "text": "Loss Functions\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\nBINARY_CROSS_ENTROPY\n\nrelax.utils.binary_cross_entropy (preds, labels)\n\nPer-sample binary cross-entropy loss function.\n\n\n\n\n\n\nParameters:\n\n\n\n\npreds (jnp.DeviceArray) ‚Äì The predicted values\nlabels (jnp.DeviceArray) ‚Äì The ground-truth labels\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(jnp.DeviceArray) ‚Äì Loss value\n\n\n\nsource\n\n\nSIGMOID\n\nrelax.utils.sigmoid (x)"
  },
  {
    "objectID": "utils.html#metrics",
    "href": "utils.html#metrics",
    "title": "Utils",
    "section": "Metrics",
    "text": "Metrics\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\nPROXIMITY\n\nrelax.utils.proximity (x, cf)\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nDIST\n\nrelax.utils.dist (x, cf, ord=2)\n\n&lt;string&gt;:1: DeprecationWarning: jax.numpy.DeviceArray is deprecated. Use jax.Array.\n\nsource\n\n\nACCURACY\n\nrelax.utils.accuracy (y_true, y_pred)"
  },
  {
    "objectID": "utils.html#config",
    "href": "utils.html#config",
    "title": "Utils",
    "section": "Config",
    "text": "Config\n\nsource\n\nGET_CONFIG\n\nrelax.utils.get_config ()"
  },
  {
    "objectID": "evaluate.html",
    "href": "evaluate.html",
    "title": "Evaluate",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "evaluate.html#parallelism-strategy",
    "href": "evaluate.html#parallelism-strategy",
    "title": "Evaluate",
    "section": "Parallelism Strategy",
    "text": "Parallelism Strategy\n\nsource\n\nBASEGENERATIONSTRATEGY\n\nCLASS relax.evaluate.BaseGenerationStrategy ()\n\nBase class for mapping strategy.\n\nsource\n\n\nITERATIVEGENERATIONSTRATEGY\n\nCLASS relax.evaluate.IterativeGenerationStrategy ()\n\nIterativly generate counterfactuals.\n\nsource\n\n\nVMAPGENERATIONSTRATEGY\n\nCLASS relax.evaluate.VmapGenerationStrategy ()\n\nGenerate counterfactuals via jax.vmap.\n\nsource\n\n\nPMAPGENERATIONSTRATEGY\n\nCLASS relax.evaluate.PmapGenerationStrategy (n_devices=None, strategy=‚Äòauto‚Äô, **kwargs)\n\nBase class for mapping strategy.\n\n\n\n\n\n\nParameters:\n\n\n\n\nn_devices (int, default=None) ‚Äì Number of devices. If None, use all available devices\nstrategy (str, default=auto) ‚Äì Strategy to generate counterfactuals\nkwargs\n\n\n\n\nsource\n\n\nBATCHEDVMAPGENERATIONSTRATEGY\n\nCLASS relax.evaluate.BatchedVmapGenerationStrategy (batch_size)\n\nAuto-batching for generate counterfactuals via jax.vmap.\n\nsource\n\n\nBATCHEDPMAPGENERATIONSTRATEGY\n\nCLASS relax.evaluate.BatchedPmapGenerationStrategy (batch_size, n_devices=None)\n\nAuto-batching for generate counterfactuals via jax.vmap.\n\nos.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n\nw = jrand.normal(jrand.PRNGKey(0), (100, 100))\nX = jrand.normal(jrand.PRNGKey(0), (1000, 100))\n\n@jit\ndef pred_fn(x): return jnp.dot(x, w.T)\n\ndef f(x, pred_fn=None, **kwargs):\n    return pred_fn(x)\n\niter_gen = IterativeGenerationStrategy()\nvmap_gen = VmapGenerationStrategy()\npmap_gen = PmapGenerationStrategy()\nbvmap_gen = BatchedVmapGenerationStrategy(128)\nbpmap_gen = BatchedPmapGenerationStrategy(128)\n\n\ncf_iter = iter_gen(f, X, pred_fn=pred_fn).block_until_ready()\n\n\ncf_vmap = vmap_gen(f, X, pred_fn=pred_fn).block_until_ready()\n\n\ncf_pmap = pmap_gen(f, X, pred_fn=pred_fn).block_until_ready()\n\n\ncf_bvmap = bvmap_gen(f, X, pred_fn=pred_fn).block_until_ready()\n\n\n# check when batch_size &gt; X.shape[0]\n_bvmap_gen = BatchedVmapGenerationStrategy(1280)\n_cf_bvmap = _bvmap_gen(f, X, pred_fn=pred_fn).block_until_ready()\nassert jnp.allclose(cf_bvmap, _cf_bvmap, atol=1e-4)\n\n\ncf_bpmap = bpmap_gen(f, X, pred_fn=pred_fn).block_until_ready()\n\n\nassert jnp.allclose(cf_iter, cf_vmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_bvmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_pmap, atol=1e-4)\nassert jnp.allclose(cf_iter, cf_bpmap, atol=1e-4)\n\n\nsource\n\n\nSTRATEGYFACTORY\n\nCLASS relax.evaluate.StrategyFactory ()\n\nFactory class for Parallelism Strategy."
  },
  {
    "objectID": "evaluate.html#generating-cf-explanation-results",
    "href": "evaluate.html#generating-cf-explanation-results",
    "title": "Evaluate",
    "section": "Generating CF Explanation Results",
    "text": "Generating CF Explanation Results\n\nsource\n\nGENERATE_CF_EXPLANATIONS\n\nrelax.evaluate.generate_cf_explanations (cf_module, datamodule, pred_fn=None, strategy=‚Äòvmap‚Äô, t_configs=None, pred_fn_args=None)\n\nGenerate CF explanations.\n\n\n\n\n\n\nParameters:\n\n\n\n\ncf_module (BaseCFModule) ‚Äì CF Explanation Module\ndatamodule (TabularDataModule) ‚Äì Data Module\npred_fn (callable, default=None) ‚Äì Predictive function\nstrategy (str | BaseGenerationStrategy, default=vmap) ‚Äì Parallelism Strategy for generating CFs\nt_configs (TrainingConfigs, default=None) ‚Äì training configs for BaseParametricCFModule\npred_fn_args (dict, default=None) ‚Äì auxiliary arguments for pred_fn\n\n\n\n\n\n\n\n\n\nReturns:\n\n\n\n‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ(Explanation)\n\n\nThe pred_fn in generate_cf_explanations is a model‚Äôs prediction function. The general format is y = pred_fn(x, **pred_fn_args). If pred_fn is not parameterized by other variables (except input x), then pred_fn_args is set to None, which is the default setting. Otherwise, you should pass these argument as a dict.\nFor example, we have a simple linear function\n\ndef linear_pred_fn(x: jnp.DeviceArray, params: jnp.DeviceArray):\n    return x @ params\n\nTo pass linear_pred_fn to generate_cf_explanations, we can either create an auxiliary function of linear_pred_fn, or pass params into pred_fn_args.\nAssuming we now have the input x and params:\n\nx = jax.random.normal(random.PRNGKey(0), shape=(5, 10)) # input\nparams = jnp.ones((10, 1)) # params\n\nWARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\nCreate an auxillary function (Not recommended)\n\naux_linear_pred_fn = lambda x: linear_pred_fn(x, params)\nexplanations = generate_cf_explanations(\n    cf_module, datamodule, aux_linear_pred_fn\n)\nThis approach could work, but if params is changed, explanations.pred_fn might not work as expected.\n\nPass params into pred_fn_args\n\nexplanations = generate_cf_explanations(\n    cf_module, datamodule, linear_pred_fn, \n    pred_fn_args=dict(params=params)\n)\nThis is a recommended approach as we will deepcopy params inside generate_cf_explanations.\nThe pred_fn in explanations only takes x: jnp.DeviceArray as an input. For example, to make predictions, we use\ny = explanations.pred_fn(x)"
  },
  {
    "objectID": "evaluate.html#evaluating-metrics",
    "href": "evaluate.html#evaluating-metrics",
    "title": "Evaluate",
    "section": "Evaluating Metrics",
    "text": "Evaluating Metrics\n\nsource\n\nBASEEVALMETRICS\n\nCLASS relax.evaluate.BaseEvalMetrics (name=None)\n\nBase evaluation metrics class.\n\nsource\n\n\nPREDICTIVEACCURACY\n\nCLASS relax.evaluate.PredictiveAccuracy (name=‚Äòaccuracy‚Äô)\n\nCompute the accuracy of the predict function.\n\nsource\n\n\nVALIDITY\n\nCLASS relax.evaluate.Validity (name=‚Äòvalidity‚Äô)\n\nCompute fraction of input instances on which CF explanation methods output valid CF examples.\n\ninputs = jnp.array([\n    [0, 1], [1, 0], [1, 1]])\ncfs_1 = jnp.array([\n    [0, 0], [0, 0], [0, 1]])\ncfs_2 = jnp.array([\n    [1, 0], [1, -2], [0, 0]])\nassert _compute_proximity(inputs, cfs_1) == 1.0\nassert _compute_proximity(inputs, cfs_2) == 2.0\n\n\nsource\n\n\nPROXIMITY\n\nCLASS relax.evaluate.Proximity (name=‚Äòproximity‚Äô)\n\nCompute L1 norm distance between input datasets and CF examples divided by the number of features.\n\nsource\n\n\nSPARSITY\n\nCLASS relax.evaluate.Sparsity (name=‚Äòsparsity‚Äô)\n\nCompute the number of feature changes between input datasets and CF examples.\n\nsource\n\n\nMANIFOLDDIST\n\nCLASS relax.evaluate.ManifoldDist (n_neighbors=1, p=2, name=‚Äòmanifold_dist‚Äô)\n\nCompute the L1 distance to the n-nearest neighbor for all CF examples.\n\nsource\n\n\nRUNTIME\n\nCLASS relax.evaluate.Runtime (name=‚Äòruntime‚Äô)\n\nGet the running time to generate CF examples.\n\nsource\n\n\nCOMPUTE_SO_SPARSITY\n\nrelax.evaluate.compute_so_sparsity (cf_results, threshold=2.0)\n\n\nsource\n\n\nCOMPUTE_SO_PROXIMITY\n\nrelax.evaluate.compute_so_proximity (cf_results, threshold=2.0)\n\n\nsource\n\n\nCOMPUTE_SO_VALIDITY\n\nrelax.evaluate.compute_so_validity (cf_results, threshold=2.0)"
  },
  {
    "objectID": "evaluate.html#benchmarking",
    "href": "evaluate.html#benchmarking",
    "title": "Evaluate",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nsource\n\nEVALUATE_CFS\n\nrelax.evaluate.evaluate_cfs (cf_exp, metrics=None, return_dict=True, return_df=False)\n\n\n\n\n\n\n\nParameters:\n\n\n\n\ncf_exp (Explanation) ‚Äì CF Explanations\nmetrics (Iterable[Union[str, BaseEvalMetrics]], default=None) ‚Äì A list of Metrics. Can be str or a subclass of BaseEvalMetrics\nreturn_dict (bool, default=True) ‚Äì return a dictionary or not (default: True)\nreturn_df (bool, default=False) ‚Äì return a pandas Dataframe or not (default: False)\n\n\n\n\nsource\n\n\nBENCHMARK_CFS\n\nrelax.evaluate.benchmark_cfs (cf_results_list, metrics=None)"
  },
  {
    "objectID": "evaluate.html#how-to-evaluate-a-cf-explanation-module",
    "href": "evaluate.html#how-to-evaluate-a-cf-explanation-module",
    "title": "Evaluate",
    "section": "How to evaluate a CF Explanation Module",
    "text": "How to evaluate a CF Explanation Module\n\nfrom relax.module import PredictiveTrainingModule\nfrom relax.trainer import train_model\nfrom relax.utils import load_json\n\n\nconfigs = load_json('assets/configs/data_configs/adult.json')\nm_configs = configs['mlp_configs']\ndata_configs = configs['data_configs']\ndata_configs['sample_frac'] = 0.1\n\nt_configs = {\n    'n_epochs': 10,\n    'monitor_metrics': 'val/val_loss',\n    'seed': 42,\n    \"batch_size\": 256\n}\n\nWe first train a model\n\ntraining_module = PredictiveTrainingModule(m_configs)\ndm = TabularDataModule(data_configs)\n\nparams, opt_state = train_model(\n    training_module, \n    dm, \n    t_configs\n)\npred_fn = lambda x, params, prng_key: \\\n    training_module.forward(params, prng_key, x, is_training=False)\n\nEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:01&lt;00:00, 53.81batch/s, train/train_loss_1=0.0791]\n\n\nNow, we can start to benchmark different methods\n\nfrom relax.methods import VanillaCF, CounterNet\n\nGenerate CF explanations for VanillaCF\n\nvanillacf = VanillaCF(dict(n_steps=1000, lr=0.001))\nvanillacf_exp = generate_cf_explanations(\n    vanillacf, dm, pred_fn,\n    pred_fn_args=dict(params=params, prng_key=random.PRNGKey(0))\n)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:10&lt;00:00, 92.93it/s]\n\n\n\nassert vanillacf_exp.cf_name == vanillacf.name\nassert vanillacf_exp.dataset_name == dm.data_name\nassert vanillacf_exp.X.shape == vanillacf_exp.cfs.shape\nassert vanillacf_exp.pred_fn(vanillacf_exp.X).shape == vanillacf_exp.y.shape\n\nGenerate CF explanations for CounterNet\n\ncounternet = CounterNet()\ncounternet_exp = generate_cf_explanations(counternet, dm, pred_fn=None)\n\nCounterNet contains parametric models. Starts training before generating explanations...\n\n\nEpoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:03&lt;00:00, 58.07batch/s, train/train_loss_1=0.0657, train/train_loss_2=0.000985, train/train_loss_3=0.0963]\n\n\nNote that CounterNet contains a predictive module, so we set pred_fn=None\n\nassert counternet_exp.cf_name == counternet.name\nassert counternet_exp.dataset_name == dm.data_name\nassert counternet_exp.X.shape == counternet_exp.cfs.shape\nassert counternet_exp.pred_fn(counternet_exp.X).shape == counternet_exp.y.shape\nassert counternet_exp.pred_fn == counternet.pred_fn\n\nIf cf_module is a subclass of BasePredFnCFModule (e.g., CounterNet), the pred_fn in Explanation will be set to cf_module.pred_fn, and the pred_fn argument passed generate_cf_explanations will be ignored.\n\ncounternet_exp_1 = generate_cf_explanations(counternet, dm, pred_fn=pred_fn)\nassert counternet_exp_1.pred_fn != pred_fn\nassert counternet_exp_1.pred_fn == counternet.pred_fn\n\nCounterNet contains parametric models. Starts training before generating explanations...\n\n\nEpoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:02&lt;00:00, 64.02batch/s, train/train_loss_1=0.0713, train/train_loss_2=0.000427, train/train_loss_3=0.0944]\n\n\nNow, we can compute metrics for benchmarking different CF explanation methods.\n\nevaluate_cfs(vanillacf_exp, return_df=True)[1]\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nVanillaCF\n0.822012\n0.93674\n7.62256\n\n\n\n\n\n\n\n\nevaluate_cfs(counternet_exp, return_df=True)[1]\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nCounterNet\n0.831347\n0.958605\n5.9374576\n\n\n\n\n\n\n\n\nbenchmark_cfs([vanillacf_exp, counternet_exp])\n\n\n\n\n\n\n\n\n\nacc\nvalidity\nproximity\n\n\n\n\nadult\nVanillaCF\n0.822012\n0.936740\n7.62256\n\n\nCounterNet\n0.831347\n0.958605\n5.9374576"
  }
]