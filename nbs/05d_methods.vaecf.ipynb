{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.vae_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.nets import MLP\n",
    "from cfnet.training_module import BaseTrainingModule, BaseCFExplanationModule\n",
    "from cfnet.utils import validate_configs, sigmoid, grad_update, cat_normalize, make_model, init_net_opt\n",
    "from cfnet.utils import accuracy, proximity\n",
    "from cfnet.methods.diverse import hinge_loss\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Encoder(hk.Module):\n",
    "    def __call__(self, x: jnp.DeviceArray, is_training: bool):\n",
    "        mean = MLP([20, 16, 14, 12, 5], 0.3, name=\"EncoderMean\")(x, is_training)\n",
    "        var = MLP([20, 16, 14, 12, 5], 0.3, name=\"EncoderVar\")(x, is_training)\n",
    "        logvar = 0.5 + var\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size: int = input_size\n",
    "\n",
    "    def __call__(self, z: jnp.DeviceArray, is_training: bool):\n",
    "        mean = MLP([12, 14, 16, 20, self.input_size], 0.3, name=\"DecoderMean\")(z, is_training)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VariationalAutoEncoder(hk.Module):\n",
    "    def __init__(self, input_size, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(input_size)\n",
    "    # def encode(self, x: jnp.DeviceArray, is_training: bool = True):\n",
    "    #     mean = MLP([20, 16, 14, 12, 5], 0.3, name=\"EncoderMean\")(x, is_training)\n",
    "    #     var = MLP([20, 16, 14, 12, 5], 0.3, name=\"EncoderVar\")(x, is_training)\n",
    "    #     logvar = 0.5 + var\n",
    "    #     return mean, logvar\n",
    "    \n",
    "    # def decode(self, z: jnp.DeviceArray, input_size: int, is_training: bool = True):\n",
    "    #     mean = MLP([12, 14, 16, 20, input_size], 0.3, name=\"DecoderMean\")(z, is_training)\n",
    "    #     return mean\n",
    "\n",
    "    def sample_latent(self, mean, logvar):\n",
    "        eps = jax.random.normal(hk.next_rng_key(), logvar.shape)\n",
    "        return mean + jnp.sqrt(logvar) * eps\n",
    "\n",
    "    def __call__(\n",
    "        self, x: jnp.DeviceArray, y_target: jnp.DeviceArray, mc_samples: int = 50, is_training: bool = True\n",
    "    ) -> jnp.DeviceArray:\n",
    "        assert mc_samples >= 1, f\"`mc_samples` should be >= 1 (got `mc_samples={mc_samples}`)\"\n",
    "        input_size = x.shape[-1]\n",
    "        y_target = y_target.reshape(y_target.shape[0], 1)\n",
    "        # TODO: vmap on Pytree\n",
    "        em, ev = self.encoder(jnp.concatenate((x, y_target), 1), is_training)\n",
    "        res = {\n",
    "            'em': em, 'ev': ev, 'z': list(), 'x_pred': list(), 'mc_samples': mc_samples\n",
    "        }\n",
    "        for _ in range(mc_samples):\n",
    "            z = self.sample_latent(em, ev)\n",
    "            x_pred = self.decoder(jnp.concatenate((z, y_target), 1), is_training)\n",
    "            res['z'].append(z); res['x_pred'].append(x_pred)\n",
    "        return res\n",
    "\n",
    "    # def compute_elbo(self, x, y_target):\n",
    "    #     y_target = y_target.reshape(y_target.shape[0], 1)\n",
    "    #     em, ev = self.encode(jnp.concatenate((x, y_target), 1))\n",
    "    #     kl_divergence = 0.5 * jnp.mean(em**2 + ev - jnp.log(ev) - 1, axis=1)\n",
    "\n",
    "    #     z = self.sample_latent(em, ev)\n",
    "    #     dm = self.decode(jnp.concatenate((z, y_target), 1))\n",
    "    #     log_px_z = jnp.array([0.])\n",
    "    #     x_pred = dm\n",
    "    #     return jnp.mean(log_px_z), jnp.mean(kl_divergence), x, x_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAECFConfigs(BaseParser):\n",
    "    validity_reg: float\n",
    "    lr: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def make_vae() -> hk.Transformed:\n",
    "    # example:\n",
    "    # net = make_model(PredictiveModel)\n",
    "    # params = net.init(...)\n",
    "    def model_fn(x, y, mc_samples=1, is_training: bool = True):\n",
    "        return VariationalAutoEncoder(x.shape[-1])(x, y, mc_samples, is_training)\n",
    "\n",
    "    return hk.transform(model_fn)\n",
    "\n",
    "\n",
    "def init_net_opt_vae(\n",
    "    net: hk.Transformed,\n",
    "    opt: optax.GradientTransformation,\n",
    "    X: jnp.DeviceArray,\n",
    "    y: jnp.DeviceArray,\n",
    "    key: random.PRNGKey\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    X = device_put(X)\n",
    "    params = net.init(key, X, y, is_training=True)\n",
    "    opt_state = opt.init(params)\n",
    "    return params, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE_CF(BaseTrainingModule, BaseCFExplanationModule):\n",
    "    name = \"VAECF\"\n",
    "\n",
    "    def __init__(self, m_configs, pred_fn):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.configs = validate_configs(m_configs, VAECFConfigs)\n",
    "        self.pred_fn = pred_fn\n",
    "        self.net = make_vae()\n",
    "        self.opt = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    def init_net_opt(self, dm, key):\n",
    "        self.update_cat_info(dm)\n",
    "        X = dm.get_sample_X()\n",
    "        y = 1. - self.pred_fn(X)\n",
    "        params, opt_state = init_net_opt_vae(\n",
    "            self.net, self.opt, X=X, y=y, key=key)\n",
    "        return params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self', 'is_training'])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        y_target = 1. - self.pred_fn(x)\n",
    "        y_target = jnp.round(y_target)\n",
    "        res = self.net.apply(params, rng_key, x, y_target, mc_samples=1, is_training=is_training)\n",
    "        cf = res['x_pred'][0]\n",
    "        cf_y = self.pred_fn(cf)\n",
    "        return cf, cf_y\n",
    "\n",
    "    def predict(self, params, rng_key, x):\n",
    "        return self.pred_fn(x)\n",
    "\n",
    "    def generate_cfs(self, X: chex.ArrayBatched, params: Optional[hk.Params], rng_key: Optional[random.PRNGKey]) -> chex.ArrayBatched:\n",
    "        cfs, _ = self.forward(params, rng_key, X, is_training=False)\n",
    "        return cat_normalize(cfs, self.cat_arrays, self.cat_idx, hard=True)\n",
    "    \n",
    "    def compute_loss(self, out, x, y_target):\n",
    "        em = out['em']\n",
    "        ev = out['ev']\n",
    "        z = out['z']\n",
    "        dm = out['x_pred']\n",
    "        mc_samples = out['mc_samples']\n",
    "\n",
    "        # KL divergence\n",
    "        kl_divergence = 0.5 * jnp.mean(em**2 + ev - jnp.log(ev) - 1, axis=1)\n",
    "        \n",
    "        validity_loss, recon_err = 0., 0.\n",
    "\n",
    "        for i in range(mc_samples):\n",
    "            x_pred = dm[i]\n",
    "            # Proximity: L1 Loss\n",
    "            recon_err += - jnp.sum(jnp.abs(x - x_pred), axis=1)\n",
    "\n",
    "            # Sum to 1 over the categorical indexes of a feature\n",
    "            for col in self.cat_arrays:\n",
    "                cat_end_idx = self.cat_idx + len(col)\n",
    "                temp = - \\\n",
    "                    jnp.abs(1.0 - jnp.sum(x_pred[:, self.cat_idx: cat_end_idx], axis=1))\n",
    "                recon_err += temp\n",
    "\n",
    "            # validity\n",
    "            cf_y = self.pred_fn(x_pred)\n",
    "            validity_loss += hinge_loss(cf_y, y_target)\n",
    "\n",
    "        recon_err = recon_err / mc_samples\n",
    "        validity_loss = - self.configs.validity_reg * validity_loss / mc_samples\n",
    "        return - jnp.mean(recon_err - kl_divergence) - validity_loss\n",
    "\n",
    "    def loss_fn(self, params, rng_key, x, y_target):\n",
    "        out = self.net.apply(params, rng_key, x, y_target, is_training=True)\n",
    "        loss = self.compute_loss(out, x, y_target)\n",
    "        return loss\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self'])\n",
    "    def _training_step(self, params, opt_state, rng_key, batch):\n",
    "        x, _ = batch\n",
    "        y_target = 1. - self.pred_fn(x)\n",
    "\n",
    "        grads = jax.grad(self.loss_fn)(params, rng_key, x, y_target)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    def training_step(self, params: hk.Params, opt_state: optax.OptState, rng_key: random.PRNGKey, batch: Tuple[jnp.array, jnp.array]) -> Tuple[hk.Params, optax.OptState]:\n",
    "        return self._training_step(params, opt_state, rng_key, batch)\n",
    "\n",
    "    def validation_step(self, params: hk.Params, rng_key: random.PRNGKey, batch: Tuple[jnp.array, jnp.array]) -> Dict[str, Any]:\n",
    "        x, y = batch\n",
    "        y_target = 1. - self.pred_fn(x)\n",
    "        cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "\n",
    "        logs = {\n",
    "            'val/validity': accuracy(y_target, cf_y),\n",
    "            'val/proximity': proximity(x, cf)\n",
    "        }\n",
    "        self.log_dict(logs)\n",
    "        return logs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfnet.train import train_model\n",
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.training_module import PredictiveTrainingModule\n",
    "from cfnet.evaluate import generate_cf_results_cfnet, evaluate_cfs\n",
    "from cfnet.utils import load_json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_configs = load_json('assets/configs/data_configs/credit_card.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TabularDataModule(adult_configs['data_configs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = PredictiveTrainingModule(adult_configs['mlp_configs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n",
      "/home/birk/code/cfnet/cfnet/_ckpt_manager.py:43: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  warnings.warn(\"`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\")\n",
      "Epoch 9: 100%|██████████| 88/88 [00:01<00:00, 75.31batch/s, train/train_loss_1=0.0836]\n"
     ]
    }
   ],
   "source": [
    "pred_params, _ = train_model(\n",
    "    pred_model, dm, t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_params = deepcopy(pred_params)\n",
    "pred_fn = lambda x: pred_model.forward(_params, random.PRNGKey(0), x, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_configs = {\n",
    "    'validity_reg': 42.0, 'lr': 0.001\n",
    "}\n",
    "vae = VAE_CF(vae_configs, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 88/88 [00:00<00:00, 137.47batch/s, val/proximity=7.5908494, val/validity=0.8614457]\n"
     ]
    }
   ],
   "source": [
    "params, _ = train_model(\n",
    "    vae, dm, t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_results = generate_cf_results_cfnet(vae, dm, params, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('', 'VAECF'): {'acc': 0.8167999982833862,\n",
       "  'validity': 0.8565333485603333,\n",
       "  'proximity': 8.435470581054688}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cfs(cf_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nbdev2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f450d1f6173d6d96822a65433f2c9ed0b856da53e162bc0666cdf9645c72e1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
