{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "> Define useful nn in `haiku`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.utils import validate_configs, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DenseBlock(hk.Module):\n",
    "    def __init__(self,\n",
    "                output_size: int,\n",
    "                dropout_rate: float = 0.3,\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        dropout_rate = self.dropout_rate if is_training else 0.0\n",
    "        # he_uniform\n",
    "        w_init = hk.initializers.VarianceScaling(2.0, 'fan_in', 'uniform')\n",
    "        x = hk.Linear(self.output_size, w_init=w_init)(x)\n",
    "        x = jax.nn.leaky_relu(x)\n",
    "        x = hk.dropout(hk.next_rng_key(), dropout_rate, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MLP(hk.Module):\n",
    "    def __init__(self,\n",
    "                sizes: List[int],\n",
    "                dropout_rate: float = 0.3,\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.sizes = sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        for size in self.sizes:\n",
    "            x = DenseBlock(size, self.dropout_rate)(x, is_training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class PredictiveMLPConfigs(BaseParser):\n",
    "    sizes: List[int]\n",
    "    dropout_rate: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictiveMLP(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_config: Dict[str, Any],\n",
    "        name: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, PredictiveMLPConfigs) #PredictiveModelConfigs(**m_config)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        is_training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        x = MLP(sizes=self.configs.sizes, dropout_rate=self.configs.dropout_rate)(x, is_training)\n",
    "        x = hk.Linear(1)(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictivConvNet(hk.Module):\n",
    "    def __init__(\n",
    "        self, name = None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True):\n",
    "        x = hk.Sequential([\n",
    "            hk.Conv2D(output_channels=32, kernel_shape=(3, 3), padding=\"SAME\"),\n",
    "            jax.nn.leaky_relu,\n",
    "            hk.Conv2D(output_channels=64, kernel_shape=(3, 3), padding=\"SAME\"),\n",
    "            jax.nn.leaky_relu,\n",
    "            hk.Flatten(),\n",
    "            hk.Linear(256),\n",
    "            jax.nn.leaky_relu,\n",
    "            hk.Linear(1),\n",
    "        ])(x)\n",
    "        return sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of cfnet.datasets failed: Traceback (most recent call last):\n",
      "  File \"/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
      "    ref.__class__ = new\n",
      "  File \"pydantic/main.py\", line 358, in pydantic.main.BaseModel.__setattr__\n",
      "ValueError: \"MNISTDataConfigs\" object has no field \"__class__\"\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape: (13007, 28, 28); train_y.shape: (13007,) \n",
      "test_X.shape: (2163, 28, 28); test_y.shape: (2163,) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/home/birk/mambaforge-pypy3/envs/cfnet/lib/python3.7/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictiv_conv_net/conv2_d': {'b': (32,), 'w': (3, 3, 28, 32)},\n",
       " 'predictiv_conv_net/conv2_d_1': {'b': (64,), 'w': (3, 3, 32, 64)},\n",
       " 'predictiv_conv_net/linear': {'b': (256,), 'w': (1792, 256)},\n",
       " 'predictiv_conv_net/linear_1': {'b': (1,), 'w': (256, 1)}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "from cfnet.utils import make_model\n",
    "from cfnet.datasets import MNISTDataModule\n",
    "\n",
    "net = make_model(None, PredictivConvNet)\n",
    "key = hk.PRNGSequence(42)\n",
    "\n",
    "dm = MNISTDataModule({})\n",
    "\n",
    "xs = random.normal(next(key), (1000, 28, 28))\n",
    "\n",
    "params = net.init(next(key), dm.get_sample_X(), is_training=True)\n",
    "y = net.apply(params, next(key), xs, is_training=True)\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.get_sample_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class CounterNetMLPConfigs(BaseParser):\n",
    "    enc_sizes: List[int]\n",
    "    dec_sizes: List[int]\n",
    "    exp_sizes: List[int]\n",
    "    dropout_rate: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CounterNetMLP(hk.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                m_config: Dict[str, Any],\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, CounterNetMLPConfigs)\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        # encoder\n",
    "        z = MLP(self.configs.enc_sizes, self.configs.dropout_rate, name=\"Encoder\")(x, is_training)\n",
    "\n",
    "        # prediction\n",
    "        pred = MLP(self.configs.dec_sizes, self.configs.dropout_rate, name=\"Predictor\")(z, is_training)\n",
    "        y_hat = hk.Linear(1, name='Predictor')(pred)\n",
    "        y_hat = sigmoid(y_hat)\n",
    "\n",
    "        # explain\n",
    "        z_exp = jnp.concatenate((z, pred), axis=-1)\n",
    "        cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name=\"Explainer\")(z_exp, is_training)\n",
    "        cf = hk.Linear(input_shape, name='Explainer')(cf)\n",
    "        return y_hat, cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class ConvExplainer(hk.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        flaten_shape, \n",
    "        z_shape_3d,\n",
    "        name: Optional[str] = None\n",
    "    ):\n",
    "        self.flatten_shape = flaten_shape\n",
    "        self.z_shape_3d = z_shape_3d\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, x): \n",
    "        x = hk.Linear(self.flatten_shape[-1], name=\"Explainer\")(x)\n",
    "        x = x.reshape(self.z_shape_3d)\n",
    "        x = jax.nn.leaky_relu(x)\n",
    "        x = hk.Conv2DTranspose(output_channels=4, kernel_shape=(3, 3), padding='SAME')(x)\n",
    "        x = jax.nn.leaky_relu(x)\n",
    "        x = hk.Conv2DTranspose(output_channels=1, kernel_shape=(3, 3), padding='SAME')(x)\n",
    "        x = jnp.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CounterNetConv(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        x = jnp.expand_dims(x, axis=-1)\n",
    "        # encoder\n",
    "        z = hk.Sequential([\n",
    "            hk.Conv2D(output_channels=4, kernel_shape=(3, 3), padding=\"SAME\"),\n",
    "            jax.nn.leaky_relu, \n",
    "            hk.Conv2D(output_channels=16, kernel_shape=(3, 3), padding=\"SAME\"),\n",
    "            jax.nn.leaky_relu,\n",
    "        ], name='Encoder')(x)\n",
    "        z_shape_3d = z.shape\n",
    "        z = hk.Flatten()(z)\n",
    "        z_shape_flattened = z.shape\n",
    "\n",
    "        # prediction\n",
    "        pred = hk.Sequential([\n",
    "            hk.Linear(50),\n",
    "            jax.nn.leaky_relu, \n",
    "        ], name='Predictor')(z)\n",
    "        y_hat = hk.Linear(1, name='Predictor')(pred)\n",
    "        y_hat = sigmoid(y_hat)\n",
    "\n",
    "        # explain\n",
    "        z_exp = jnp.concatenate((z, pred), axis=-1)\n",
    "        # z_exp = hk.Linear(z_shape_flattened[-1], name=\"Explainer\")(z_exp)\n",
    "        # z_exp = z_exp.reshape(z_shape_3d)\n",
    "\n",
    "        # cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name=\"Explainer\")(z_exp)\n",
    "        cf = ConvExplainer(z_shape_flattened, z_shape_3d, name='Explainer')(z_exp)\n",
    "        return y_hat, cf.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "m_configs = {\n",
    "    \"enc_sizes\": [10, 10],\n",
    "    \"dec_sizes\": [10, 10],\n",
    "    \"exp_sizes\": [10, 10],\n",
    "    \"dropout_rate\": 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counter_net_mlp/Encoder/dense_block/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Encoder/dense_block_1/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Explainer/dense_block/linear': {'b': (10,), 'w': (20, 10)},\n",
       " 'counter_net_mlp/Explainer/dense_block_1/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Explainer_1': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Predictor/dense_block/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Predictor/dense_block_1/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_mlp/Predictor_1': {'b': (1,), 'w': (10, 1)}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfnet.utils import make_model\n",
    "\n",
    "net = make_model(m_configs, CounterNetMLP)\n",
    "key = hk.PRNGSequence(42)\n",
    "\n",
    "xs = random.normal(next(key), (1000, 10))\n",
    "\n",
    "params = net.init(next(key), xs, is_training=True)\n",
    "y = net.apply(params, next(key), xs, is_training=True)\n",
    "jax.tree_map(lambda x: x.shape, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('cfnet')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
