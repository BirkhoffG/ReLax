{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "> Define useful nn in `haiku`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.utils import validate_configs, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DenseBlock(hk.Module):\n",
    "    def __init__(self,\n",
    "                output_size: int,\n",
    "                dropout_rate: float = 0.3,\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        dropout_rate = self.dropout_rate if is_training else 0.0\n",
    "        # he_uniform\n",
    "        w_init = hk.initializers.VarianceScaling(2.0, 'fan_in', 'uniform')\n",
    "        x = hk.Linear(self.output_size, w_init=w_init)(x)\n",
    "        x = jax.nn.leaky_relu(x)\n",
    "        x = hk.dropout(hk.next_rng_key(), dropout_rate, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MLP(hk.Module):\n",
    "    def __init__(self,\n",
    "                sizes: List[int],\n",
    "                dropout_rate: float = 0.3,\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.sizes = sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        for size in self.sizes:\n",
    "            x = DenseBlock(size, self.dropout_rate)(x, is_training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class PredictiveModelConfigs(BaseParser):\n",
    "    sizes: List[int]\n",
    "    dropout_rate: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictiveModel(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_config: Dict[str, Any],\n",
    "        name: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, PredictiveModelConfigs) #PredictiveModelConfigs(**m_config)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        is_training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        x = MLP(sizes=self.configs.sizes, dropout_rate=self.configs.dropout_rate)(x, is_training)\n",
    "        x = hk.Linear(1)(x)\n",
    "        x = sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformed(init=<function without_state.<locals>.init_fn at 0x7f8b944037a0>, apply=<function without_state.<locals>.apply_fn at 0x7f8b94403830>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forward(x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "    return PredictiveModel([10, 10], dropout_rate=0.3)(x, is_training)\n",
    "\n",
    "net = hk.transform(model_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CounterNetModelConfigs(BaseParser):\n",
    "    enc_sizes: List[int]\n",
    "    dec_sizes: List[int]\n",
    "    exp_sizes: List[int]\n",
    "    dropout_rate: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetModel(hk.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                m_config: Dict[str, Any],\n",
    "                name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, CounterNetModelConfigs)\n",
    "\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                is_training: bool = True) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        # encoder\n",
    "        z = MLP(self.configs.enc_sizes, self.configs.dropout_rate, name=\"Encoder\")(x, is_training)\n",
    "\n",
    "        # prediction\n",
    "        pred = MLP(self.configs.dec_sizes, self.configs.dropout_rate, name=\"Predictor\")(z, is_training)\n",
    "        y_hat = hk.Linear(1, name='Predictor')(pred)\n",
    "        y_hat = sigmoid(y_hat)\n",
    "\n",
    "        # explain\n",
    "        z_exp = jnp.concatenate((z, pred), axis=-1)\n",
    "        cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name=\"Explainer\")(z_exp, is_training)\n",
    "        cf = hk.Linear(input_shape, name='Explainer')(cf)\n",
    "        return y_hat, cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "    params = {\n",
    "        \"enc_sizes\": [10, 10],\n",
    "        \"dec_sizes\": [10, 10],\n",
    "        \"exp_sizes\": [10, 10],\n",
    "        \"dropout_rate\": 0.3\n",
    "    }\n",
    "    return CounterNetModel(**params)(x, is_training)\n",
    "\n",
    "net = hk.transform(model_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'counter_net_model/Encoder/dense_block/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_model/Encoder/dense_block_1/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_model/Explainer/dense_block/linear': {'b': (10,), 'w': (20, 10)},\n",
       " 'counter_net_model/Explainer/dense_block_1/linear': {'b': (10,),\n",
       "  'w': (10, 10)},\n",
       " 'counter_net_model/Predictor/dense_block/linear': {'b': (10,), 'w': (10, 10)},\n",
       " 'counter_net_model/Predictor/dense_block_1/linear': {'b': (10,),\n",
       "  'w': (10, 10)},\n",
       " 'counter_net_model/linear': {'b': (1,), 'w': (10, 1)},\n",
       " 'counter_net_model/linear_1': {'b': (10,), 'w': (10, 10)}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = hk.PRNGSequence(42)\n",
    "\n",
    "xs = random.normal(next(key), (1000, 10))\n",
    "\n",
    "params = net.init(next(key), xs, is_training=True)\n",
    "y = net.apply(params, next(key), xs, is_training=True)\n",
    "jax.tree_map(lambda x: x.shape, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
