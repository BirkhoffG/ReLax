{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "> Modules used for defining model architecture and training procedure, which are passed to `train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.data import TabularDataModule\n",
    "from relax.logger import TensorboardLogger\n",
    "from relax.utils import validate_configs, sigmoid, accuracy, init_net_opt, grad_update, make_hk_module, show_doc as show_parser_doc\n",
    "from fastcore.basics import patch\n",
    "from functools import partial\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n",
    "\n",
    "Networks are [haiku.module](https://dm-haiku.readthedocs.io/en/latest/api.html#common-modules), \n",
    "which define model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseNetwork(ABC):\n",
    "    \"\"\"BaseNetwork needs a `is_training` argument\"\"\"\n",
    "\n",
    "    def __call__(self, *, is_training: bool):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "class DenseBlock(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: int,  # Output dimensionality.\n",
    "        dropout_rate: float = 0.3,  # Dropout rate.\n",
    "        name: str | None = None,  # Name of the Module\n",
    "    ):\n",
    "        \"\"\"A `DenseBlock` consists of a dense layer, followed by Leaky Relu and a dropout layer.\"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        dropout_rate = self.dropout_rate if is_training else 0.0\n",
    "        # he_uniform\n",
    "        w_init = hk.initializers.VarianceScaling(2.0, \"fan_in\", \"uniform\")\n",
    "        x = hk.Linear(self.output_size, w_init=w_init)(x)\n",
    "        x = jax.nn.leaky_relu(x)\n",
    "        x = hk.dropout(hk.next_rng_key(), dropout_rate, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L28){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseBlock.__init__\n",
       "\n",
       ">      DenseBlock.__init__ (output_size:int, dropout_rate:float=0.3,\n",
       ">                           name:Union[str,NoneType]=None)\n",
       "\n",
       "A `DenseBlock` consists of a dense layer, followed by Leaky Relu and a dropout layer.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| output_size | int |  | Output dimensionality. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | str \\| None | None | Name of the Module |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L28){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DenseBlock.__init__\n",
       "\n",
       ">      DenseBlock.__init__ (output_size:int, dropout_rate:float=0.3,\n",
       ">                           name:Union[str,NoneType]=None)\n",
       "\n",
       "A `DenseBlock` consists of a dense layer, followed by Leaky Relu and a dropout layer.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| output_size | int |  | Output dimensionality. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | str \\| None | None | Name of the Module |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DenseBlock.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "class MLP(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes: Iterable[int],  # Sequence of layer sizes.\n",
    "        dropout_rate: float = 0.3,  # Dropout rate.\n",
    "        name: str | None = None,  # Name of the Module\n",
    "    ):\n",
    "        \"\"\"A `MLP` consists of a list of `DenseBlock` layers.\"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.sizes = sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        for size in self.sizes:\n",
    "            x = DenseBlock(size, self.dropout_rate)(x, is_training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L51){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP.__init__\n",
       "\n",
       ">      MLP.__init__ (sizes:Iterable[int], dropout_rate:float=0.3,\n",
       ">                    name:Union[str,NoneType]=None)\n",
       "\n",
       "A `MLP` consists of a list of `DenseBlock` layers.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sizes | Iterable[int] |  | Sequence of layer sizes. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | str \\| None | None | Name of the Module |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L51){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP.__init__\n",
       "\n",
       ">      MLP.__init__ (sizes:Iterable[int], dropout_rate:float=0.3,\n",
       ">                    name:Union[str,NoneType]=None)\n",
       "\n",
       "A `MLP` consists of a list of `DenseBlock` layers.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sizes | Iterable[int] |  | Sequence of layer sizes. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | str \\| None | None | Name of the Module |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLP.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class PredictiveModelConfigs(BaseParser):\n",
    "    \"\"\"Configurator of `PredictiveModel`.\"\"\"\n",
    "\n",
    "    sizes: List[int]  # Sequence of layer sizes.\n",
    "    dropout_rate: float = 0.3  # Dropout rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "class PredictiveModel(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes: List[int], # Sequence of layer sizes.\n",
    "        dropout_rate: float = 0.3,  # Dropout rate.\n",
    "        name: Optional[str] = None,  # Name of the module.\n",
    "    ):\n",
    "        \"\"\"A basic predictive model for binary classification.\"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.configs = PredictiveModelConfigs(\n",
    "            sizes=sizes, dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        x = MLP(sizes=self.configs.sizes, dropout_rate=self.configs.dropout_rate)(\n",
    "            x, is_training\n",
    "        )\n",
    "        x = hk.Linear(1)(x)\n",
    "        x = jax.nn.sigmoid(x)\n",
    "        # x = sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PredictiveModel.__init__\n",
       "\n",
       ">      PredictiveModel.__init__ (sizes:List[int], dropout_rate:float=0.3,\n",
       ">                                name:Union[str,NoneType]=None)\n",
       "\n",
       "A basic predictive model for binary classification.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sizes | List[int] |  | Sequence of layer sizes. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | Optional[str] | None | Name of the module. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/module.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PredictiveModel.__init__\n",
       "\n",
       ">      PredictiveModel.__init__ (sizes:List[int], dropout_rate:float=0.3,\n",
       ">                                name:Union[str,NoneType]=None)\n",
       "\n",
       "A basic predictive model for binary classification.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sizes | List[int] |  | Sequence of layer sizes. |\n",
       "| dropout_rate | float | 0.3 | Dropout rate. |\n",
       "| name | Optional[str] | None | Name of the module. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PredictiveModel.__init__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `make_hk_module` to create a `haiku.Transformed` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.utils import make_hk_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = make_hk_module(PredictiveModel, sizes=[50, 20, 10], dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = hk.PRNGSequence(42)\n",
    "xs = random.normal(next(key), (1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then initalize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = net.init(next(key), xs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view model's structure via `jax.tree_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictive_model/linear': {'b': (1,), 'w': (10, 1)},\n",
       " 'predictive_model/mlp/dense_block/linear': {'b': (50,), 'w': (10, 50)},\n",
       " 'predictive_model/mlp/dense_block_1/linear': {'b': (20,), 'w': (50, 20)},\n",
       " 'predictive_model/mlp/dense_block_2/linear': {'b': (10,), 'w': (20, 10)}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model output is produced via `apply` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net.apply(params, next(key), xs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more usage of `haiku.module`, please refer to \n",
    "[Haiku documentation](https://dm-haiku.readthedocs.io/en/latest/api.html#haiku-fundamentals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Modules API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class BaseTrainingModule(ABC):\n",
    "    pass\n",
    "\n",
    "@patch(as_prop=True)\n",
    "def logger(\n",
    "    self:BaseTrainingModule\n",
    ") -> TensorboardLogger | None:\n",
    "    \"\"\"A logger property\"\"\"\n",
    "    pass\n",
    "\n",
    "@patch\n",
    "def log(self:BaseTrainingModule, \n",
    "        name: str, # Name of the log\n",
    "        value: Any # value\n",
    "    ) -> None:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseTrainingModule(ABC):\n",
    "    hparams: Dict[str, Any]\n",
    "    logger: TensorboardLogger | None\n",
    "\n",
    "    def save_hyperparameters(self, configs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        self.hparams = deepcopy(configs)\n",
    "        return self.hparams\n",
    "\n",
    "    def init_logger(self, logger: TensorboardLogger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def log(self, name: str, value: Any):\n",
    "        self.log_dict({name: value})\n",
    "\n",
    "    def log_dict(self, dictionary: Dict[str, Any]):\n",
    "        if self.logger:\n",
    "            # self.logger.log({k: np.asarray(v) for k, v in dictionary.items()})\n",
    "            self.logger.log_dict(dictionary)\n",
    "        else:\n",
    "            raise ValueError(\"Logger has not been initliazed.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def init_net_opt(\n",
    "        self, data_module: TabularDataModule, key: random.PRNGKey\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validation_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Dict[str, Any]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "class PredictiveTrainingModuleConfigs(BaseParser):\n",
    "    lr: float = Field(description='Learning rate.')\n",
    "    sizes: List[int] = Field(description='Sequence of layer sizes.')\n",
    "    dropout_rate: float = Field(0.3, description='Dropout rate') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PredictiveTrainingModule(BaseTrainingModule):\n",
    "    def __init__(self, m_configs: Dict | PredictiveTrainingModuleConfigs):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.configs = validate_configs(m_configs, PredictiveTrainingModuleConfigs)\n",
    "        self.net = make_hk_module(\n",
    "            PredictiveModel, \n",
    "            sizes=self.configs.sizes, \n",
    "            dropout_rate=self.configs.dropout_rate\n",
    "        )\n",
    "        self.opt = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        return self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "\n",
    "    def init_net_opt(self, data_module, key):\n",
    "        X, _ = data_module.train_dataset[:100]\n",
    "        params, opt_state = init_net_opt(\n",
    "            self.net, self.opt, X=X, key=key\n",
    "        )\n",
    "        return params, opt_state\n",
    "\n",
    "    def loss_fn(self, params, rng_key, batch, is_training: bool = True):\n",
    "        x, y = batch\n",
    "        y_pred = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        return jnp.mean(vmap(optax.l2_loss)(y_pred, y))\n",
    "\n",
    "    # def _training_step(self, params, opt_state, rng_key, batch):\n",
    "    #     grads = jax.grad(self.loss_fn)(params, rng_key, batch)\n",
    "    #     upt_params, opt_state = grad_update(grads, params, opt_state, self.opt)\n",
    "    #     return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step(self, params, opt_state, rng_key, batch):\n",
    "        grads = jax.grad(self.loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    def training_step(self, params, opt_state, rng_key, batch):\n",
    "        params, opt_state = self._training_step(params, opt_state, rng_key, batch)\n",
    "\n",
    "        loss = self.loss_fn(params, rng_key, batch)\n",
    "        self.log_dict({\"train/train_loss_1\": loss.item()})\n",
    "        return params, opt_state\n",
    "\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self.net.apply(params, rng_key, x, is_training=False)\n",
    "        loss = self.loss_fn(params, rng_key, batch, is_training=False)\n",
    "        logs = {\"val/val_loss\": loss.item(), \"val/val_accuracy\": accuracy(y, y_pred)}\n",
    "        self.log_dict(logs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
