{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.test import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import einops\n",
    "import os, sys, json, pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor\n",
    "\n",
    "E.g., MinMaxScaler, OneHotEncoder.\n",
    "\n",
    "Note: it only works with preprocessing a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _check_xs(xs: np.ndarray):\n",
    "    if xs.ndim > 2 or (xs.ndim == 2 and xs.shape[1] != 1):\n",
    "        raise ValueError(f\"MinMaxScaler only supports array with a single feature, but got shape={xs.shape}.\")\n",
    "    \n",
    "        \n",
    "class DataPreprocessor:\n",
    "    \n",
    "    def fit(self, xs, y=None):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit_transform(self, xs, y=None):\n",
    "        self.fit(xs, y)\n",
    "        return self.transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def to_dict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # def __dict__(self):\n",
    "    #     return self.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinMaxScaler(DataPreprocessor): \n",
    "    def fit(self, xs, y=None):\n",
    "        _check_xs(xs)\n",
    "        self.min_ = xs.min(axis=0)\n",
    "        self.max_ = xs.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return (xs - self.min_) / (self.max_ - self.min_)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return xs * (self.max_ - self.min_) + self.min_\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.min_ = params[\"min_\"]\n",
    "        self.max_ = params[\"max_\"]\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"min_\": self.min_, \"max_\": self.max_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.randn(100, )\n",
    "scaler = MinMaxScaler()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert np.allclose(xs, scaler.inverse_transform(transformed_xs))\n",
    "\n",
    "\n",
    "xs = xs.reshape(100, 1)\n",
    "scaler = MinMaxScaler()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert np.allclose(xs, scaler.inverse_transform(transformed_xs))\n",
    "# Test from_dict and to_dict\n",
    "scaler_1 = MinMaxScaler().from_dict(scaler.to_dict())\n",
    "assert np.allclose(scaler.transform(xs), scaler_1.transform(xs))\n",
    "\n",
    "xs = xs.reshape(50, 2)\n",
    "scaler = MinMaxScaler()\n",
    "test_fail(lambda: scaler.fit_transform(xs), contains=\"MinMaxScaler only supports array with a single feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _unique(xs):\n",
    "    if xs.dtype == object:\n",
    "        # Note: np.unique does not work with object dtype\n",
    "        # We will enforce xs to be string type\n",
    "        # It assumes that xs is a list of strings, and might not work\n",
    "        # for other cases (e.g., list of string and numbers)\n",
    "        return np.unique(xs.astype(str))\n",
    "    return np.unique(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderPreprocessor(DataPreprocessor):\n",
    "    def _fit(self, xs, y=None):\n",
    "        _check_xs(xs)\n",
    "        self.categories_ = _unique(xs)\n",
    "\n",
    "    def _transform(self, xs):\n",
    "        \"\"\"Transform data to ordinal encoding.\"\"\"\n",
    "        if xs.dtype == object:\n",
    "            xs = xs.astype(str)\n",
    "        ordinal = np.searchsorted(self.categories_, xs)\n",
    "        return einops.rearrange(ordinal, 'k n -> n k')\n",
    "    \n",
    "    def _inverse_transform(self, xs):\n",
    "        \"\"\"Transform ordinal encoded data back to original data.\"\"\"\n",
    "        return self.categories_[xs].T\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.categories_ = params[\"categories_\"]\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"categories_\": self.categories_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OrdinalPreprocessor(EncoderPreprocessor):\n",
    "    def fit(self, xs, y=None):\n",
    "        self._fit(xs, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        if xs.ndim == 1:\n",
    "            raise ValueError(f\"OrdinalPreprocessor only supports 2D array with a single feature, \"\n",
    "                             f\"but got shape={xs.shape}.\")\n",
    "        return self._transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return self._inverse_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "enc = OrdinalPreprocessor().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs)\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "\n",
    "xs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\n",
    "enc = OrdinalPreprocessor().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs.astype(str))\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "\n",
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, ))\n",
    "test_fail(lambda: OrdinalPreprocessor().fit_transform(xs), \n",
    "    contains=\"OrdinalPreprocessor only supports 2D array with a single feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OneHotEncoder(EncoderPreprocessor):\n",
    "    # Fit the encoder without sci-kit OneHotEncoder.\n",
    "    def fit(self, xs, y=None):\n",
    "        self._fit(xs, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, xs):\n",
    "        if xs.ndim == 1:\n",
    "            raise ValueError(f\"OneHotEncoder only supports 2D array with a single feature, \"\n",
    "                             f\"but got shape={xs.shape}.\")\n",
    "        xs_int = self._transform(xs)\n",
    "        one_hot_feats = jax.nn.one_hot(xs_int, len(self.categories_))\n",
    "        return einops.rearrange(one_hot_feats, 'k n d -> n (k d)')\n",
    "\n",
    "    def inverse_transform(self, xs):\n",
    "        xs_int = np.argmax(xs, axis=-1)\n",
    "        return self._inverse_transform(xs_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "enc = OneHotEncoder().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs)\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OneHotEncoder().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "\n",
    "xs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\n",
    "enc = OneHotEncoder().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs.astype(str))\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OneHotEncoder().from_dict(enc.to_dict())\n",
    "enc_2 = OneHotEncoder()\n",
    "enc_2.from_dict(enc_1.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "assert np.all(enc.transform(xs) == enc_2.transform(xs))\n",
    "\n",
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, ))\n",
    "test_fail(lambda: OneHotEncoder().fit_transform(xs), \n",
    "    contains=\"OneHotEncoder only supports 2D array with a single feature\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformation:\n",
    "    def __init__(self, name, transformer):\n",
    "        self.name = name\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, xs, y=None):\n",
    "        self.transformer.fit(xs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return self.transformer.transform(xs)\n",
    "\n",
    "    def fit_transform(self, xs, y=None):\n",
    "        return self.transformer.fit_transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return self.transformer.inverse_transform(xs)\n",
    "\n",
    "    def apply_constraints(self, xs):\n",
    "        return xs\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.name = params[\"name\"]\n",
    "        self.transformer.from_dict(params[\"transformer\"])\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"name\": self.name, \"transformer\": self.transformer.to_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinMaxTransformation(Transformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"minmax\", MinMaxScaler())\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False):\n",
    "        return jnp.clip(cfs, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.randn(100, 1)\n",
    "scaler = MinMaxTransformation()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "\n",
    "cfs = np.random.randn(100, 1)\n",
    "cf_constrained = scaler.apply_constraints(xs, cfs)\n",
    "assert np.all(cf_constrained >= 0) and np.all(cf_constrained <= 1)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "scaler_1 = MinMaxTransformation().from_dict(scaler.to_dict())\n",
    "assert np.allclose(scaler.transform(xs), scaler_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OneHotTransformation(Transformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ohe\", OneHotEncoder())\n",
    "\n",
    "    @property\n",
    "    def categories(self) -> int:\n",
    "        return len(self.transformer.categories_)\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False):\n",
    "        return jax.lax.cond(\n",
    "            hard,\n",
    "            true_fun=lambda x: jax.nn.one_hot(jnp.argmax(x, axis=-1), self.categories),\n",
    "            false_fun=lambda x: jax.nn.softmax(x, axis=-1),\n",
    "            operand=cfs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "ohe_t = OneHotTransformation().fit(xs)\n",
    "transformed_xs = ohe_t.transform(xs)\n",
    "\n",
    "cfs = jax.random.uniform(jax.random.PRNGKey(0), shape=(100, 3))\n",
    "# Test hard=True which applies softmax function.\n",
    "soft = ohe_t.apply_constraints(transformed_xs, cfs, hard=False)\n",
    "assert jnp.allclose(soft.sum(axis=-1), 1)\n",
    "assert jnp.all(soft >= 0)\n",
    "assert jnp.all(soft <= 1)\n",
    "\n",
    "# Test hard=True which enforce one-hot constraint.\n",
    "hard = ohe_t.apply_constraints(transformed_xs, cfs, hard=True)\n",
    "assert np.all([1 in x for x in hard])\n",
    "assert np.all([0 in x for x in hard])\n",
    "assert jnp.allclose(hard.sum(axis=-1), 1)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "ohe_t_1 = OneHotTransformation().from_dict(ohe_t.to_dict())\n",
    "assert np.allclose(ohe_t.transform(xs), ohe_t_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OrdinalTransformation(Transformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ordinal\", OrdinalPreprocessor())\n",
    "\n",
    "    @property\n",
    "    def categories(self) -> int:\n",
    "        return len(self.transformer.categories_)\n",
    "    \n",
    "class IdentityTransformation(Transformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"identity\", None)\n",
    "\n",
    "    def fit(self, xs, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return xs\n",
    "    \n",
    "    def fit_transform(self, xs, y=None):\n",
    "        return xs\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False):\n",
    "        return cfs\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {'name': 'identity'}\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.name = params[\"name\"]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "encoder = OrdinalTransformation().fit(xs)\n",
    "transformed_xs = encoder.transform(xs)\n",
    "assert np.all(encoder.inverse_transform(transformed_xs) == xs)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "encoder_1 = OrdinalTransformation().from_dict(encoder.to_dict())\n",
    "assert np.allclose(encoder.transform(xs), encoder_1.transform(xs))\n",
    "\n",
    "xs = np.random.randn(100, 1)\n",
    "scaler = IdentityTransformation()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert np.all(transformed_xs == xs)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "scaler_1 = IdentityTransformation().from_dict(scaler.to_dict())\n",
    "assert np.allclose(scaler.transform(xs), scaler_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "PREPROCESSING_TRANSFORMATIONS = {\n",
    "    'ohe': OneHotTransformation,\n",
    "    'minmax': MinMaxTransformation,\n",
    "    'ordinal': OrdinalPreprocessor,\n",
    "    'identity': IdentityTransformation,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Feature:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        data: np.ndarray,\n",
    "        transformation: str | Transformation,\n",
    "        transformed_data = None,\n",
    "        is_immutable: bool = False,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        if isinstance(transformation, str):\n",
    "            self.transformation = PREPROCESSING_TRANSFORMATIONS[transformation]()\n",
    "        elif isinstance(transformation, Transformation):\n",
    "            self.transformation = transformation\n",
    "        elif isinstance(transformation, dict):\n",
    "            # TODO: only supported transformation can be used for serialization\n",
    "            t_name = transformation['name']\n",
    "            if t_name not in PREPROCESSING_TRANSFORMATIONS.keys():\n",
    "                raise ValueError(\"Only supported transformation can be inited from dict. \"\n",
    "                                 f\"Got {t_name}, but should be one of {PREPROCESSING_TRANSFORMATIONS.keys()}.\")\n",
    "            self.transformation = PREPROCESSING_TRANSFORMATIONS[t_name]().from_dict(transformation)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformer {transformation}\")\n",
    "        self._transformed_data = transformed_data\n",
    "        self.is_immutable = is_immutable\n",
    "\n",
    "    @property\n",
    "    def transformed_data(self):\n",
    "        if self._transformed_data is None:\n",
    "            return self.fit_transform(self.data)\n",
    "        else:\n",
    "            return self._transformed_data\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(**d)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'data': self.data,\n",
    "            'transformed_data': self.transformed_data,\n",
    "            'transformation': self.transformation.to_dict(),\n",
    "            'is_immutable': self.is_immutable,\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Feature(\" \\\n",
    "               f\"name={self.name}, \\ndata={self.data}, \\n\" \\\n",
    "               f\"transformed_data={self.transformed_data}, \\n\" \\\n",
    "               f\"transformer={self.transformation}, \\n\" \\\n",
    "               f\"is_immutable={self.is_immutable})\"\n",
    "    \n",
    "    __str__ = __repr__\n",
    "\n",
    "    def __get_item__(self, idx):\n",
    "        return {\n",
    "            'data': self.data[idx],\n",
    "            'transformed_data': self.transformed_data[idx],\n",
    "        }\n",
    "\n",
    "    def fit(self):\n",
    "        self.transformation.fit(self.data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return self.transformation.transform(xs)\n",
    "\n",
    "    def fit_transform(self, xs):\n",
    "        return self.transformation.fit_transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return self.transformation.inverse_transform(xs)\n",
    "    \n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False):\n",
    "        return jax.lax.cond(\n",
    "            self.is_immutable,\n",
    "            true_fun=lambda xs: xs,\n",
    "            false_fun=lambda _: self.transformation.apply_constraints(xs, cfs, hard),\n",
    "            operand=xs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cont = Feature(\n",
    "    name='continuous',\n",
    "    data=np.random.randn(100, 1),\n",
    "    transformation='minmax',\n",
    "    is_immutable=False,\n",
    ")\n",
    "assert feat_cont.transformed_data.shape == (100, 1)\n",
    "assert feat_cont.transformed_data.min() >= 0\n",
    "assert feat_cont.transformed_data.max() <= 1\n",
    "assert jnp.allclose(\n",
    "    feat_cont.inverse_transform(feat_cont.transformed_data), feat_cont.data)\n",
    "\n",
    "feat_cat = Feature(\n",
    "    name='category',\n",
    "    data=np.random.choice(['a', 'b', 'c'], size=(100, 1)),\n",
    "    transformation='ohe',\n",
    "    is_immutable=False,\n",
    ")\n",
    "assert feat_cat.transformed_data.shape == (100, 3)\n",
    "assert np.all(feat_cat.inverse_transform(feat_cat.transformed_data) == feat_cat.data)\n",
    "\n",
    "# TODO: Need test cases for from_dict and to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _is_array(x):\n",
    "    return isinstance(x, np.ndarray) or isinstance(x, jnp.ndarray) or isinstance(x, list)\n",
    "\n",
    "def save_pytree(pytree, saved_dir):\n",
    "    with open(os.path.join(saved_dir, \"data.npy\"), \"wb\") as f:\n",
    "        for x in jax.tree_util.tree_leaves(pytree):\n",
    "            np.save(f, x)\n",
    "\n",
    "    tree_struct = jax.tree_util.tree_map(lambda t: _is_array(t), pytree)\n",
    "    with open(os.path.join(saved_dir, \"treedef.json\"), \"w\") as f:\n",
    "        json.dump(tree_struct, f)\n",
    "\n",
    "\n",
    "def load_pytree(saved_dir):\n",
    "    with open(os.path.join(saved_dir, \"treedef.json\"), \"r\") as f:\n",
    "        tree_struct = json.load(f)\n",
    "\n",
    "    leaves, treedef = jax.tree_util.tree_flatten(tree_struct)\n",
    "    with open(os.path.join(saved_dir, \"data.npy\"), \"rb\") as f:\n",
    "        flat_state = [\n",
    "            np.load(f, allow_pickle=True) if is_arr else np.load(f, allow_pickle=True).item()\n",
    "            for is_arr in leaves\n",
    "        ]\n",
    "    return jax.tree_util.tree_unflatten(treedef, flat_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytree = {\n",
    "    'a': np.random.randn(100, 1),\n",
    "    'b': 1,\n",
    "    'c': {\n",
    "        'd': True,\n",
    "        'e': \"Hello\",\n",
    "        'f': np.array([\"a\", \"b\", \"c\"])\n",
    "    }\n",
    "}\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "save_pytree(pytree, 'tmp')\n",
    "pytree_loaded = load_pytree('tmp')\n",
    "assert np.allclose(pytree['a'], pytree_loaded['a'])\n",
    "assert pytree['a'].dtype == pytree_loaded['a'].dtype\n",
    "assert pytree['b'] == pytree_loaded['b']\n",
    "assert pytree['c']['d'] == pytree_loaded['c']['d']\n",
    "assert pytree['c']['e'] == pytree_loaded['c']['e']\n",
    "assert np.all(pytree['c']['f'] == pytree_loaded['c']['f'])\n",
    "\n",
    "pytree = [\n",
    "    np.random.randn(100, 1),\n",
    "    {'a': 1, 'b': np.array([1, 2, 3])},\n",
    "    1,\n",
    "    [1, 2, 3],\n",
    "    \"good\"\n",
    "]\n",
    "save_pytree(pytree, 'tmp')\n",
    "pytree_loaded = load_pytree('tmp')\n",
    "\n",
    "assert np.allclose(pytree[0], pytree_loaded[0])\n",
    "assert pytree[0].dtype == pytree_loaded[0].dtype\n",
    "assert pytree[1]['a'] == pytree_loaded[1]['a']\n",
    "assert np.all(pytree[1]['b'] == pytree_loaded[1]['b'])\n",
    "assert pytree[2] == pytree_loaded[2]\n",
    "assert pytree[3] == pytree_loaded[3]\n",
    "assert isinstance(pytree_loaded[3], list)\n",
    "assert pytree[4] == pytree_loaded[4]\n",
    "\n",
    "shutil.rmtree('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeaturesList:\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: list[Feature] | FeaturesList,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        if isinstance(features, FeaturesList):\n",
    "            self._features = features.features\n",
    "            self._feature_indices = features.feature_indices\n",
    "            self._transformed_data = features.transformed_data\n",
    "        elif isinstance(features, Feature):\n",
    "            self._features = [features]\n",
    "            self._feature_indices = []\n",
    "            self._transformed_data = None\n",
    "        elif isinstance(features, list):\n",
    "            if len(features) > 0 and not isinstance(features[0], Feature):\n",
    "                raise ValueError(f\"Invalid features type: {type(features[0]).__name__}\")\n",
    "            self._features = features\n",
    "            self._feature_indices = []\n",
    "            self._transformed_data = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown features type {type(features)}\")\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return self._features\n",
    "\n",
    "    @property\n",
    "    def feature_indices(self):\n",
    "        if self._feature_indices is None:\n",
    "            self._transform_data()\n",
    "        return self._feature_indices\n",
    "    \n",
    "    @property\n",
    "    def transformed_data(self):\n",
    "        if self._transformed_data is None:\n",
    "            self._transform_data()\n",
    "        return self._transformed_data\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'features': [feat.to_dict() for feat in self.features],\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(\n",
    "            features=[Feature.from_dict(feat) for feat in d['features']],\n",
    "        )\n",
    "\n",
    "    def save(self, saved_dir):\n",
    "        os.makedirs(saved_dir, exist_ok=True)\n",
    "        save_pytree(self.to_dict(), saved_dir)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_from_path(cls, saved_dir):\n",
    "        return cls.from_dict(load_pytree(saved_dir))\n",
    "\n",
    "    def _transform_data(self):\n",
    "        self._feature_indices = []\n",
    "        self._transformed_data = []\n",
    "        start, end = 0, 0\n",
    "        for feat in self.features:\n",
    "            transformed_data = feat.transformed_data\n",
    "            end += transformed_data.shape[-1]\n",
    "            self._feature_indices.append((start, end))\n",
    "            self._transformed_data.append(transformed_data)\n",
    "            start = end\n",
    "\n",
    "        self._transformed_data = jnp.concatenate(self._transformed_data, axis=-1)\n",
    "\n",
    "    def transform(self, data):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def inverse_transform(self, xs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False):\n",
    "        constrainted_cfs = []\n",
    "        for (start, end), feat in zip(self.feature_indices, self.features):\n",
    "            _cfs = feat.apply_constraints(xs[:, start:end], cfs[:, start:end], hard)\n",
    "            constrainted_cfs.append(_cfs)\n",
    "        return jnp.concatenate(constrainted_cfs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets/adult/data.csv')\n",
    "cont_feats = ['age', 'hours_per_week']\n",
    "cat_feats = [\"workclass\", \"education\", \"marital_status\",\"occupation\", \"race\", \"gender\"]\n",
    "\n",
    "feats_list = FeaturesList([\n",
    "    Feature(name, df[name].to_numpy().reshape(-1, 1), 'minmax') for name in cont_feats\n",
    "] + [\n",
    "    Feature(name, df[name].to_numpy().reshape(-1, 1), 'ohe') for name in cat_feats\n",
    "])\n",
    "assert feats_list.transformed_data.shape == (32561, 29)\n",
    "cfs = np.random.randn(10, 29)\n",
    "assert feats_list.apply_constraints(feats_list.transformed_data[:10, :], cfs, hard=False).shape == (10, 29)\n",
    "assert feats_list.apply_constraints(feats_list.transformed_data[:10, :], cfs, hard=True).shape == (10, 29)\n",
    "\n",
    "# Test save and load\n",
    "feats_list.save('tmp/data_module/')\n",
    "feats_list_1 = FeaturesList.load_from_path('tmp/data_module/')\n",
    "# remove tmp folder\n",
    "shutil.rmtree('tmp')\n",
    "\n",
    "# assert feats_list.feature_indices == [(0, 1), (1, 5)]\n",
    "# cfs = np.random.randn(10, 5)\n",
    "# feats_list.apply_constraints(feats_list.transformed_data[:10, :], cfs, hard=False)\n",
    "# feats_list.apply_constraints(feats_list.transformed_data[:10, :], cfs, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as SkOneHotEncoder, MinMaxScaler as SkMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_ohe = SkOneHotEncoder(sparse_output=False)\n",
    "sk_minmax = SkMinMaxScaler()\n",
    "\n",
    "for feat in feats_list.features:\n",
    "    if feat.name in cont_feats:\n",
    "        assert np.allclose(\n",
    "            sk_minmax.fit_transform(feat.data),\n",
    "            feat.transformed_data,\n",
    "        ), f\"Failed at {feat.name}. \"\n",
    "    else:\n",
    "        assert np.allclose(\n",
    "            sk_ohe.fit_transform(feat.data),\n",
    "            feat.transformed_data,\n",
    "        ), f\"Failed at {feat.name}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
