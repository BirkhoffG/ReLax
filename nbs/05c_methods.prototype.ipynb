{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtoCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.interfaces import BaseCFExplanationModule, LocalCFExplanationModule\n",
    "from cfnet.datasets import TabularDataModule\n",
    "from cfnet.module import BaseTrainingModule\n",
    "from cfnet.train import train_model\n",
    "from cfnet.utils import (\n",
    "    check_cat_info,\n",
    "    validate_configs,\n",
    "    binary_cross_entropy,\n",
    "    cat_normalize,\n",
    "    make_model,\n",
    "    init_net_opt,\n",
    "    grad_update,\n",
    ")\n",
    "from cfnet.nets import MLP\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class AEConfigs(BaseParser):\n",
    "    enc_sizes: List[int]\n",
    "    dec_sizes: List[int]\n",
    "    dropout_rate: float = 0.3\n",
    "    lr: float = 0.001\n",
    "\n",
    "\n",
    "class AE(hk.Module):\n",
    "    def __init__(self, m_config: Dict[str, Any], name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(\n",
    "            m_config, AEConfigs\n",
    "        )  # PredictiveModelConfigs(**m_config)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        z = MLP(\n",
    "            sizes=self.configs.enc_sizes,\n",
    "            dropout_rate=self.configs.dropout_rate,\n",
    "            name=\"Encoder\",\n",
    "        )(x, is_training)\n",
    "        x = MLP(\n",
    "            sizes=self.configs.enc_sizes,\n",
    "            dropout_rate=self.configs.dropout_rate,\n",
    "            name=\"Decoder\",\n",
    "        )(z, is_training)\n",
    "        x = hk.Linear(input_shape, name=\"Decoder\")(x)\n",
    "        return x, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class AETrainingModule(BaseTrainingModule):\n",
    "    def __init__(self, m_configs: Dict[str, Any]):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.net = make_model(m_configs, AE)\n",
    "        self.configs = validate_configs(m_configs, AEConfigs)\n",
    "        # self.configs = PredictiveTrainingModuleConfigs(**m_configs)\n",
    "        self.opt = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    def init_net_opt(self, data_module, key):\n",
    "        params, opt_state = init_net_opt(\n",
    "            self.net, self.opt, X=data_module.get_sample_X(), key=key\n",
    "        )\n",
    "        return params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        return self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "\n",
    "    def encode(self, params, rng_key, x):\n",
    "        _, z = self.forward(params, rng_key, x, is_training=False)\n",
    "        return z\n",
    "\n",
    "    def loss_fn(self, params, rng_key, batch, is_training=True):\n",
    "        x, y = batch\n",
    "        x_hat, z = self.forward(params, rng_key, x, is_training)\n",
    "        return jnp.mean(vmap(optax.l2_loss)(x, x_hat))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step(self, params, opt_state, rng_key, batch):\n",
    "        grads = jax.grad(self.loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        upt_params, opt_state = self._training_step(params, opt_state, rng_key, batch)\n",
    "\n",
    "        loss = self.loss_fn(params, rng_key, batch)\n",
    "        self.log_dict({\"train/train_loss_1\": loss.item()})\n",
    "        return params, opt_state\n",
    "\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        loss = self.loss_fn(params, rng_key, batch, is_training=False)\n",
    "        logs = {\n",
    "            \"val/val_loss\": loss.item(),\n",
    "        }\n",
    "        self.log_dict(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _proto_cf(\n",
    "    x: jnp.DeviceArray,  # `x` shape: (k,), where `k` is the number of features\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],  # y = pred_fn(x)\n",
    "    n_steps: int,\n",
    "    lr: float,  # learning rate for each `cf` optimization step\n",
    "    lambda_: float,  #  loss = validity_loss + lambda_params * cost\n",
    "    cat_arrays: List[List[str]],\n",
    "    cat_idx: int,\n",
    "    ae: AETrainingModule,\n",
    "    ae_params: hk.Params,\n",
    "    sampled_data_pos: jnp.DeviceArray,\n",
    "    sampled_data_neg: jnp.DeviceArray,\n",
    "    sampled_label: jnp.DeviceArray,\n",
    ") -> jnp.DeviceArray:  # return `cf` shape: (k,)\n",
    "    def proto(data):\n",
    "        return ae.encode(ae_params, jax.random.PRNGKey(0), data)\n",
    "\n",
    "    def loss_fn_1(cf_y: jnp.DeviceArray, y_prime: jnp.DeviceArray):\n",
    "        return jnp.mean(binary_cross_entropy(y_pred=cf_y, y=y_prime))\n",
    "\n",
    "    def loss_fn_2(x: jnp.DeviceArray, cf: jnp.DeviceArray):\n",
    "        return jnp.mean(optax.l2_loss(cf, x)) + 0.1 * jnp.mean(\n",
    "            jnp.mean(jnp.abs(x - cf))\n",
    "        )\n",
    "\n",
    "    def loss_fn_3(cf, data):\n",
    "        error = proto(cf) - proto(data)\n",
    "        return jnp.mean(0.5 * (error) ** 2)\n",
    "\n",
    "    def loss_fn(\n",
    "        cf: jnp.DeviceArray,  # `cf` shape: (k, 1)\n",
    "        x: jnp.DeviceArray,  # `x` shape: (k, 1)\n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    "    ):\n",
    "        y_pred = pred_fn(x)\n",
    "        y_prime = 1.0 - y_pred\n",
    "        cf_y = pred_fn(cf)\n",
    "\n",
    "        y_prime_round = jnp.mean(jnp.round(y_prime))\n",
    "\n",
    "        # print(sampled_label.shape)\n",
    "        # print(y_prime.shape)\n",
    "        return (\n",
    "            loss_fn_1(cf_y, y_prime)\n",
    "            + loss_fn_2(x, cf)\n",
    "            + loss_fn_3(cf, sampled_data_pos) * y_prime_round\n",
    "            + loss_fn_3(cf, sampled_data_neg) * (1 - y_prime_round)\n",
    "        )\n",
    "\n",
    "    @jax.jit\n",
    "    def gen_cf_step(\n",
    "        x: jnp.DeviceArray, cf: jnp.DeviceArray, opt_state: optax.OptState\n",
    "    ) -> Tuple[jnp.DeviceArray, optax.OptState]:\n",
    "        cf_grads = jax.grad(loss_fn)(cf, x, pred_fn)\n",
    "        cf, opt_state = grad_update(cf_grads, cf, opt_state, opt)\n",
    "        cf = cat_normalize(cf, cat_arrays=cat_arrays, cat_idx=cat_idx, hard=False)\n",
    "        cf = jnp.clip(cf, 0.0, 1.0)\n",
    "        return cf, opt_state\n",
    "\n",
    "    x_size = x.shape\n",
    "    if len(x_size) > 1 and x_size[0] != 1:\n",
    "        raise ValueError(\n",
    "            f\"\"\"Invalid Input Shape: Require `x.shape` = (1, k) or (k, ),\n",
    "but got `x.shape` = {x.shape}. This method expects a single input instance.\"\"\"\n",
    "        )\n",
    "    if len(x_size) == 1:\n",
    "        x = x.reshape(1, -1)\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    opt = optax.rmsprop(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "    cf = cat_normalize(cf, cat_arrays=cat_arrays, cat_idx=cat_idx, hard=True)\n",
    "    return cf.reshape(x_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ProtoCFConfig(BaseParser):\n",
    "    n_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    lambda_: float = 0.01  # loss = validity_loss + lambda_params * cost\n",
    "    ae_configs: Dict[str, Any] = {\n",
    "        \"enc_sizes\": [50, 10],\n",
    "        \"dec_sizes\": [10, 50],\n",
    "        \"dropout_rate\": 0.3,\n",
    "        \"lr\": 0.03,\n",
    "    }\n",
    "    ae_t_configs: Dict[str, str] = {\n",
    "        \"batch_size\": 128,\n",
    "        \"n_epochs\": 10, \n",
    "        \"monitor_metrics\": \"val/val_loss\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProtoCF(LocalCFExplanationModule):\n",
    "    name = \"ProtoCF\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: Union[Dict[str, Any], ProtoCFConfig],\n",
    "        data_module: Optional[TabularDataModule] = None,\n",
    "    ):\n",
    "        self.configs = validate_configs(configs, ProtoCFConfig)\n",
    "        if data_module:\n",
    "            self.update_cat_info(data_module)\n",
    "\n",
    "    def update_cat_info(self, data_module: TabularDataModule):\n",
    "        sampled_data, sampled_label = data_module.get_samples()\n",
    "        self.sampled_data, self.sampled_label = map(\n",
    "            jnp.array, (sampled_data, sampled_label)\n",
    "        )\n",
    "\n",
    "        self.sampled_pos = self.sampled_data[(self.sampled_label == 1.0).reshape(-1), :]\n",
    "        self.sampled_neg = self.sampled_data[(self.sampled_label == 0.0).reshape(-1), :]\n",
    "        self.ae = AETrainingModule(self.configs.ae_configs)\n",
    "        self.ae_params, _ = train_model(self.ae, data_module, self.configs.ae_t_configs)\n",
    "        return super().update_cat_info(data_module)\n",
    "\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: jnp.ndarray,  # `x` shape: (k,), where `k` is the number of features\n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    "    ) -> jnp.DeviceArray:\n",
    "        return _proto_cf(\n",
    "            x=x,  # `x` shape: (k,), where `k` is the number of features\n",
    "            pred_fn=pred_fn,  # y = pred_fn(x)\n",
    "            n_steps=self.configs.n_steps,\n",
    "            lr=self.configs.lr,  # learning rate for each `cf` optimization step\n",
    "            lambda_=self.configs.lambda_,  #  loss = validity_loss + lambda_params * cost\n",
    "            cat_arrays=self.cat_arrays,\n",
    "            cat_idx=self.cat_idx,\n",
    "            ae=self.ae,\n",
    "            ae_params=self.ae_params,\n",
    "            sampled_data_pos=self.sampled_pos,\n",
    "            sampled_data_neg=self.sampled_neg,\n",
    "            sampled_label=self.sampled_label,\n",
    "        )\n",
    "\n",
    "    @check_cat_info\n",
    "    def generate_cfs(\n",
    "        self,\n",
    "        X: jnp.DeviceArray,  # `x` shape: (b, k), where `b` is batch size, `k` is the number of features\n",
    "        pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray],\n",
    "        is_parallel: bool = False,\n",
    "    ) -> jnp.DeviceArray:\n",
    "        def _generate_cf(x: jnp.DeviceArray) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, pred_fn)\n",
    "\n",
    "        return (\n",
    "            jax.vmap(_generate_cf)(X) if not is_parallel else jax.pmap(_generate_cf)(X)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {\n",
    "    \"data_dir\": \"assets/data/s_adult.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"batch_size\": 256,\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\n",
    "        \"age\",\n",
    "        \"hours_per_week\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"race\",\n",
    "        \"gender\"\n",
    "    ],\n",
    "}\n",
    "m_configs = {\n",
    "    \"sizes\": [50, 10, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.03,\n",
    "}\n",
    "\n",
    "ae_configs = {\n",
    "    \"enc_sizes\": [50, 10],\n",
    "    \"dec_sizes\": [10, 50],\n",
    "    \"dropout_rate\": 0.3,\n",
    "    'lr': 0.03,\n",
    "}\n",
    "\n",
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'seed': 42,\n",
    "    \"batch_size\": 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cfnet.module import PredictiveTrainingModule\n",
    "from cfnet.train import train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_module = PredictiveTrainingModule(m_configs)\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    training_module, \n",
    "    TabularDataModule(data_configs), \n",
    "    t_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TabularDataModule(data_configs)\n",
    "pred_fn = lambda x: training_module.forward(params, random.PRNGKey(0), x, is_training=False)\n",
    "\n",
    "cf_exp = ProtoCF({})\n",
    "cf_exp.update_cat_info(dm)\n",
    "\n",
    "X, y = dm.test_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.8215207, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sum(jnp.round(pred_fn(X)) == y) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 130.14it/s]\n"
     ]
    }
   ],
   "source": [
    "cf = cf_exp.generate_cf(X[0], pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = cf_exp.generate_cfs(X, pred_fn)\n",
    "y_pred = pred_fn(X)\n",
    "cf_pred = pred_fn(cfs)\n",
    "y_prime = 1. - jnp.round(y_pred)\n",
    "validity = jnp.sum(jnp.round(cf_pred) == y_prime) / len(cf_pred)\n",
    "validity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nbdev2')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
