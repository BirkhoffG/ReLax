{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from cfnet.import_essentials import *\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderABC(ABC):\n",
    "    def __init__(\n",
    "        self, \n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        num_workers: int = 0,\n",
    "        drop_last: bool = False\n",
    "    ):\n",
    "        pass \n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __next__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "def _numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [_numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "class DataLoaderPytorch(DataLoaderABC):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        num_workers: int = 0,\n",
    "        drop_last: bool = False\n",
    "    ):  \n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise ImportError(\"`Pytorch` library has not been installed. Try `pip install torch`.\"\n",
    "            \"Please refer to pytorch documentation for details: https://pytorch.org/get-started/.\")\n",
    "        \n",
    "        self.dataloader = torch.DataLoader(\n",
    "            dataset, \n",
    "            batchsize=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            num_workers=num_workers, \n",
    "            drop_last=drop_last,\n",
    "            collate_fn=_numpy_collate,\n",
    "        ) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderJax:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,  # dataset, a Dataset object\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        seed: int = 42,  # seed for random number generator\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=0,\n",
    "        collate_fn=_numpy_collate,  # collate function, default is _numpy_collate()\n",
    "        pin_memory=False,\n",
    "        drop_last: bool = False,  # if true, dataloader drops the last batch that is less than the batch size\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "    ):\n",
    "        # Attributes from pytorch data loader (implemented)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.collate_fn = collate_fn\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        # Attributes from pytorch data loader (not implemented)\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.worker_init_fn = worker_init_fn\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self.data_len: int = len(dataset)  # Length of the dataset\n",
    "        self.key_seq: hk.PRNGSequence = hk.PRNGSequence(\n",
    "            self.seed\n",
    "        )  # random number sequence\n",
    "        self.key_seq.reserve(\n",
    "            len(self)\n",
    "        )  # generate some random number as key based on the number of batches\n",
    "        self.key = next(self.key_seq)  # obtain a random key from the sequence\n",
    "        self.indices: jax.numpy.array = jax.numpy.arange(\n",
    "            self.data_len\n",
    "        )  # available indices in the dataset\n",
    "        self.pose: int = 0  # record the current position in the dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            batches = len(self.dataset) // self.batch_size  # get the floor of division\n",
    "        else:\n",
    "            batches = -(\n",
    "                len(self.dataset) // -self.batch_size\n",
    "            )  # get the ceil of division\n",
    "        return batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pose <= self.data_len:\n",
    "            if self.shuffle:\n",
    "                self.key = next(self.key_seq)\n",
    "                self.indices = jax.random.permutation(self.key, self.indices)\n",
    "            batch_data = [self.dataset[i] for i in self.indices[: self.batch_size]]\n",
    "            self.indices = self.indices[self.batch_size :]\n",
    "            if self.drop_last and len(self.indices) < self.batch_size:\n",
    "                self.pose = 0\n",
    "                self.indices = jax.numpy.arange(self.data_len)\n",
    "                raise StopIteration\n",
    "            self.pose += self.batch_size\n",
    "            return self.collate_fn(batch_data)\n",
    "        else:\n",
    "            self.pose = 0\n",
    "            self.indices = jax.numpy.arange(self.data_len)\n",
    "            raise StopIteration\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "backend2dataloader = {\n",
    "    'jax': DataLoaderJax,\n",
    "    'pytorch': DataLoaderPytorch,\n",
    "    'tensorflow': None,\n",
    "    'merlin': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _dispatch_datalaoder(backend: str):\n",
    "    dataloader_backends = backend2dataloader.keys()\n",
    "    if not backend in dataloader_backends:\n",
    "        raise ValueError(f\"backend=`{backend}` is not supported for dataloader. Should be one of {dataloader_backends}.\")\n",
    "    \n",
    "    dataloader_cls = backend2dataloader[backend]\n",
    "    if dataloader_cls is None:\n",
    "        raise NotImplementedError(f'backend=`{backend}` is not supported yet.')\n",
    "    return dataloader_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend,\n",
    "        dataset,\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "        self.__class__ = _dispatch_datalaoder(backend)\n",
    "        self.__init__(dataset, batch_size, shuffle, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a simple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n",
    "\n",
    "def step(w, x, y):\n",
    "    lr = 0.1\n",
    "    grad = jax.grad(loss)(w, x, y)\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def train(dataloader, key):\n",
    "    x, y = next(dataloader)\n",
    "    w = jax.random.normal(key, shape=(1, x.shape[1]))\n",
    "    n_epochs = 10\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in dataloader:\n",
    "            w = step(w, x, y)\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=20)\n",
    "dataset = Dataset(X, y.reshape(-1, 1))\n",
    "keys = hk.PRNGSequence(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2963/3639804090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2963/1621340836.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2963/1046399104.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2963/1046399104.py\u001b[0m in \u001b[0;36m_numpy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_numpy_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "train(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_imutable_idx_list(\n",
    "    imutable_col_names: List[str],\n",
    "    discrete_col_names: List[str],\n",
    "    continuous_col_names: List[str],\n",
    "    cat_arrays: List[List[str]],\n",
    ") -> List[int]:\n",
    "    imutable_idx_list = []\n",
    "    for idx, col_name in enumerate(continuous_col_names):\n",
    "        if col_name in imutable_col_names:\n",
    "            imutable_idx_list.append(idx)\n",
    "\n",
    "    cat_idx = len(continuous_col_names)\n",
    "\n",
    "    for i, (col_name, cols) in enumerate(zip(discrete_col_names, cat_arrays)):\n",
    "        cat_end_idx = cat_idx + len(cols)\n",
    "        if col_name in imutable_col_names:\n",
    "            imutable_idx_list += list(range(cat_idx, cat_end_idx))\n",
    "        cat_idx = cat_end_idx\n",
    "    return imutable_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataModuleConfigs(BaseParser):\n",
    "    batch_size: int\n",
    "    discret_cols: List[str] = []\n",
    "    continous_cols: List[str] = []\n",
    "    imutable_cols: List[str] = []\n",
    "    normalizer: Optional[Any] = None\n",
    "    encoder: Optional[Any] = None\n",
    "    sample_frac: Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _data_name2configs(data_name: str):\n",
    "    with open('../assets/configs/{}.json'.format(data_name)) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        data_configs['data_name'] = data_name\n",
    "        data_configs['discret_cols'] = data['discret_cols']\n",
    "        data_configs['continous_cols'] = data['continous_cols']\n",
    "        data_configs['imutable_cols'] = data.get('imutable_cols', [])\n",
    "        data_configs['sample_frac'] = data.get('sample_frac', [])\n",
    "        data_configs['normalizer'] = data.get('normalizer', [])\n",
    "        data_configs['encoder'] = data.get('encoder', [])\n",
    "        data_configs['data_dir'] = _download_data(data_name)\n",
    "    return data_configs\n",
    "\n",
    "def _download_data(data_name: str):\n",
    "        url = 'https://github.com/BirkhoffG/cfnet/raw/master/assets/data/{}.csv'.format(data_name)\n",
    "        path = Path(os.getcwd())\n",
    "        path = path / \"cf_data\"\n",
    "        if not path.exists():\n",
    "            os.makedirs(path)\n",
    "        path = path / f'{data_name}.csv'\n",
    "        if path.is_file():\n",
    "            return path\n",
    "        else:\n",
    "            urlretrieve(url,path)\n",
    "            return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TabularDataModule:\n",
    "    discret_cols: List[str] = []\n",
    "    continous_cols: List[str] = []\n",
    "    imutable_cols: List[str] = []\n",
    "    normalizer: Optional[Any] = None\n",
    "    encoder: Optional[OneHotEncoder] = None\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    sample_frac: Optional[float] = None\n",
    "    batch_size: int = 128\n",
    "    data_name: str = \"\"\n",
    "\n",
    "    def __init__(self, data_configs: dict | str = None):\n",
    "        if isinstance(data_configs, str):\n",
    "            data_configs = _data_name2configs(data_configs)\n",
    "            self.data = pd.read_csv(Path(data_configs['data_dir']))\n",
    "        elif isinstance(data_configs, dict):\n",
    "            # read data\n",
    "            self.data = pd.read_csv(Path(data_configs['data_dir']))\n",
    "\n",
    "        # update configs\n",
    "        self._update_configs(data_configs)\n",
    "        self.check_cols()\n",
    "        # update cat_idx\n",
    "        self.cat_idx = len(self.continous_cols)\n",
    "        # prepare data\n",
    "        self.prepare_data()\n",
    "\n",
    "\n",
    "    def check_cols(self):\n",
    "        self.data = self.data.astype({col: np.float for col in self.continous_cols})\n",
    "        # check imutable cols\n",
    "        cols = self.continous_cols + self.discret_cols\n",
    "        for col in self.imutable_cols:\n",
    "            assert (\n",
    "                 col in cols\n",
    "             ), f\"imutable_cols=[{col}] is not specified in `continous_cols` or `discret_cols`.\"\n",
    "\n",
    "    def _update_configs(self, configs):\n",
    "        for k, v in configs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        def split_x_and_y(data: pd.DataFrame):\n",
    "            X = data[data.columns[:-1]]\n",
    "            y = data[[data.columns[-1]]]\n",
    "            return X, y\n",
    "\n",
    "        X, y = split_x_and_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        if self.normalizer:\n",
    "            X_cont = self.normalizer.transform(X[self.continous_cols])\n",
    "        else:\n",
    "            self.normalizer = MinMaxScaler()\n",
    "            X_cont = (\n",
    "                 self.normalizer.fit_transform(X[self.continous_cols])\n",
    "                 if self.continous_cols\n",
    "                 else np.array([[] for _ in range(len(X))])\n",
    "             )\n",
    "\n",
    "        if self.encoder:\n",
    "            X_cat = self.encoder.transform(X[self.discret_cols])\n",
    "        else:\n",
    "            self.encoder = OneHotEncoder(sparse=False)\n",
    "            X_cat = (\n",
    "                 self.encoder.fit_transform(X[self.discret_cols])\n",
    "                 if self.discret_cols\n",
    "                 else np.array([[] for _ in range(len(X))])\n",
    "             )\n",
    "        X = np.concatenate((X_cont, X_cat), axis=1)\n",
    "        # get categorical arrays\n",
    "        self.cat_arrays = self.encoder.categories_ if self.discret_cols else []\n",
    "        self.imutable_idx_list = find_imutable_idx_list(\n",
    "            imutable_col_names=self.imutable_cols,\n",
    "            discrete_col_names=self.discret_cols,\n",
    "            continuous_col_names=self.continous_cols,\n",
    "            cat_arrays=self.cat_arrays,\n",
    "        )\n",
    "\n",
    "        # prepare train & test\n",
    "        train_test_tuple = train_test_split(X, y.to_numpy(), shuffle=False)\n",
    "        train_X, test_X, train_y, test_y = map(\n",
    "             lambda x: x.astype(jnp.float32), train_test_tuple\n",
    "         )\n",
    "        if self.sample_frac:\n",
    "            train_size = int(len(train_X) * self.sample_frac)\n",
    "            train_X, train_y = train_X[:train_size], train_y[:train_size]\n",
    "        self.train_dataset = Dataset(train_X, train_y)\n",
    "        self.val_dataset = Dataset(test_X, test_y)\n",
    "        self.test_dataset = self.val_dataset\n",
    "\n",
    "    def train_dataloader(self, seed, batch_size):\n",
    "        return DataLoader(\n",
    "             self.train_dataset,\n",
    "             seed=seed,\n",
    "             batch_size=batch_size,\n",
    "             pin_memory=True,\n",
    "             shuffle=True,\n",
    "             num_workers=0,\n",
    "         )\n",
    "\n",
    "    def val_dataloader(self, seed, batch_size):\n",
    "        return DataLoader(\n",
    "             self.val_dataset,\n",
    "             seed=seed,\n",
    "             batch_size=batch_size * 4,\n",
    "             pin_memory=True,\n",
    "             shuffle=False,\n",
    "             num_workers=0,\n",
    "         )\n",
    "\n",
    "    def test_dataloader(self, seed, batch_size):\n",
    "        return DataLoader(\n",
    "             self.val_dataset,\n",
    "             seed=seed,\n",
    "             batch_size=batch_size,\n",
    "             pin_memory=True,\n",
    "             shuffle=False,\n",
    "             num_workers=0,\n",
    "         )\n",
    "\n",
    "    def get_sample_X(self, frac: float | None = None):\n",
    "        train_X, _ = self.get_samples(frac)\n",
    "        return train_X\n",
    "\n",
    "    def get_samples(self, frac: float | None = None):\n",
    "        if frac is None:\n",
    "            frac = 0.1\n",
    "        train_X, train_y = self.train_dataset[:]\n",
    "        train_size = int(len(train_X) * frac)\n",
    "        return train_X[:train_size], train_y[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "data_configs = {\n",
    "    \"data_dir\": \"assets/data/s_adult.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"batch_size\": 256,\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\n",
    "        \"age\",\n",
    "        \"hours_per_week\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"race\",\n",
    "        \"gender\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/4qh59pkn75xdzh4r61851p4h0000gn/T/ipykernel_3311/1646380093.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.data = self.data.astype({col: np.float for col in self.continous_cols})\n"
     ]
    }
   ],
   "source": [
    "dm = TabularDataModule(data_configs)\n",
    "seed=42\n",
    "batch_size=data_configs[\"batch_size\"]\n",
    "t_dataloader = dm.train_dataloader(seed,batch_size)\n",
    "x, y = next(iter(t_dataloader))\n",
    "assert x.shape[0] == 256\n",
    "assert x.shape[1] == 29\n",
    "assert dm.sample_frac == 0.1\n",
    "\n",
    "l = 0\n",
    "t_dataloader = dm.train_dataloader(seed,batch_size)\n",
    "for i in t_dataloader:\n",
    "    l += 1\n",
    "assert l == len(t_dataloader)\n",
    "\n",
    "t_dataloader = dm.val_dataloader(seed,batch_size)\n",
    "x, y = next(iter(t_dataloader))\n",
    "assert x.shape[0] == 256 * 4\n",
    "assert x.shape[1] == 29\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  hours_per_week      workclass     education marital_status  \\\n",
      "0      42.0            45.0        Private       HS-grad        Married   \n",
      "1      32.0            40.0  Self-Employed  Some-college        Married   \n",
      "2      35.0            40.0        Private         Assoc         Single   \n",
      "3      36.0            40.0        Private       HS-grad         Single   \n",
      "4      57.0            35.0        Private        School        Married   \n",
      "...     ...             ...            ...           ...            ...   \n",
      "32556  66.0            40.0        Private     Bachelors        Married   \n",
      "32557  35.0            80.0  Self-Employed       HS-grad        Married   \n",
      "32558  21.0            10.0     Government  Some-college         Single   \n",
      "32559  24.0            40.0        Private  Some-college        Married   \n",
      "32560  46.0            40.0        Private       HS-grad        Married   \n",
      "\n",
      "         occupation   race  gender  income  \n",
      "0       Blue-Collar  White    Male       1  \n",
      "1       Blue-Collar  White    Male       0  \n",
      "2      White-Collar  White  Female       1  \n",
      "3       Blue-Collar  White    Male       0  \n",
      "4           Service  White    Male       0  \n",
      "...             ...    ...     ...     ...  \n",
      "32556  Professional  White    Male       1  \n",
      "32557   Blue-Collar  White    Male       1  \n",
      "32558       Service  White    Male       0  \n",
      "32559         Sales  White    Male       0  \n",
      "32560         Sales  White  Female       0  \n",
      "\n",
      "[32561 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/4qh59pkn75xdzh4r61851p4h0000gn/T/ipykernel_3311/1646380093.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.data = self.data.astype({col: np.float for col in self.continous_cols})\n"
     ]
    }
   ],
   "source": [
    "dm_2 = TabularDataModule(\"adult\")\n",
    "seed=42\n",
    "batch_size=dm_2.batch_size\n",
    "t_dataloader = dm.train_dataloader(seed,batch_size)\n",
    "x, y = next(iter(t_dataloader))\n",
    "print(dm.data)\n",
    "assert x.shape[0] == 256\n",
    "assert x.shape[1] == 29\n",
    "assert dm.sample_frac == 0.1\n",
    "\n",
    "l = 0\n",
    "t_dataloader = dm.train_dataloader(seed,batch_size)\n",
    "for i in t_dataloader:\n",
    "    l += 1\n",
    "assert l == len(t_dataloader)\n",
    "\n",
    "t_dataloader = dm.val_dataloader(seed,batch_size)\n",
    "x, y = next(iter(t_dataloader))\n",
    "assert x.shape[0] == 256 * 4\n",
    "assert x.shape[1] == 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nbdev2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f450d1f6173d6d96822a65433f2c9ed0b856da53e162bc0666cdf9645c72e1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
