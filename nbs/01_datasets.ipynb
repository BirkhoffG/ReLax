{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from cfnet.import_essentials import *\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "def _numpy_collate(batch):\n",
    "  if isinstance(batch[0], np.ndarray):\n",
    "    return np.stack(batch)\n",
    "  elif isinstance(batch[0], (tuple,list)):\n",
    "    transposed = zip(*batch)\n",
    "    return [_numpy_collate(samples) for samples in transposed]\n",
    "  else:\n",
    "    return np.array(batch)\n",
    "\n",
    "class NumpyLoader(DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=_numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataModuleConfigs(BaseParser):\n",
    "    batch_size: int\n",
    "    discret_cols: List[str] = []\n",
    "    continous_cols: List[str] = []\n",
    "    normalizer: Optional[Any] = None\n",
    "    encoder: Optional[Any] = None\n",
    "    sample_frac: Optional[float] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TabularDataModule:\n",
    "    discret_cols: List[str] = []\n",
    "    continous_cols: List[str] = []\n",
    "    normalizer: Optional[Any] = None\n",
    "    encoder: Optional[OneHotEncoder] = None\n",
    "    data: Optional[pd.DataFrame] = None\n",
    "    sample_frac: Optional[float] = None\n",
    "    batch_size: int = 128\n",
    "\n",
    "    def __init__(self, data_configs: Dict):\n",
    "        # read data\n",
    "        self.data = pd.read_csv(Path(data_configs['data_dir']))\n",
    "        # update configs\n",
    "        self._update_configs(data_configs)\n",
    "        # update cat_idx\n",
    "        self.cat_idx = len(self.continous_cols)\n",
    "        # prepare data\n",
    "        self.prepare_data()\n",
    "\n",
    "    def _update_configs(self, configs):\n",
    "        for k, v in configs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        def split_x_and_y(data: pd.DataFrame):\n",
    "            X = data[data.columns[:-1]]\n",
    "            y = data[[data.columns[-1]]]\n",
    "            return X, y\n",
    "\n",
    "        X, y = split_x_and_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        if self.normalizer:\n",
    "            X_cont = self.normalizer.transform(X[self.continous_cols])\n",
    "        else:\n",
    "            self.normalizer = MinMaxScaler()\n",
    "            X_cont = self.normalizer.fit_transform(\n",
    "                X[self.continous_cols]) if self.continous_cols else np.array([[] for _ in range(len(X))])\n",
    "\n",
    "        if self.encoder:\n",
    "            X_cat = self.encoder.transform(X[self.discret_cols])\n",
    "        else:\n",
    "            self.encoder = OneHotEncoder(sparse=False)\n",
    "            X_cat = self.encoder.fit_transform(\n",
    "                X[self.discret_cols]) if self.discret_cols else np.array([[] for _ in range(len(X))])\n",
    "        X = np.concatenate((X_cont, X_cat), axis=1)\n",
    "        # get categorical arrays\n",
    "        self.cat_arrays = self.encoder.categories_ if self.discret_cols else []\n",
    "\n",
    "        # prepare train & test\n",
    "        train_test_tuple = train_test_split(X, y.to_numpy(), shuffle=False)\n",
    "        train_X, test_X, train_y, test_y = map(lambda x: x.astype(jnp.float32), train_test_tuple)\n",
    "        if self.sample_frac:\n",
    "            train_size = int(len(train_X) * self.sample_frac)\n",
    "            train_X, train_y = train_X[:train_size], train_y[:train_size]\n",
    "        self.train_dataset = NumpyDataset(train_X, train_y)\n",
    "        self.val_dataset = NumpyDataset(test_X, test_y)\n",
    "        self.test_dataset = self.val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return NumpyLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return NumpyLoader(self.val_dataset, batch_size=self.batch_size * 4,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return NumpyLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def get_sample_X(self, frac: Optional[float]=None):\n",
    "        if frac is None:\n",
    "            frac = 0.1\n",
    "        train_X, train_y = self.train_dataset[:]\n",
    "        train_size = int(len(train_X) * frac)\n",
    "        return train_X[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {\n",
    "    \"data_dir\": \"assets/data/s_adult.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    \"batch_size\": 256,\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\n",
    "        \"age\",\n",
    "        \"hours_per_week\"\n",
    "    ],\n",
    "    \"discret_cols\": [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"race\",\n",
    "        \"gender\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TabularDataModule(data_configs)\n",
    "dataloader = dm.train_dataloader()\n",
    "x, y = next(iter(dataloader))\n",
    "assert x.shape[0] == 256\n",
    "assert x.shape[1] == 29\n",
    "assert dm.sample_frac == 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[:][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
