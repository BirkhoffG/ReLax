{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "> Evaluating and benchmarking the quality of CF explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "# | default_exp evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.trainer import TrainingConfigs\n",
    "from relax.data import TabularDataModule\n",
    "from relax.utils import accuracy, proximity\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule, BasePredFnCFModule\n",
    "from relax.methods.counternet import CounterNet\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from fastcore.test import test_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@dataclass\n",
    "class Explanation:\n",
    "    \"\"\"Generated CF Explanations class.\"\"\"\n",
    "    cf_name: str  # cf method's name\n",
    "    data_module: TabularDataModule  # data module\n",
    "    cfs: jnp.DeviceArray  # generated cf explanation of `X`\n",
    "    total_time: float  # total runtime\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]  # predict function\n",
    "    dataset_name: str = str()  # dataset name\n",
    "    X: jnp.ndarray = None  # input\n",
    "    y: jnp.ndarray = None  # label\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.data_module:\n",
    "            if self.dataset_name == str():\n",
    "                self.dataset_name = self.data_module.data_name\n",
    "            test_X, label = self.data_module.test_dataset[:]\n",
    "            if self.X is None:\n",
    "                self.X = test_X\n",
    "            if self.y is None:\n",
    "                self.y = label\n",
    "\n",
    "CFExplanationResults = Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/relax/tree/master/blob/master/relax/evaluate.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EXPLANATION\n",
       "\n",
       "::: {.doc-sig}\n",
       "\n",
       " CLASS relax.evaluate.<b>Explanation</b> <em>(cf_name, data_module, cfs, total_time, pred_fn, dataset_name='', X=None, y=None)</em>\n",
       "\n",
       ":::\n",
       "\n",
       "Generated CF Explanations class."
      ],
      "text/plain": [
       "<relax.docs.CustomizedMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to `Explanation`:\n",
    "\n",
    "* `cf_name`: cf method's name\n",
    "* `dataset_name`: dataset name\n",
    "* `X`: input\n",
    "* `y`: label\n",
    "* `cfs`: generated cf explanation of `X`\n",
    "* `total_time`: total runtime\n",
    "* `pred_fn`: predict function with only one input argument, \n",
    "and output a label (i.e., its format is `y=pred_fn(x)`).\n",
    "* `data_module`: data module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseGenerationStrategy:\n",
    "    \"\"\"Base class for mapping strategy.\"\"\"\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        fn: Callable, # Function to generate cf for a single input\n",
    "        X: jnp.ndarray, # Input instances to be explained\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        **kwargs\n",
    "    ) -> Array: # Generated counterfactual explanations\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IterativeGenerationStrategy(BaseGenerationStrategy):\n",
    "    \"\"\"Iterativly generate counterfactuals.\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        fn: Callable, # Function to generate cf for a single input\n",
    "        X: jnp.ndarray, # Input instances to be explained\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        **kwargs\n",
    "    ) -> Array: # Generated counterfactual explanations\n",
    "        \n",
    "        assert X.ndim == 2\n",
    "        cfs = jnp.stack([fn(X[i], pred_fn=pred_fn, **kwargs) for i in range(X.shape[0])])\n",
    "        assert X.shape == cfs.shape\n",
    "        return cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VmapGenerationStrategy(BaseGenerationStrategy):\n",
    "    \"\"\"Generate counterfactuals via `jax.vmap`.\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        fn: Callable, # Function to generate cf for a single input\n",
    "        X: jnp.ndarray, # Input instances to be explained\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        **kwargs\n",
    "    ) -> Array: # Generated counterfactual explanations\n",
    "        \n",
    "        assert X.ndim == 2\n",
    "        partial_fn = partial(fn, pred_fn=pred_fn, **kwargs)\n",
    "        cfs = jax.vmap(partial_fn)(X)\n",
    "        return cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _pad_divisible_X(\n",
    "    X: Array,\n",
    "    n_devices: int\n",
    "):\n",
    "    \"\"\"Pad `X` to be divisible by `n_devices`.\"\"\"\n",
    "    if X.shape[0] % n_devices != 0:\n",
    "        pad_size = n_devices - X.shape[0] % n_devices\n",
    "        X = jnp.concatenate([X, jnp.zeros((pad_size, *X.shape[1:]))])\n",
    "    X_padded = X.reshape(n_devices, -1, *X.shape[1:])\n",
    "    return X_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "X = jnp.ones((5, 29))\n",
    "X_padded = _pad_divisible_X(X, 2)\n",
    "assert X_padded.shape == (2, 3, 29)\n",
    "assert X.sum() == X_padded.sum()\n",
    "\n",
    "X = jnp.ones((5, 29))\n",
    "X_padded = _pad_divisible_X(X, 6)\n",
    "assert X_padded.shape == (6, 1, 29)\n",
    "\n",
    "X = jnp.ones((5, 29))\n",
    "X_padded = _pad_divisible_X(X, 1)\n",
    "assert X_padded.shape == (1, 5, 29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PmapGenerationStrategy(BaseGenerationStrategy):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_devices: int = None, # Number of devices. If None, use all available devices\n",
    "        strategy: str = 'auto', # Strategy to generate counterfactuals\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.strategy = strategy\n",
    "        self.n_devices = n_devices or jax.device_count()\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        fn: Callable, # Function to generate cf for a single input\n",
    "        X: jnp.ndarray, # Input instances to be explained\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        **kwargs\n",
    "    ) -> Array: # Generated counterfactual explanations\n",
    "        \n",
    "        assert X.ndim == 2\n",
    "        X_padded = _pad_divisible_X(X, self.n_devices)\n",
    "        partial_fn = partial(fn, pred_fn=pred_fn, **kwargs)\n",
    "        cfs = jax.pmap(jax.vmap(partial_fn))(X_padded)\n",
    "        cfs = cfs.reshape(-1, *cfs.shape[2:])\n",
    "        cfs = cfs[:X.shape[0]]\n",
    "        return cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "w = jrand.normal(jrand.PRNGKey(0), (100, 100))\n",
    "X = jrand.normal(jrand.PRNGKey(0), (1000, 100))\n",
    "\n",
    "@jit\n",
    "def pred_fn(x): return jnp.dot(x, w.T)\n",
    "\n",
    "def f(x, pred_fn=None, **kwargs):\n",
    "    return pred_fn(x)\n",
    "\n",
    "iter_gen = IterativeGenerationStrategy()\n",
    "vmap_gen = VmapGenerationStrategy()\n",
    "pmap_gen = PmapGenerationStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_iter = iter_gen(f, X, pred_fn=pred_fn).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_vmap = vmap_gen(f, X, pred_fn=pred_fn).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_pmap = pmap_gen(f, X, pred_fn=pred_fn).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.allclose(cf_iter, cf_vmap, atol=1e-4)\n",
    "assert jnp.allclose(cf_iter, cf_pmap, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StrategyFactory(object):\n",
    "    \"\"\"Factory class for Parallelism Strategy.\"\"\"\n",
    "\n",
    "    __strategy_map = {\n",
    "        'iter': IterativeGenerationStrategy(),\n",
    "        'vmap': VmapGenerationStrategy(),\n",
    "        'pmap': PmapGenerationStrategy(),\n",
    "    }\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        raise ValueError(\"This class should not be instantiated.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_default_strategy() -> BaseGenerationStrategy:\n",
    "        \"\"\"Get default strategy.\"\"\"\n",
    "        return VmapGenerationStrategy()\n",
    "\n",
    "    @classmethod\n",
    "    def get_strategy(cls, strategy: str | BaseGenerationStrategy) -> BaseGenerationStrategy:\n",
    "        \"\"\"Get strategy.\"\"\"\n",
    "        if isinstance(strategy, BaseGenerationStrategy):\n",
    "            return strategy\n",
    "        elif isinstance(strategy, str) and strategy in cls.__strategy_map:\n",
    "            return cls.__strategy_map[strategy]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "it = StrategyFactory.get_strategy('iter')\n",
    "vm = StrategyFactory.get_strategy('vmap')\n",
    "pm = StrategyFactory.get_strategy('pmap')\n",
    "default = StrategyFactory.get_default_strategy()\n",
    "cus = StrategyFactory.get_strategy(VmapGenerationStrategy())\n",
    "\n",
    "assert isinstance(it, IterativeGenerationStrategy)\n",
    "assert isinstance(vm, VmapGenerationStrategy)\n",
    "assert isinstance(pm, PmapGenerationStrategy)\n",
    "assert isinstance(default, VmapGenerationStrategy)\n",
    "assert isinstance(cus, VmapGenerationStrategy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating CF Explanation Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _validate_configs(\n",
    "    cf_module: BaseCFModule,\n",
    "    datamodule: TabularDataModule,\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray] = None,\n",
    "    t_configs=None\n",
    "):\n",
    "    if (pred_fn is None) and (not isinstance(cf_module, BasePredFnCFModule)):\n",
    "        warnings.warn(f\"`{type(cf_module).__name__}` is not a subclass of `BasePredFnCFModule`.\"\n",
    "            \"This might cause problems as you set `pred_fn=None`, \"\n",
    "            f\"which infers that `{type(cf_module).__name__}` has an attribute `pred_fn`.\")\n",
    "\n",
    "\n",
    "def _prepare_module(\n",
    "    cf_module: BaseCFModule,\n",
    "    datamodule: TabularDataModule\n",
    "):\n",
    "    cf_module.hook_data_module(datamodule)\n",
    "    return cf_module\n",
    "\n",
    "def _train_parametric_module(\n",
    "    cf_module: BaseParametricCFModule,\n",
    "    datamodule: TabularDataModule,\n",
    "    t_configs=None,\n",
    "    pred_fn=None\n",
    "):\n",
    "    if not cf_module._is_module_trained():\n",
    "        print(f'{type(cf_module).__name__} contains parametric models. '\n",
    "            'Starts training before generating explanations...')\n",
    "        cf_module.train(datamodule, t_configs, pred_fn=pred_fn)\n",
    "    return cf_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _check_aux_pred_fn_args(pred_fn_args: dict | None):\n",
    "    if pred_fn_args is None:\n",
    "        return dict()\n",
    "    elif isinstance(pred_fn_args, dict):\n",
    "        return pred_fn_args\n",
    "    else:\n",
    "        raise ValueError(f'`pred_fn_args` should be a `dict`,',\n",
    "            f'but got `{type(pred_fn_args).__name__}`')\n",
    "\n",
    "class _AuxPredFn:\n",
    "    def __init__(self, pred_fn, pred_fn_args: dict | None):\n",
    "        self.pred_fn = pred_fn\n",
    "        self.fn_args = deepcopy(_check_aux_pred_fn_args(pred_fn_args))\n",
    "\n",
    "    def __call__(self, x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "        return self.pred_fn(x, **self.fn_args)\n",
    "\n",
    "\n",
    "def _check_pred_fn(\n",
    "    pred_fn: callable | None, \n",
    "    cf_module: BaseCFModule\n",
    ") -> callable:\n",
    "    if pred_fn is None:\n",
    "        try:\n",
    "            pred_fn = cf_module.pred_fn\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                    \"`generate_cf_explanations` is incorrectly configured.\"\n",
    "                    f\"It is supposed to be `pred_fn != None`,\"\n",
    "                    f\"or `{type(cf_module).__name__}` has attribute `pred_fn`.\"\n",
    "                    f\"However, we got `pred_fn={pred_fn}`, \"\n",
    "                    f\"and `{type(cf_module).__name__}` has not attribute `pred_fn`.\"\n",
    "            )\n",
    "    elif isinstance(cf_module, BasePredFnCFModule):\n",
    "        # override pred_fn if `cf_module` has `pred_fn`\n",
    "        pred_fn = cf_module.pred_fn\n",
    "    return pred_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_cf_explanations(\n",
    "    cf_module: BaseCFModule, # CF Explanation Module\n",
    "    datamodule: TabularDataModule, # Data Module\n",
    "    pred_fn: callable = None, # Predictive function\n",
    "    strategy: str | BaseGenerationStrategy = 'vmap', # Parallelism Strategy for generating CFs\n",
    "    t_configs: TrainingConfigs = None, # training configs for `BaseParametricCFModule`\n",
    "    pred_fn_args: dict = None # auxiliary arguments for `pred_fn` \n",
    ") -> Explanation:\n",
    "    \"\"\"Generate CF explanations.\"\"\"\n",
    "\n",
    "    _validate_configs(cf_module, datamodule, pred_fn, t_configs)\n",
    "    cf_module = _prepare_module(cf_module, datamodule)\n",
    "\n",
    "    if isinstance(cf_module, BaseParametricCFModule):\n",
    "        cf_module = _train_parametric_module(\n",
    "            cf_module, datamodule, t_configs=t_configs, pred_fn=pred_fn\n",
    "        )\n",
    "    X, _ = datamodule.test_dataset[:]\n",
    "    \n",
    "    # create `pred_fn` which only takes `x` as an input\n",
    "    if pred_fn is not None:\n",
    "        pred_fn = _AuxPredFn(pred_fn, pred_fn_args=pred_fn_args)\n",
    "\n",
    "    strategy = StrategyFactory.get_strategy(strategy)\n",
    "    current_time = time.time()\n",
    "    cfs = strategy(cf_module.generate_cf, X, pred_fn=pred_fn)\n",
    "    total_time = time.time() - current_time\n",
    "\n",
    "    # check pred_fn\n",
    "    pred_fn = _check_pred_fn(pred_fn, cf_module)\n",
    "\n",
    "    return Explanation(\n",
    "        cf_name=cf_module.name,\n",
    "        data_module=datamodule,\n",
    "        cfs=cfs,\n",
    "        total_time=total_time,\n",
    "        pred_fn=pred_fn,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pred_fn` in `generate_cf_explanations` is a model's prediction function. \n",
    "The general format is `y = pred_fn(x, **pred_fn_args)`. \n",
    "If `pred_fn` is not parameterized by other variables (except input `x`), \n",
    "then `pred_fn_args` is set to `None`, which is the default setting.\n",
    "Otherwise, you should pass these argument as a `dict`.\n",
    "\n",
    "For example, we have a simple linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_pred_fn(x: jnp.DeviceArray, params: jnp.DeviceArray):\n",
    "    return x @ params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass `linear_pred_fn` to `generate_cf_explanations`, \n",
    "we can either create an auxiliary function of `linear_pred_fn`,\n",
    "or pass `params` into `pred_fn_args`.\n",
    "\n",
    "Assuming we now have the input `x` and `params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "x = jax.random.normal(random.PRNGKey(0), shape=(5, 10)) # input\n",
    "params = jnp.ones((10, 1)) # params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an auxillary function (Not recommended)\n",
    "\n",
    "```python\n",
    "aux_linear_pred_fn = lambda x: linear_pred_fn(x, params)\n",
    "explanations = generate_cf_explanations(\n",
    "    cf_module, datamodule, aux_linear_pred_fn\n",
    ")\n",
    "```\n",
    "\n",
    "This approach could work, but if `params` is changed, \n",
    "`explanations.pred_fn` might not work as expected.\n",
    "\n",
    "2. Pass `params` into `pred_fn_args`\n",
    "\n",
    "```python\n",
    "explanations = generate_cf_explanations(\n",
    "    cf_module, datamodule, linear_pred_fn, \n",
    "    pred_fn_args=dict(params=params)\n",
    ")\n",
    "```\n",
    "\n",
    "This is a recommended approach as we will deepcopy `params` inside `generate_cf_explanations`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pred_fn` in `explanations` only takes `x: jnp.DeviceArray` as an input.\n",
    "For example, to make predictions, we use\n",
    "\n",
    "```python\n",
    "y = explanations.pred_fn(x)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseEvalMetrics(ABC):\n",
    "    \"\"\"Base evaluation metrics class.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = None):\n",
    "        if name is None: name = type(self).__name__\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        has_name = hasattr(self, 'name')\n",
    "        if not has_name:\n",
    "            raise ValidationError(\n",
    "                \"EvalMetrics must have a name. Add the following as the first line in your \"\n",
    "                f\"__init__ method:\\n\\nsuper({self.__name__}, self).__init__()\")\n",
    "        return self.name\n",
    "\n",
    "    def __call__(self, cf_explanations: Explanation) -> Any:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_acc(\n",
    "    input: jnp.DeviceArray, # input dim: [N, k]\n",
    "    label: jnp.DeviceArray, # label dim: [N] or [N, 1]\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    ") -> float:\n",
    "    y_pred = pred_fn(input).reshape(-1, 1).round()\n",
    "    label = label.reshape(-1, 1)\n",
    "    return accuracy(y_pred, label).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fake_pred_fn = lambda x: x.clip(0, 1).mean(axis=1)\n",
    "inputs = jnp.array([\n",
    "    [0., 0.99], [0.1, 0.1], [0.99, 0.1], [0.99, 0.99] # [0, 0, 1, 1]\n",
    "])\n",
    "labels_1 = jnp.array([0, 0, 1, 1])\n",
    "labels_2 = jnp.array([0, 1, 0, 1])\n",
    "assert _compute_acc(inputs, labels_1, fake_pred_fn) == 1.0\n",
    "assert _compute_acc(inputs, labels_2, fake_pred_fn) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PredictiveAccuracy(BaseEvalMetrics):\n",
    "    \"\"\"Compute the accuracy of the predict function.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"accuracy\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, y = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_acc(X, y, cf_explanations.pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "_acc = PredictiveAccuracy()\n",
    "assert _acc.name == \"accuracy\"\n",
    "assert str(_acc) == \"accuracy\"\n",
    "_acc = PredictiveAccuracy('acc')\n",
    "assert _acc.name == \"acc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_val(\n",
    "    input: jnp.DeviceArray, # input dim: [N, k]\n",
    "    cfs: jnp.DeviceArray, # cfs dim: [N, k]\n",
    "    pred_fn: Callable[[jnp.DeviceArray], jnp.DeviceArray]\n",
    "):\n",
    "    y_pred = pred_fn(input).reshape(-1, 1).round()\n",
    "    y_prime = jnp.ones_like(y_pred) - y_pred\n",
    "    cf_y = pred_fn(cfs).reshape(-1, 1).round()\n",
    "    return accuracy(y_prime, cf_y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fake_pred_fn = lambda x: x.clip(0, 1).mean(axis=1)\n",
    "inputs = jnp.array([\n",
    "    [0., 0.99], [0.1, 0.1], [0.99, 0.1], [0.99, 0.99] # [0, 0, 1, 1]\n",
    "])\n",
    "cfs_1 = jnp.array([\n",
    "    [0.99, 0.99], [0.1, 0.1], [0.1, 0.1], [0., 0.99] # [1, 0, 0, 0]\n",
    "])\n",
    "cfs_2 = jnp.array([\n",
    "    [0.99, 0.], [0.1, 0.1], [0.1, 0.1], [0., 0.99] # [0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "assert _compute_val(inputs, cfs_1, fake_pred_fn) == 0.75\n",
    "assert _compute_val(inputs, cfs_2, fake_pred_fn) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Validity(BaseEvalMetrics):\n",
    "    \"\"\"Compute fraction of input instances on which CF explanation methods output valid CF examples.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"validity\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_val(\n",
    "            X, cf_explanations.cfs, cf_explanations.pred_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_proximity(\n",
    "    inputs: jnp.DeviceArray, # input dim: [N, k]\n",
    "    cfs: jnp.DeviceArray, # cfs dim: [N, k]\n",
    "):\n",
    "    prox = jnp.linalg.norm(inputs - cfs, ord=1, axis=1).mean()\n",
    "    return prox.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = jnp.array([\n",
    "    [0, 1], [1, 0], [1, 1]])\n",
    "cfs_1 = jnp.array([\n",
    "    [0, 0], [0, 0], [0, 1]])\n",
    "cfs_2 = jnp.array([\n",
    "    [1, 0], [1, -2], [0, 0]])\n",
    "assert _compute_proximity(inputs, cfs_1) == 1.0\n",
    "assert _compute_proximity(inputs, cfs_2) == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Proximity(BaseEvalMetrics):\n",
    "    \"\"\"Compute L1 norm distance between input datasets and CF examples divided by the number of features.\"\"\"\n",
    "    def __init__(self, name: str = \"proximity\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_proximity(X, cf_explanations.cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_spar(\n",
    "    input: jnp.DeviceArray,\n",
    "    cfs: jnp.DeviceArray,\n",
    "    cat_idx: int\n",
    "):\n",
    "    # calculate sparsity\n",
    "    cat_sparsity = proximity(input[:, cat_idx:], cfs[:, cat_idx:]) / 2\n",
    "    cont_sparsity = jnp.linalg.norm(\n",
    "        jnp.abs(input[:, :cat_idx] - cfs[:, :cat_idx]), ord=0, axis=1\n",
    "    ).mean()\n",
    "    return (cont_sparsity + cat_sparsity).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsity(BaseEvalMetrics):\n",
    "    \"\"\"Compute the number of feature changes between input datasets and CF examples.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"sparsity\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_spar(X, cf_explanations.cfs, cf_explanations.cat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _compute_manifold_dist(\n",
    "    input: jnp.DeviceArray,\n",
    "    cfs: jnp.DeviceArray,\n",
    "    n_neighbors: int = 1,\n",
    "    p: int = 2\n",
    "):\n",
    "    knn = NearestNeighbors(n_neighbors=n_neighbors, p=p)\n",
    "    knn.fit(input)\n",
    "    nearest_dist, nearest_points = knn.kneighbors(cfs, 1, return_distance=True)\n",
    "    return jnp.mean(nearest_dist).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ManifoldDist(BaseEvalMetrics):\n",
    "    \"\"\"Compute the L1 distance to the n-nearest neighbor for all CF examples.\"\"\"\n",
    "    def __init__(self, n_neighbors: int = 1, p: int = 2, name: str = \"manifold_dist\"):\n",
    "        super().__init__(name=name)\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        X, _ = cf_explanations.data_module.test_dataset[:]\n",
    "        return _compute_manifold_dist(\n",
    "            X, cf_explanations.cfs, self.n_neighbors, self.p\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Runtime(BaseEvalMetrics):\n",
    "    \"\"\"Get the running time to generate CF examples.\"\"\"\n",
    "    def __init__(self, name: str = \"runtime\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, cf_explanations: Explanation) -> float:\n",
    "        return cf_explanations.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pred_fn_test = lambda x: jnp.clip(x + 0.5, 0., 1)\n",
    "\n",
    "x_1 = jnp.array([[0.1], [0.34], [0.4], [-0.2], [0.7]])\n",
    "cf_1 = jnp.array([[-0.2], [0.4], [-0.1], [0.7], [-0.1]])\n",
    "y_1 = jnp.array([1., 0., 1., 1., 1.])\n",
    "x_2 = jnp.array([[-0.5], [0.34], [0.4], [-0.2], [0.7]])\n",
    "cf_2 = jnp.array([[0.2], [0.4], [-0.1], [0.7], [-0.1]])\n",
    "y_2 = jnp.array([[0.], [1.], [1.], [1.], [1.]])\n",
    "\n",
    "_acc_1 = _compute_acc(x_1, y_1, pred_fn_test)\n",
    "_acc_2 = _compute_acc(x_2, y_2, pred_fn_test)\n",
    "_val_1 = _compute_val(x_1, cf_1, pred_fn_test)\n",
    "_val_2 = _compute_val(x_2, cf_2, pred_fn_test)\n",
    "\n",
    "assert jnp.isclose(_acc_1, 0.6)\n",
    "assert jnp.isclose(_acc_2, 0.8)\n",
    "assert jnp.isclose(_val_1, 0.8)\n",
    "assert jnp.isclose(_val_2, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _create_second_order_cfs(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    X, y = cf_results.data_module.test_dataset[:]\n",
    "    cfs = cf_results.cfs\n",
    "    scaler = cf_results.data_module.normalizer\n",
    "    cat_idx = cf_results.data_module.cat_idx\n",
    "\n",
    "    # get normalized threshold = threshold / (max - min)\n",
    "    data_range = scaler.data_range_\n",
    "    thredshold_normed = threshold / data_range\n",
    "\n",
    "    # select continous features\n",
    "    x_cont = X[:, :cat_idx]\n",
    "    cf_cont = cfs[:, :cat_idx]\n",
    "    # calculate the diff between x and c\n",
    "    cont_diff = jnp.abs(x_cont - cf_cont) <= thredshold_normed\n",
    "    # new cfs\n",
    "    cfs_cont_hat = jnp.where(cont_diff, x_cont, cf_cont)\n",
    "\n",
    "    cfs_hat = jnp.concatenate((cfs_cont_hat, cfs[:, cat_idx:]), axis=-1)\n",
    "    return cfs_hat\n",
    "\n",
    "\n",
    "def compute_so_validity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_validity = Validity()\n",
    "    return compute_validity(cf_results_so)\n",
    "\n",
    "\n",
    "def compute_so_proximity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_proximity = Proximity()\n",
    "    return compute_proximity(cf_results_so)\n",
    "\n",
    "\n",
    "def compute_so_sparsity(cf_results: CFExplanationResults, threshold: float = 2.0):\n",
    "    cfs_hat = _create_second_order_cfs(cf_results, threshold)\n",
    "    cf_results_so = deepcopy(cf_results)\n",
    "    cf_results_so.cfs = cfs_hat\n",
    "    compute_sparsity = Sparsity()\n",
    "    return compute_sparsity(cf_results_so)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def fake_explanations():\n",
    "    \"\"\"Generate sudo explanations for testing.\"\"\"\n",
    "    from relax.data import load_data\n",
    "\n",
    "    dm = load_data(\"adult\")\n",
    "    X, y = dm.test_dataset[:]\n",
    "    cfs = X\n",
    "    dn = dm.data_name\n",
    "    pred_fn = lambda x: jax.random.bernoulli(jax.random.PRNGKey(0), 0.5, (x.shape[0], 1)).astype(float)\n",
    "    assert y.shape == pred_fn(X).shape\n",
    "    return Explanation(\n",
    "        cf_name='sudo', data_module=dm, cfs=cfs, pred_fn=pred_fn, total_time=0.0, dataset_name=dn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# METRICS = dict(\n",
    "#     acc=PredictiveAccuracy(),\n",
    "#     accuracy=PredictiveAccuracy(),\n",
    "#     validity=Validity(),\n",
    "#     proximity=Proximity(),\n",
    "#     runtime=Runtime(),\n",
    "#     manifold_dist=ManifoldDist(),\n",
    "#     # validity=compute_so_validity,\n",
    "#     # so_proximity=compute_so_proximity,\n",
    "#     # so_sparsity=compute_so_sparsity\n",
    "# )\n",
    "\n",
    "METRICS_CALLABLE = [\n",
    "    PredictiveAccuracy('acc'),\n",
    "    PredictiveAccuracy('accuracy'),\n",
    "    Validity(),\n",
    "    Proximity(),\n",
    "    Runtime(),\n",
    "    ManifoldDist(),\n",
    "]\n",
    "\n",
    "METRICS = { m.name: m for m in METRICS_CALLABLE }\n",
    "\n",
    "DEFAULT_METRICS = [\"acc\", \"validity\", \"proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for m in METRICS.keys(): assert isinstance(m, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_metric(metric: str | BaseEvalMetrics, cf_exp: Explanation):\n",
    "    if isinstance(metric, str):\n",
    "        if metric not in METRICS.keys():\n",
    "            raise ValueError(f\"'{metric}' is not supported. Must be one of {METRICS.keys()}\")\n",
    "        res = METRICS[metric](cf_exp)\n",
    "    elif callable(metric):\n",
    "        # f(cf_exp) not supported for now\n",
    "        if not isinstance(metric, BaseEvalMetrics):\n",
    "            raise ValueError(f\"metric needs to be a subclass of `BaseEvalMetrics`.\")\n",
    "        res = metric(cf_exp)\n",
    "    else:\n",
    "        raise ValueError(f\"{type(metric).__name__} is not supported as a metric.\")\n",
    "    \n",
    "    if isinstance(res, jnp.ndarray) and res.shape == (1,):\n",
    "        res = res.item()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "exp = fake_explanations()\n",
    "_acc_1 = _get_metric('acc', exp)\n",
    "test_fail(lambda: _get_metric('acc_1', exp), contains='is not supported')\n",
    "_acc_2 = _get_metric(PredictiveAccuracy(), exp)\n",
    "assert jnp.allclose(_acc_1, _acc_2)\n",
    "# functional callable not supported\n",
    "test_fail(lambda: _get_metric(Proximity, exp), contains='needs to be a subclass')\n",
    "test_fail(lambda: _get_metric(lambda: 1., exp), contains='needs to be a subclass') \n",
    "\n",
    "for m in METRICS_CALLABLE:\n",
    "    _res = _get_metric(m, exp)\n",
    "    assert isinstance(_res, (int, float))\n",
    "    assert not isinstance(_res, jnp.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def evaluate_cfs(\n",
    "    cf_exp: Explanation, # CF Explanations\n",
    "    metrics: Iterable[Union[str, BaseEvalMetrics]] = None, # A list of Metrics. Can be `str` or a subclass of `BaseEvalMetrics`\n",
    "    return_dict: bool = True, # return a dictionary or not (default: True)\n",
    "    return_df: bool = False # return a pandas Dataframe or not (default: False)\n",
    "):\n",
    "    cf_name = cf_exp.cf_name\n",
    "    data_name = cf_exp.data_module.data_name\n",
    "    result_dict = { (data_name, cf_name): dict() }\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = DEFAULT_METRICS\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_name = str(metric)\n",
    "        result_dict[(data_name, cf_name)][metric_name] = _get_metric(metric, cf_exp)\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient=\"index\")\n",
    "    \n",
    "    if return_dict and return_df:\n",
    "        return (result_dict, result_df)\n",
    "    elif return_dict or return_df:\n",
    "        return result_df if return_df else result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('adult', 'sudo'): {'accuracy': 0.4939196705818176, 'validity': 0.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "exp = fake_explanations()\n",
    "evaluate_cfs(exp)\n",
    "evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"])\n",
    "d, df = evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"], return_df=True)\n",
    "assert isinstance(d, dict)\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "df = evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"], return_df=True, return_dict=False)\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "evaluate_cfs(exp, metrics=[PredictiveAccuracy(), Validity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def benchmark_cfs(\n",
    "    cf_results_list: Iterable[CFExplanationResults],\n",
    "    metrics: Optional[Iterable[str]] = None,\n",
    "):\n",
    "    dfs = [\n",
    "        evaluate_cfs(\n",
    "            cf_exp=cf_results, metrics=metrics, return_dict=False, return_df=True\n",
    "        )\n",
    "        for cf_results in cf_results_list\n",
    "    ]\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a CF Explanation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.module import PredictiveTrainingModule\n",
    "from relax.trainer import train_model\n",
    "from relax.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_json('assets/configs/data_configs/adult.json')\n",
    "m_configs = configs['mlp_configs']\n",
    "data_configs = configs['data_configs']\n",
    "data_configs['sample_frac'] = 0.1\n",
    "\n",
    "t_configs = {\n",
    "    'n_epochs': 10,\n",
    "    'monitor_metrics': 'val/val_loss',\n",
    "    'seed': 42,\n",
    "    \"batch_size\": 256\n",
    "} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 96/96 [00:01<00:00, 53.81batch/s, train/train_loss_1=0.0791]\n"
     ]
    }
   ],
   "source": [
    "training_module = PredictiveTrainingModule(m_configs)\n",
    "dm = TabularDataModule(data_configs)\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    training_module, \n",
    "    dm, \n",
    "    t_configs\n",
    ")\n",
    "pred_fn = lambda x, params, prng_key: \\\n",
    "    training_module.forward(params, prng_key, x, is_training=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start to benchmark different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.methods import VanillaCF, CounterNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for `VanillaCF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 92.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "vanillacf = VanillaCF(dict(n_steps=1000, lr=0.001))\n",
    "vanillacf_exp = generate_cf_explanations(\n",
    "    vanillacf, dm, pred_fn,\n",
    "    pred_fn_args=dict(params=params, prng_key=random.PRNGKey(0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "assert vanillacf_exp.cf_name == vanillacf.name\n",
    "assert vanillacf_exp.dataset_name == dm.data_name\n",
    "assert vanillacf_exp.X.shape == vanillacf_exp.cfs.shape\n",
    "assert vanillacf_exp.pred_fn(vanillacf_exp.X).shape == vanillacf_exp.y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for `CounterNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterNet contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 191/191 [00:03<00:00, 58.07batch/s, train/train_loss_1=0.0657, train/train_loss_2=0.000985, train/train_loss_3=0.0963]\n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "counternet = CounterNet()\n",
    "counternet_exp = generate_cf_explanations(counternet, dm, pred_fn=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `CounterNet` contains a predictive module, so we set `pred_fn=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| slow\n",
    "assert counternet_exp.cf_name == counternet.name\n",
    "assert counternet_exp.dataset_name == dm.data_name\n",
    "assert counternet_exp.X.shape == counternet_exp.cfs.shape\n",
    "assert counternet_exp.pred_fn(counternet_exp.X).shape == counternet_exp.y.shape\n",
    "assert counternet_exp.pred_fn == counternet.pred_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `cf_module` is a subclass of `BasePredFnCFModule` (e.g., `CounterNet`),\n",
    "the `pred_fn` in `Explanation` will be set to `cf_module.pred_fn`,\n",
    "and the `pred_fn` argument passed `generate_cf_explanations` will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CounterNet contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 191/191 [00:02<00:00, 64.02batch/s, train/train_loss_1=0.0713, train/train_loss_2=0.000427, train/train_loss_3=0.0944]\n"
     ]
    }
   ],
   "source": [
    "#| slow\n",
    "counternet_exp_1 = generate_cf_explanations(counternet, dm, pred_fn=pred_fn)\n",
    "assert counternet_exp_1.pred_fn != pred_fn\n",
    "assert counternet_exp_1.pred_fn == counternet.pred_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute metrics for benchmarking different CF explanation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>VanillaCF</th>\n",
       "      <td>0.822012</td>\n",
       "      <td>0.93674</td>\n",
       "      <td>7.62256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      acc  validity proximity\n",
       "adult VanillaCF  0.822012   0.93674   7.62256"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| slow\n",
    "evaluate_cfs(vanillacf_exp, return_df=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>CounterNet</th>\n",
       "      <td>0.831347</td>\n",
       "      <td>0.958605</td>\n",
       "      <td>5.9374576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       acc  validity  proximity\n",
       "adult CounterNet  0.831347  0.958605  5.9374576"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| slow\n",
    "evaluate_cfs(counternet_exp, return_df=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">adult</th>\n",
       "      <th>VanillaCF</th>\n",
       "      <td>0.822012</td>\n",
       "      <td>0.936740</td>\n",
       "      <td>7.62256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CounterNet</th>\n",
       "      <td>0.831347</td>\n",
       "      <td>0.958605</td>\n",
       "      <td>5.9374576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       acc  validity  proximity\n",
       "adult VanillaCF   0.822012  0.936740    7.62256\n",
       "      CounterNet  0.831347  0.958605  5.9374576"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| slow\n",
    "benchmark_cfs([vanillacf_exp, counternet_exp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
