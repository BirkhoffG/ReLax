{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Module\n",
    "\n",
    "> `DataModule` for training parametric models, generating CF explanations, and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.utils import load_json, validate_configs, cat_normalize\n",
    "from cfnet.data.loader import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataModule(ABC):\n",
    "    \"\"\"DataModule Interface\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def data_name(self) -> str: \n",
    "        return\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def data(self) -> Any:\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def val_dataset(self) -> Dataset:\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return\n",
    "\n",
    "    def train_dataloader(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def val_dataloader(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_dataloader(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_data(self) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data) -> jnp.DeviceArray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, x: jnp.DeviceArray) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply_constraints(self, x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabula Data Module\n",
    "\n",
    "`DataModule` for processing tabular data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_imutable_idx_list(\n",
    "    imutable_col_names: List[str],\n",
    "    discrete_col_names: List[str],\n",
    "    continuous_col_names: List[str],\n",
    "    cat_arrays: List[List[str]],\n",
    ") -> List[int]:\n",
    "    imutable_idx_list = []\n",
    "    for idx, col_name in enumerate(continuous_col_names):\n",
    "        if col_name in imutable_col_names:\n",
    "            imutable_idx_list.append(idx)\n",
    "\n",
    "    cat_idx = len(continuous_col_names)\n",
    "\n",
    "    for i, (col_name, cols) in enumerate(zip(discrete_col_names, cat_arrays)):\n",
    "        cat_end_idx = cat_idx + len(cols)\n",
    "        if col_name in imutable_col_names:\n",
    "            imutable_idx_list += list(range(cat_idx, cat_end_idx))\n",
    "        cat_idx = cat_end_idx\n",
    "    return imutable_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _check_cols(data: pd.DataFrame, configs: TabularDataModuleConfigs) -> pd.DataFrame:\n",
    "    data = data.astype({\n",
    "        col: float for col in configs.continous_cols\n",
    "    })\n",
    "    \n",
    "    cols = configs.continous_cols + configs.discret_cols\n",
    "    # check target columns\n",
    "    target_col = data.columns[-1]\n",
    "    assert not target_col in cols, \\\n",
    "        f\"continous_cols or discret_cols contains target_col={target_col}.\"\n",
    "    \n",
    "    # check imutable cols\n",
    "    for col in configs.imutable_cols:\n",
    "        assert col in cols, \\\n",
    "            f\"imutable_cols=[{col}] is not specified in `continous_cols` or `discret_cols`.\"\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _process_data(\n",
    "    df: pd.DataFrame | None, configs: TabularDataModuleConfigs\n",
    ") -> pd.DataFrame:\n",
    "    if df is None:\n",
    "        df = pd.read_csv(configs.data_dir)\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "        df = df\n",
    "    else:\n",
    "        raise ValueError(f\"{type(df).__name__} is not supported as an input type for `TabularDataModule`.\")\n",
    "\n",
    "    df = _check_cols(df, configs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _transform_df(\n",
    "    transformer: TransformerMixin,\n",
    "    data: pd.DataFrame,\n",
    "    cols: List[str] | None,\n",
    "):\n",
    "    return (\n",
    "        transformer.transform(data[cols])\n",
    "            if cols else np.array([[] for _ in range(len(data))])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test\n",
    "df = pd.read_csv('assets/data/s_adult.csv')\n",
    "cols = ['age', 'hours_per_week']\n",
    "sca = MinMaxScaler().fit(df[cols])\n",
    "x = _transform_df(sca, df, cols)\n",
    "assert x.shape == (len(df), len(cols))\n",
    "\n",
    "cols = []\n",
    "sca = MinMaxScaler()\n",
    "x = _transform_df(sca, df, cols)\n",
    "assert x.shape == (len(df), len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _inverse_transform_np(\n",
    "    transformer: TransformerMixin,\n",
    "    x: jnp.DeviceArray,\n",
    "    cols: List[str] | None\n",
    "):\n",
    "    assert len(cols) <= x.shape[-1], \\\n",
    "        f\"x.shape={x.shape} probably will not match len(cols)={len(cols)}\"\n",
    "    if cols:\n",
    "        data = transformer.inverse_transform(x)\n",
    "        return pd.DataFrame(data=data, columns=cols)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test\n",
    "df = pd.read_csv('assets/data/s_adult.csv')\n",
    "cols = ['age', 'hours_per_week']\n",
    "sca = MinMaxScaler().fit(df[cols])\n",
    "x = _transform_df(sca, df, cols)\n",
    "data = _inverse_transform_np(sca, x, cols)\n",
    "\n",
    "assert x.shape == (len(df), len(cols))\n",
    "assert np.allclose(df[cols].values, data.values)\n",
    "\n",
    "cols = []\n",
    "sca = MinMaxScaler()\n",
    "x = _transform_df(sca, df, cols)\n",
    "data = _inverse_transform_np(sca, x, cols)\n",
    "\n",
    "assert x.shape == (len(df), len(cols))\n",
    "assert data is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test\n",
    "df = pd.read_csv('assets/data/s_adult.csv')\n",
    "cols = ['workclass', 'education']\n",
    "sca = OneHotEncoder().fit(df[cols])\n",
    "x = _transform_df(sca, df, cols)\n",
    "data = _inverse_transform_np(sca, x, cols)\n",
    "\n",
    "assert x.shape[0] == len(df)\n",
    "assert df[cols].equals(data)\n",
    "\n",
    "cols = []\n",
    "sca = OneHotEncoder()\n",
    "x = _transform_df(sca, df, cols)\n",
    "data = _inverse_transform_np(sca, x, cols)\n",
    "\n",
    "assert x.shape == (len(df), len(cols))\n",
    "assert data is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _init_scalar_encoder(\n",
    "    data: pd.DataFrame,\n",
    "    configs: TabularDataModuleConfigs\n",
    "):  \n",
    "    # fit scalar\n",
    "    if configs.normalizer:\n",
    "        scalar = configs.normalizer\n",
    "    else:\n",
    "        scalar = MinMaxScaler()\n",
    "        if configs.continous_cols:\n",
    "            scalar.fit(data[configs.continous_cols])\n",
    "    \n",
    "    X_cont = _transform_df(\n",
    "        scalar, data, configs.continous_cols\n",
    "    )\n",
    "\n",
    "    # fit encoder\n",
    "    if configs.encoder:\n",
    "        encoder = configs.encoder\n",
    "    else:\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        if configs.discret_cols:\n",
    "            encoder.fit(data[configs.discret_cols])\n",
    "    \n",
    "    X_cat = _transform_df(\n",
    "        encoder, data, configs.discret_cols\n",
    "    )\n",
    "    return dict(\n",
    "        X_cont=X_cont, X_cat=X_cat, scalar=scalar, encoder=encoder\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TabularDataModuleConfigs(BaseParser):\n",
    "    \"\"\"Config of `TabularDataModule`.\"\"\"\n",
    "\n",
    "    data_dir: str\n",
    "    data_name: str\n",
    "    discret_cols: List[str] = []\n",
    "    continous_cols: List[str] = []\n",
    "    imutable_cols: List[str] = []\n",
    "    normalizer: Optional[Any] = None\n",
    "    encoder: Optional[Any] = None\n",
    "    sample_frac: Optional[float] = None\n",
    "    backend: str = 'jax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TabularDataModule(BaseDataModule):\n",
    "    \"\"\"DataModule for tabular data\"\"\"\n",
    "    cont_scalar = None # scalar for normalizing continuous features\n",
    "    cat_encoder = None # encoder for encoding categorical features\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_config: dict | TabularDataModuleConfigs, # Configurator of `TabularDataModule`\n",
    "        df: pd.DataFrame = None # Dataframe which overrides `data_dir` in `data_config` (if not None)\n",
    "    ):\n",
    "        self._configs: TabularDataModuleConfigs = validate_configs(\n",
    "            data_config, TabularDataModuleConfigs\n",
    "        )\n",
    "        self._data = _process_data(df, self._configs)\n",
    "        # init idx lists\n",
    "        self.cat_idx = len(self._configs.continous_cols)\n",
    "        self._imutable_idx_list = []\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        scalar_encoder_dict = _init_scalar_encoder(\n",
    "            data=self._data, configs=self._configs\n",
    "        )\n",
    "        self.cont_scalar = scalar_encoder_dict['scalar']\n",
    "        self.cat_encoder = scalar_encoder_dict['encoder']\n",
    "        X = np.concatenate(\n",
    "            (scalar_encoder_dict['X_cont'], scalar_encoder_dict['X_cat']),\n",
    "            axis=1\n",
    "        )\n",
    "        y  = self._data.iloc[:, -1:] # last column is the target columns\n",
    "\n",
    "        self._imutable_idx_list = find_imutable_idx_list(\n",
    "            imutable_col_names=self._configs.imutable_cols,\n",
    "            discrete_col_names=self._configs.discret_cols,\n",
    "            continuous_col_names=self._configs.continous_cols,\n",
    "            cat_arrays=self.cat_encoder.categories_,\n",
    "        )\n",
    "        \n",
    "        # prepare train & test\n",
    "        train_test_tuple = train_test_split(X, y.to_numpy(), shuffle=False)\n",
    "        train_X, test_X, train_y, test_y = map(\n",
    "             lambda x: x.astype(float), train_test_tuple\n",
    "         )\n",
    "        if self._configs.sample_frac:\n",
    "            train_size = int(len(train_X) * self._configs.sample_frac)\n",
    "            train_X, train_y = train_X[:train_size], train_y[:train_size]\n",
    "        \n",
    "        self._train_dataset = Dataset(train_X, train_y)\n",
    "        self._val_dataset = Dataset(test_X, test_y)\n",
    "        self._test_dataset = self.val_dataset\n",
    "\n",
    "    @property\n",
    "    def data_name(self) -> str: \n",
    "        return self._configs.data_name\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> Any:\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def train_dataset(self) -> Dataset:\n",
    "        return self._train_dataset\n",
    "    \n",
    "    @property\n",
    "    def val_dataset(self) -> Dataset:\n",
    "        return self._val_dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self) -> Dataset:\n",
    "        return self._test_dataset\n",
    "\n",
    "    def train_dataloader(self, batch_size):\n",
    "        return DataLoader(self.train_dataset, self._configs.backend, \n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val_dataset, self._configs.backend,\n",
    "            batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False\n",
    "        )\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        # TODO: validate `data`\n",
    "        X_cont = _transform_df(\n",
    "            self.cont_scalar, data, self._configs.continous_cols\n",
    "        )\n",
    "        X_cat = _transform_df(\n",
    "            self.cat_encoder, data, self._configs.discret_cols\n",
    "        )\n",
    "        return np.concatenate((X_cont, X_cat), axis=1)\n",
    "\n",
    "    def inverse_transform(self, x: jnp.DeviceArray) -> pd.DataFrame:\n",
    "        X_cont_df = _inverse_transform_np(\n",
    "            self.cont_scalar, x[:, :self.cat_idx], self._configs.continous_cols\n",
    "        )\n",
    "        X_cat_df = _inverse_transform_np(\n",
    "            self.cat_encoder, x[:, self.cat_idx:], self._configs.discret_cols\n",
    "        )\n",
    "        return pd.concat(\n",
    "            [X_cont_df, X_cat_df], axis=1\n",
    "        )\n",
    "\n",
    "    def apply_constraints(\n",
    "        self, \n",
    "        cf: jnp.DeviceArray, \n",
    "        hard: bool = False\n",
    "    ) -> jnp.DeviceArray:\n",
    "        cat_arrays = self.cat_encoder.categories_ \\\n",
    "            if self._configs.discret_cols else []\n",
    "        cf = cat_normalize(\n",
    "            cf, cat_arrays=cat_arrays, \n",
    "            cat_idx=len(self._configs.continous_cols),\n",
    "            hard=hard\n",
    "        )\n",
    "        return cf\n",
    "\n",
    "    def project(self, x: jnp.DeviceArray, cf: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "        cf = cf.at[:, self._imutable_idx_list].set(x[:, self._imutable_idx_list])\n",
    "        return cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def samples(datamodule: BaseDataModule, frac: float = 1.0): \n",
    "    X, y = datamodule.train_dataset[:]\n",
    "    size = int(len(X) * frac)\n",
    "    return X[:size], y[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def check_datamodule(dm: TabularDataModule, data_configs: dict):\n",
    "    batch_size = 256\n",
    "\n",
    "    X, y = dm.train_dataset[:]\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    X, y = dm.val_dataset[:]\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    X, y = dm.test_dataset[:]\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    dl = dm.train_dataloader(batch_size)\n",
    "    x, y = next(iter(dl))\n",
    "    assert x.shape[0] == batch_size\n",
    "\n",
    "    dl = dm.val_dataloader(batch_size)\n",
    "    x, y = next(iter(dl))\n",
    "    assert x.shape[0] == batch_size\n",
    "\n",
    "    dl = dm.test_dataloader(batch_size)\n",
    "    x, y = next(iter(dl))\n",
    "    assert x.shape[0] == batch_size\n",
    "\n",
    "    ############################################################\n",
    "    # test `transform` and `inverse_transform`\n",
    "    ############################################################\n",
    "    df = dm.inverse_transform(X)\n",
    "    assert len(df) == len(X)\n",
    "    assert len(df.columns) == \\\n",
    "        len(data_configs['continous_cols'] + data_configs['discret_cols']) \n",
    "\n",
    "    data = dm.transform(df)\n",
    "    assert np.allclose(X, data)\n",
    "    \n",
    "    ############################################################\n",
    "    # test `apply_constraints` ad `projec`\n",
    "    ##########################################################\n",
    "    cat_idx = len(data_configs['continous_cols'])\n",
    "    n_cat_feat = len(data_configs['discret_cols'])\n",
    "    dl = dm.test_dataloader(batch_size)\n",
    "    x, y = next(iter(dl))\n",
    "    cf = random.normal(\n",
    "        random.PRNGKey(0), x.shape\n",
    "    )\n",
    "    cf = dm.apply_constraints(cf)\n",
    "    assert jnp.allclose(jnp.sum(cf[:, cat_idx:]), len(cf) * n_cat_feat)\n",
    "\n",
    "    cf = dm.apply_constraints(cf, hard=True)\n",
    "    assert jnp.count_nonzero(cf == 1) == len(cf) * n_cat_feat\n",
    "\n",
    "    cf = dm.project(x, cf)\n",
    "    assert x.shape == cf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_configs = {\n",
    "    \"data_dir\": \"assets/data/s_adult.csv\",\n",
    "    \"data_name\": \"adult\",\n",
    "    'sample_frac': 0.1,\n",
    "    \"continous_cols\": [\"age\", \"hours_per_week\"],\n",
    "    \"discret_cols\": [\n",
    "        \"workclass\", \"education\", \"marital_status\",\n",
    "        \"occupation\", \"race\", \"gender\"\n",
    "    ],\n",
    "}\n",
    "dm = TabularDataModule(data_configs)\n",
    "check_datamodule(dm, data_configs)\n",
    "\n",
    "# immutable\n",
    "_data_configs = deepcopy(data_configs)\n",
    "_data_configs[\"imutable_cols\"] = [\"race\",\"gender\"]\n",
    "dm = TabularDataModule(data_configs)\n",
    "check_datamodule(dm, data_configs)\n",
    "\n",
    "# no cont\n",
    "_data_configs = deepcopy(data_configs)\n",
    "_data_configs['continous_cols'] = []\n",
    "dm = TabularDataModule(data_configs)\n",
    "check_datamodule(dm, data_configs)\n",
    "\n",
    "# no cat\n",
    "_data_configs = deepcopy(data_configs)\n",
    "_data_configs['discret_cols'] = []\n",
    "dm = TabularDataModule(data_configs)\n",
    "check_datamodule(dm, data_configs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "High-level interfaces for loading default data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "DEFAULT_DATA_CONFIGS = {\n",
    "    'adult': {\n",
    "        'data' :'assets/data/s_adult.csv',\n",
    "        'conf' :'assets/configs/data_configs/adult.json',\n",
    "    },\n",
    "    'heloc': {\n",
    "        'data': 'assets/data/s_home.csv',\n",
    "        'conf': 'assets/configs/data_configs/home.json'\n",
    "    },\n",
    "    'oulad': {\n",
    "        'data': 'assets/data/s_student.csv',\n",
    "        'conf': 'assets/configs/data_configs/student.json'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _validate_dataname(data_name: str):\n",
    "    if data_name not in DEFAULT_DATA_CONFIGS.keys():\n",
    "        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA_CONFIGS.keys()}, '\n",
    "            f'but got data_name={data_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_data(\n",
    "    data_name: str, return_config: bool = False\n",
    ") -> TabularDataModule:\n",
    "    _validate_dataname(data_name)\n",
    "\n",
    "    # get data/config urls\n",
    "    _data_path = DEFAULT_DATA_CONFIGS[data_name]['data']\n",
    "    _conf_path = DEFAULT_DATA_CONFIGS[data_name]['conf']\n",
    "    \n",
    "    data_url = f\"https://github.com/BirkhoffG/cfnet/raw/master/{_data_path}\"\n",
    "    conf_url = f\"https://github.com/BirkhoffG/cfnet/raw/master/{_conf_path}\"\n",
    "\n",
    "    # create new dir\n",
    "    data_dir = Path(os.getcwd()) / \"cf_data\"\n",
    "    if not data_dir.exists():\n",
    "        os.makedirs(data_dir)\n",
    "    data_path = data_dir / f'{data_name}.csv'\n",
    "    conf_path = data_dir / f'{data_name}.json'\n",
    "\n",
    "    # download data/configs\n",
    "    if not data_path.is_file():\n",
    "        urlretrieve(data_url, data_path)    \n",
    "    if not conf_path.is_file():\n",
    "        urlretrieve(conf_url, conf_path)\n",
    "\n",
    "    # read config\n",
    "    config = load_json(conf_path)['data_configs']\n",
    "    config['data_dir'] = str(data_path)\n",
    "\n",
    "    data_module = TabularDataModule(config)\n",
    "\n",
    "    if return_config:\n",
    "        return data_module, config\n",
    "    else:\n",
    "        return data_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for data_name in DEFAULT_DATA_CONFIGS.keys():\n",
    "    dm, config = load_data(data_name, return_config=True)\n",
    "    check_datamodule(dm, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
