{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> Support various dataloader for loading batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.utils import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import torch.utils.data as torch_data\n",
    "except ModuleNotFoundError:\n",
    "    torch_data = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset(ABC):\n",
    "    \"\"\"A pytorch-like abstract Dataset class.\"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ArrayDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        *arrays: jnp.DeviceArray # Numpy array with same first dimension\n",
    "    ):\n",
    "        assert all(arrays[0].shape[0] == arr.shape[0] for arr in arrays), \\\n",
    "            \"All arrays must have the same dimension.\"\n",
    "        self.arrays = arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arrays[0].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(arr[index] for arr in self.arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "X = jax.random.normal(key, shape=(10, 10))\n",
    "y = jax.random.normal(key, shape=(10, ))\n",
    "ds = ArrayDataset(X, y)\n",
    "\n",
    "assert len(ds) == 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing `Dataset` using `ds[idx]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = ds[1]\n",
    "assert jnp.array_equal(x1, X[1])\n",
    "assert jnp.array_equal(y1, y[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataLoader(ABC):\n",
    "    \"\"\"Dataloader Interface\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset,\n",
    "        backend: str,\n",
    "        batch_size: int = 1,    # Batch size\n",
    "        shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # Drop last batches or not\n",
    "        **kwargs # Aux arguments\n",
    "    ):\n",
    "        pass \n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __next__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_dataloader(dataloader_cls, samples=1000, batch_size=12):\n",
    "    feats = jnp.arange(samples).repeat(10).reshape(samples, 10)\n",
    "    labels = jnp.arange(samples).reshape(samples, 1)\n",
    "    ds = ArrayDataset(feats, labels)\n",
    "    # N % batchsize != 0\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert jnp.array_equal(_X, feats)\n",
    "        assert jnp.array_equal(_Y, labels)\n",
    "\n",
    "    dl = dataloader_cls(ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        last_idx = len(X_list) * batch_size\n",
    "        assert jnp.array_equal(_X, feats[: last_idx])\n",
    "        assert jnp.array_equal(_Y, labels[: last_idx])\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert jnp.sum(_X) == jnp.sum(feats), \\\n",
    "            f\"jnp.sum(_X)={jnp.sum(_X)}, jnp.sum(feats)={jnp.sum(feats)}\"\n",
    "\n",
    "\n",
    "    dl_shuffle = dataloader_cls(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for _ in range(2):\n",
    "        X_list, Y_list = [], []\n",
    "        for x, y in dl_shuffle:\n",
    "            assert jnp.array_equal(x[:, :1], y)\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "        _X, _Y = map(jnp.concatenate, (X_list, Y_list))\n",
    "        assert not jnp.array_equal(_X, feats)\n",
    "        assert not jnp.array_equal(_Y, labels)\n",
    "        assert len(_X) == len(X_list) * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class JaxDataloader(BaseDataLoader):\n",
    "    \"\"\"Dataloder in vanilla Jax\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        backend: str = 'jax', # Position argument\n",
    "        batch_size: int = 1,  # Batch size\n",
    "        shuffle: bool = False,  # If true, dataloader shuffles before sampling each batch\n",
    "        drop_last: bool = False, # Drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.keys = hk.PRNGSequence(get_config().global_seed)\n",
    "        self.data_len = len(dataset)  # Length of the dataset\n",
    "        self.indices = jnp.arange(self.data_len) # available indices in the dataset\n",
    "        self.pose = 0  # record the current position in the dataset\n",
    "        self._shuffle()\n",
    "\n",
    "    def _shuffle(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = jax.random.permutation(next(self.keys), self.indices)\n",
    "        \n",
    "    def _stop_iteration(self):\n",
    "        self.pose = 0\n",
    "        self._shuffle()\n",
    "        raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            batches = len(self.dataset) // self.batch_size  # get the floor of division\n",
    "        else:\n",
    "            batches = -(len(self.dataset) // -self.batch_size)  # get the ceil of division\n",
    "        return batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pose + self.batch_size <= self.data_len:\n",
    "            batch_indices = self.indices[self.pose: self.pose + self.batch_size]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        elif self.pose < self.data_len and not self.drop_last:\n",
    "            batch_indices = self.indices[self.pose:]\n",
    "            batch_data = self.dataset[batch_indices, ...]\n",
    "            self.pose += self.batch_size\n",
    "            return batch_data\n",
    "        else:\n",
    "            self._stop_iteration()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(JaxDataloader, samples=100, batch_size=12)\n",
    "test_dataloader(JaxDataloader, samples=100, batch_size=10)\n",
    "test_dataloader(JaxDataloader, samples=101, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# copy from https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch\n",
    "def _numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [_numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def _convert_dataset_pytorch(dataset: Dataset):\n",
    "    class _TorchDataset(torch_data.Dataset):\n",
    "        def __init__(self, dataset: Dataset): self.dataset = dataset\n",
    "        def __len__(self): return len(self.dataset)\n",
    "        def __getitem__(self, idx): return self.dataset[idx]\n",
    "    \n",
    "    return _TorchDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TorchDataloader(BaseDataLoader):\n",
    "    \"\"\"Use `Pytorch` to load batches. It requires [pytorch](https://pytorch.org/get-started/) to be installed.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Dataset,\n",
    "        backend: str = 'pytorch', # positional argument\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        num_workers: int = 0, # number of workers\n",
    "        drop_last: bool = False, # drop last batch or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        if torch_data is None:\n",
    "            raise ModuleNotFoundError(\"`pytorch` library needs to be installed. Try `pip install torch`.\"\n",
    "            \"Please refer to pytorch documentation for details: https://pytorch.org/get-started/.\")\n",
    "        \n",
    "        dataset = _convert_dataset_pytorch(dataset)\n",
    "        self.dataloader = torch_data.DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            num_workers=num_workers, \n",
    "            drop_last=drop_last,\n",
    "            collate_fn=_numpy_collate,\n",
    "            **kwargs\n",
    "        ) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_dataloader(TorchDataloader, samples=100, batch_size=12)\n",
    "# test_dataloader(TorchDataloader, samples=1000, batch_size=10)\n",
    "# test_dataloader(TorchDataloader, samples=1001, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(frozen=True)\n",
    "class DataloaderBackends:\n",
    "    jax: BaseDataLoader = JaxDataloader\n",
    "    pytorch: BaseDataLoader = TorchDataloader\n",
    "    tensorflow: BaseDataLoader = None\n",
    "    merlin: BaseDataLoader = None\n",
    "\n",
    "    __all__ = dict(\n",
    "        jax=jax, pytorch=pytorch, tensorflow=tensorflow, merlin=merlin\n",
    "    )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.__all__[key]\n",
    "\n",
    "    @classmethod\n",
    "    def supported(cls) -> List[str]:\n",
    "        return [\n",
    "            backend for backend, dl_cls in cls.__all__.items() if dl_cls is not None\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _dispatch_dataloader(\n",
    "    backend: str # dataloader backend\n",
    ") -> BaseDataLoader:\n",
    "    \"\"\"Return Dataloader class based on given `backend`\"\"\"\n",
    "\n",
    "    backends = DataloaderBackends()\n",
    "    if not backend in DataloaderBackends.supported():\n",
    "        raise ValueError(f\"backend=`{backend}` is either an invalid backend or not supported yet. \"\n",
    "            f\"Should be one of {backends.supported}.\")\n",
    "    \n",
    "    dl_cls = backends[backend]\n",
    "    return dl_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/relax/tree/master/blob/master/relax/data/loader.py#L176){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/relax/tree/master/blob/master/relax/data/loader.py#L176){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### _dispatch_dataloader\n",
       "\n",
       ">      _dispatch_dataloader (backend:str)\n",
       "\n",
       "Return Dataloader class based on given `backend`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| backend | str | dataloader backend |\n",
       "| **Returns** | **BaseDataLoader** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(_dispatch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _dispatch_dataloader('jax') == JaxDataloader\n",
    "assert _dispatch_dataloader('pytorch') == TorchDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader(BaseDataLoader):\n",
    "    \"\"\"Main Dataloader class to load Numpy data batches\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        backend: str, # Dataloader backend; Currently supports `['jax', 'pytorch']`\n",
    "        batch_size: int = 1,  # batch size\n",
    "        shuffle: bool = False,  # if true, dataloader shuffles before sampling each batch\n",
    "        num_workers: int = 0, # number of workers\n",
    "        drop_last: bool = False, # drop last batches or not\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.__class__ = _dispatch_dataloader(backend)\n",
    "        self.__init__(\n",
    "            dataset=dataset, \n",
    "            backend=backend, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=shuffle, \n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Minimum Example of using Dataloader\n",
    "\n",
    "We showcase how to use `Dataloader` for training a simple regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10000, n_features=20)\n",
    "dataset = ArrayDataset(X, y.reshape(-1, 1))\n",
    "keys = hk.PRNGSequence(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `loss`, `step`, `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w, x, y):\n",
    "    return jnp.mean(vmap(optax.l2_loss)(x @ w.T, y))\n",
    "\n",
    "def step(w, x, y):\n",
    "    lr = 0.1\n",
    "    grad = jax.grad(loss)(w, x, y)\n",
    "    w -= lr * grad\n",
    "    return w\n",
    "\n",
    "def train(dataloader: DataLoader, key: random.PRNGKey):\n",
    "    w = jax.random.normal(key, shape=(1, 20))\n",
    "    n_epochs = 10\n",
    "    for _ in range(n_epochs):\n",
    "        for x, y in dataloader:\n",
    "            w = step(w, x, y)\n",
    "    return w\n",
    "\n",
    "def eval(dataloader: DataLoader, w):\n",
    "    err = []\n",
    "    for x, y in dataloader:\n",
    "        err.append(loss(w, x, y))\n",
    "    return np.mean(err)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderJax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'jax', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "dataloader = DataLoader(dataset, 'jax', batch_size=200, shuffle=True)\n",
    "w = train(dataloader, next(keys))\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this linear regression model via `DataLoaderPytorch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, 'pytorch', batch_size=128, shuffle=True)\n",
    "w = train(dataloader, next(keys)).block_until_ready()\n",
    "assert np.allclose(eval(dataloader, w), 0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
