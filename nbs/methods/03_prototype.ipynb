{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proto CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule\n",
    "from relax.data import TabularDataModule\n",
    "from relax.module import BaseTrainingModule, MLP\n",
    "from relax.trainer import train_model, TrainingConfigs\n",
    "from relax.utils import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class AEConfigs(BaseParser):\n",
    "    enc_sizes: List[int]\n",
    "    dec_sizes: List[int]\n",
    "    dropout_rate: float = 0.3\n",
    "    lr: float = 0.001\n",
    "\n",
    "class AE(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_config: Dict[str, Any],\n",
    "        name: Optional[str] = None\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, AEConfigs) #PredictiveModelConfigs(**m_config)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        is_training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        z = MLP(sizes=self.configs.enc_sizes, dropout_rate=self.configs.dropout_rate, name='Encoder')(x, is_training)\n",
    "        x = MLP(sizes=self.configs.enc_sizes, dropout_rate=self.configs.dropout_rate, name='Decoder')(z, is_training)\n",
    "        x = hk.Linear(input_shape, name='Decoder')(x)\n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class AETrainingModule(BaseTrainingModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_configs: Dict[str, Any]\n",
    "    ):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.net = make_model(m_configs, AE)\n",
    "        self.configs = validate_configs(m_configs, AEConfigs)\n",
    "        self.opt = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    def init_net_opt(self, data_module, key):\n",
    "        X, _ = data_module.train_dataset[:100]\n",
    "        params, opt_state = init_net_opt(\n",
    "            self.net, self.opt, X=X, key=key\n",
    "        )\n",
    "        return params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self', 'is_training'])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        return self.net.apply(params, rng_key, x, is_training = is_training)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self', ])\n",
    "    def encode(self, params, rng_key, x):\n",
    "        _, z = self.forward(params, rng_key, x, is_training=False)\n",
    "        return z\n",
    "\n",
    "    @partial(jax.jit, static_argnames=['self', 'is_training'])\n",
    "    def loss_fn(self, params, rng_key, batch, is_training=True):\n",
    "        x, y = batch\n",
    "        x_hat, z = self.forward(params, rng_key, x, is_training)\n",
    "        return jnp.mean(vmap(optax.l2_loss)(x, x_hat))\n",
    "\n",
    "    @partial(jit, static_argnames=['self'])\n",
    "    def _training_step(self, params, opt_state, rng_key, batch):\n",
    "        loss, grads = jax.value_and_grad(self.loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt)\n",
    "        return loss, upt_params, opt_state\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        loss, upt_params, opt_state = self._training_step(params, opt_state, rng_key, batch)\n",
    "\n",
    "        # loss = self.loss_fn(params, rng_key, batch)\n",
    "        self.log_dict({\n",
    "            'train/train_loss_1': loss.item()\n",
    "        })\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        loss = self.loss_fn(params, rng_key, batch, is_training=False)\n",
    "        logs = {\n",
    "            'val/val_loss': loss.item(),\n",
    "        }\n",
    "        self.log_dict(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@auto_reshaping('x')\n",
    "def _proto_cf(\n",
    "    x: jax.Array, # `x` shape: (k,), where `k` is the number of features\n",
    "    pred_fn: Callable[[jax.Array], jax.Array], # y = pred_fn(x)\n",
    "    n_steps: int,\n",
    "    lr: float, # learning rate for each `cf` optimization step\n",
    "    lambda_: float, #  loss = validity_loss + lambda_params * cost\n",
    "    ae: AETrainingModule,\n",
    "    ae_params: hk.Params,\n",
    "    sampled_data_pos: jax.Array,\n",
    "    sampled_data_neg: jax.Array,\n",
    "    sampled_label: jax.Array,\n",
    "    apply_constraints_fn: Callable\n",
    ") -> jax.Array: # return `cf` shape: (k,)\n",
    "    @jit\n",
    "    def proto(data):\n",
    "        return ae.encode(ae_params, jax.random.PRNGKey(0), data)\n",
    "\n",
    "    @jit\n",
    "    def loss_fn_1(cf_y: jax.Array, y_prime: jax.Array):\n",
    "        return jnp.mean(binary_cross_entropy(preds=cf_y, labels=y_prime))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn_2(x: jax.Array, cf: jax.Array):\n",
    "        return jnp.mean(optax.l2_loss(cf, x)) + 0.1 * jnp.mean(jnp.mean(jnp.abs(x - cf)))\n",
    "\n",
    "    @jit\n",
    "    def loss_fn_3(cf, data):\n",
    "        error = proto(cf) - proto(data)\n",
    "        return jnp.mean(0.5 * (error) ** 2)\n",
    "\n",
    "    @partial(jit, static_argnames=['pred_fn'])\n",
    "    def loss_fn(\n",
    "        cf: jax.Array, # `cf` shape: (k, 1)\n",
    "        x: jax.Array,  # `x` shape: (k, 1)\n",
    "        pred_fn: Callable[[jax.Array], jax.Array]\n",
    "    ):\n",
    "        y_pred = pred_fn(x)\n",
    "        y_prime = 1. - y_pred\n",
    "        cf_y = pred_fn(cf)\n",
    "\n",
    "        y_prime_round = jnp.mean(jnp.round(y_prime))\n",
    "\n",
    "        return loss_fn_1(cf_y, y_prime) + loss_fn_2(x, cf) \\\n",
    "            + loss_fn_3(cf, sampled_data_pos) * y_prime_round + loss_fn_3(cf, sampled_data_neg) * (1 - y_prime_round)\n",
    "\n",
    "    @loop_tqdm(n_steps)\n",
    "    def gen_cf_step(\n",
    "        i, cf_opt_state: Tuple[Array, optax.OptState]\n",
    "    ) -> Tuple[Array, optax.OptState]:\n",
    "        cf, opt_state = cf_opt_state\n",
    "        cf_grads = jax.grad(loss_fn)(cf, x, pred_fn)\n",
    "        cf, opt_state = grad_update(cf_grads, cf, opt_state, opt)\n",
    "        cf = apply_constraints_fn(x, cf, hard=False)\n",
    "        # cf = jnp.clip(cf, 0., 1.)\n",
    "        return cf, opt_state\n",
    "\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    opt = optax.rmsprop(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    \n",
    "    cf, opt_state = lax.fori_loop(0, n_steps, gen_cf_step, (cf, opt_state))\n",
    "    # for _ in tqdm(range(n_steps)):\n",
    "    #     cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "\n",
    "    cf = apply_constraints_fn(x, cf, hard=True)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ProtoCFConfig(BaseParser):\n",
    "    \n",
    "    n_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    lambda_: float = 0.01 # loss = validity_loss + lambda_params * cost\n",
    "    ae_configs: Dict[str, Any] = {\n",
    "        \"enc_sizes\": [50, 10],\n",
    "        \"dec_sizes\": [10, 50],\n",
    "        \"dropout_rate\": 0.3,\n",
    "        'lr': 0.03,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProtoCF(BaseCFModule, BaseParametricCFModule):\n",
    "    name = \"ProtoCF\"\n",
    "    _ae_params: hk.Params = None\n",
    "    _ae_module: AETrainingModule\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        configs: Dict | ProtoCFConfig = None\n",
    "    ):\n",
    "        if configs is None:\n",
    "            configs = ProtoCFConfig()\n",
    "        self.configs = validate_configs(configs, ProtoCFConfig)\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data_module: TabularDataModule, # data module\n",
    "        t_configs: TrainingConfigs | dict = None, # training configs\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        _default_t_configs = dict(n_epochs=10, batch_size=128)\n",
    "        if t_configs is None: \n",
    "            t_configs = _default_t_configs\n",
    "        t_configs = validate_configs(t_configs, TrainingConfigs)\n",
    "        # train autoencoder\n",
    "        self._ae_module = AETrainingModule(self.configs.ae_configs)\n",
    "        self._ae_params, _ = train_model(self._ae_module, data_module, t_configs)\n",
    "\n",
    "        sampled_data, sampled_label = next(iter(data_module.train_dataloader(t_configs.batch_size)))\n",
    "        self.sampled_data, self.sampled_label = map(jnp.array, (sampled_data, sampled_label))\n",
    "\n",
    "        self.sampled_pos = self.sampled_data[(self.sampled_label == 1.).reshape(-1), :]\n",
    "        self.sampled_neg = self.sampled_data[(self.sampled_label == 0.).reshape(-1), :]\n",
    "\n",
    "    def _is_module_trained(self) -> bool: \n",
    "        return not (self._ae_params is None)\n",
    "    \n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: jnp.ndarray, # `x` shape: (k,), where `k` is the number of features\n",
    "        pred_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> jax.Array:\n",
    "        return _proto_cf(\n",
    "            x= x, # `x` shape: (k,), where `k` is the number of features\n",
    "            pred_fn=pred_fn, # y = pred_fn(x)\n",
    "            n_steps=self.configs.n_steps,\n",
    "            lr=self.configs.lr, # learning rate for each `cf` optimization step\n",
    "            lambda_=self.configs.lambda_, #  loss = validity_loss + lambda_params * cost\n",
    "            ae=self._ae_module,\n",
    "            ae_params=self._ae_params,\n",
    "            sampled_data_pos=self.sampled_pos,\n",
    "            sampled_data_neg=self.sampled_neg,\n",
    "            sampled_label=self.sampled_label,\n",
    "            apply_constraints_fn=self.data_module.apply_constraints            \n",
    "        )\n",
    "\n",
    "    def generate_cfs(\n",
    "        self,\n",
    "        X: jax.Array, # `x` shape: (b, k), where `b` is batch size, `k` is the number of features\n",
    "        pred_fn: Callable[[jax.Array], jax.Array],\n",
    "        is_parallel: bool = False\n",
    "    ) -> jax.Array:\n",
    "        def _generate_cf(x: jax.Array) -> jnp.ndarray:\n",
    "            return self.generate_cf(x, pred_fn)\n",
    "        return jax.vmap(_generate_cf)(X) if not is_parallel else jax.pmap(_generate_cf)(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data import load_data\n",
    "from relax.module import PredictiveTrainingModule, PredictiveTrainingModuleConfigs, load_pred_model\n",
    "from relax.evaluate import generate_cf_explanations, benchmark_cfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dm = load_data('adult', data_configs=dict(sample_frac=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "\n",
    "# load model\n",
    "params, training_module = load_pred_model('adult')\n",
    "\n",
    "# predict function\n",
    "pred_fn = lambda x, params, key: training_module.forward(\n",
    "    params, key, x, is_training=False\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `ProtoCF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocf = ProtoCF()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtoCF contains parametric models. Starts training before generating explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/relax/_ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  warnings.warn(\n",
      "Epoch 4: 100%|██████████| 20/20 [00:00<00:00, 536.57batch/s, train/train_loss_1=0.0574]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b33e6948bd4cc0b9de1f0eeaeb8da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| output: false\n",
    "cf_exp = generate_cf_explanations(\n",
    "    protocf, dm, pred_fn=pred_fn, \n",
    "    t_configs=dict(\n",
    "        n_epochs=5, batch_size=128\n",
    "    ), \n",
    "    pred_fn_args=dict(\n",
    "        params=params, key=random.PRNGKey(0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>ProtoCF</th>\n",
       "      <td>0.8241</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>6.427959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  acc  validity  proximity\n",
       "adult ProtoCF  0.8241  0.812308   6.427959"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_cfs([cf_exp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
