{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAECF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.vaecf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule\n",
    "from relax.utils import *\n",
    "from relax.module import MLP, BaseTrainingModule\n",
    "from relax.data import *\n",
    "from relax.trainer import train_model, TrainingConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def hindge_embedding_loss(\n",
    "    inputs: Array, targets: Array, margin: float = 1.0, reduction: str = \"mean\"\n",
    "):\n",
    "    \"\"\"Hinge embedding loss. (Reduce mean over batch)\"\"\"\n",
    "    assert targets.shape == (1,)\n",
    "    # assert margin == 1. or margin == -1.\n",
    "    loss = jnp.where(\n",
    "        targets == 1,\n",
    "        inputs,\n",
    "        jax.nn.relu(margin - inputs)\n",
    "    )\n",
    "    if reduction is None:\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return jnp.mean(loss)\n",
    "    elif reduction == \"sum\":\n",
    "        return jnp.sum(loss)\n",
    "    else:\n",
    "        raise ValueError(f\"reduction must be one of [None, 'mean', 'sum'], but got {reduction}\")\n",
    "    # loss = jnp.mean(loss)\n",
    "    # return loss   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "x = np.array([0.3, -0.1, 0.5, -0.5, .99, -.99])\n",
    "y = np.array([-1])\n",
    "margin = 0.165\n",
    "assert jnp.isclose(\n",
    "    F.hinge_embedding_loss(torch.tensor(x), torch.tensor(y), margin).cpu().numpy(),\n",
    "    hindge_embedding_loss(x, y, margin)\n",
    ")\n",
    "assert jnp.allclose(\n",
    "    F.hinge_embedding_loss(torch.tensor(x), torch.tensor(y), margin, reduction='none').cpu().numpy(),\n",
    "    hindge_embedding_loss(x, y, margin, reduction=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = jrand.uniform(jrand.PRNGKey(0), (6, ))\n",
    "y = jnp.array([1, 1, 0, 0, 0, 0])\n",
    "target = jnp.array([-1])\n",
    "\n",
    "tempt_1, tempt_0 = pred_prob[y == 1], pred_prob[y == 0]\n",
    "validity_loss_1 = hindge_embedding_loss(tempt_1 - (1. - tempt_1), target, 0.165) + \\\n",
    "    hindge_embedding_loss(1. - 2 * tempt_0, target, 0.165)\n",
    "\n",
    "\n",
    "# tempt_1 = hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165)\n",
    "# tempt_0 = hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165)\n",
    "# validity_loss = jnp.where(\n",
    "#     y == 1, tempt_1, tempt_0\n",
    "# )\n",
    "\n",
    "tempt_1 = jnp.where(\n",
    "    y == 1,\n",
    "    hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165, reduction=None),\n",
    "    0\n",
    ").sum() / y.sum()\n",
    "tempt_0 = jnp.where(\n",
    "    y == 0,\n",
    "    hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165, reduction=None),\n",
    "    0\n",
    ").sum() / (y.shape[0] - y.sum())\n",
    "# validity_loss = jnp.where(\n",
    "#     y == 1,\n",
    "#     hindge_embedding_loss(pred_prob - (1. - pred_prob), target, 0.165, reduction=None),\n",
    "#     hindge_embedding_loss(1. - 2 * pred_prob, target, 0.165, reduction=None)\n",
    "# )\n",
    "# validity_loss_2 = jnp.sum(validity_loss)\n",
    "validity_loss_2 = tempt_1 + tempt_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.32720906, dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindge_embedding_loss(tempt_1 - (1. - tempt_1), target, 0.165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.99879485, dtype=float32),\n",
       " DeviceArray(0.99879485, dtype=float32))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validity_loss_1, validity_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, sizes: List[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = MLP(\n",
    "            sizes[:-1], dropout_rate=dropout, name=\"encoder_mean\")\n",
    "        self.encoded_size = sizes[-1]\n",
    "    \n",
    "    def __call__(self, x: Array, is_training: bool):\n",
    "        mu = self.encoder(x, is_training)\n",
    "        mu = hk.Linear(self.encoded_size, name='mu')(mu)\n",
    "        logvar = self.encoder(x, is_training)\n",
    "        logvar = hk.Linear(self.encoded_size, name='logvar')(logvar) + 0.5\n",
    "        logvar = jax.nn.sigmoid(logvar) + 0.5\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sizes: List[int], \n",
    "        input_size: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder = MLP(\n",
    "            sizes, dropout_rate=dropout, name=\"Decoder\")\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def __call__(self, z: Array, is_training: bool):\n",
    "        mu_dec = self.decoder(z, is_training=is_training)\n",
    "        mu_dec = hk.Linear(self.input_size, name='mu_x')(mu_dec)\n",
    "        mu_dec = jax.nn.sigmoid(mu_dec)\n",
    "        return mu_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class VAECFModuleConfigs(BaseParser):\n",
    "    \"\"\"Configurator of `VAECFModule`.\"\"\"\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [20, 16, 14, 12, 5],\n",
    "        description=\"Sequence of Encoder layer sizes.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [12, 14, 16, 20],\n",
    "        description=\"Sequence of Decoder layer sizes.\"\n",
    "    )\n",
    "    dropout_rate: float = Field(\n",
    "        0.1, description=\"Dropout rate.\"\n",
    "    )\n",
    "    lr: float = Field(\n",
    "        1e-3, description=\"Learning rate.\"\n",
    "    )\n",
    "    mu_samples: int = Field(\n",
    "        50, description=\"Number of samples for mu.\"\n",
    "    )\n",
    "    validity_reg: float = Field(\n",
    "        42.0, description=\"Regularization for validity.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECFModule(BaseTrainingModule):\n",
    "    pred_fn: Callable\n",
    "\n",
    "    def __init__(self, m_configs: Dict = None):\n",
    "        if m_configs is None: m_configs = {}\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.m_config = validate_configs(m_configs, VAECFModuleConfigs)\n",
    "        self.opt = optax.adam(self.m_config.lr)\n",
    "\n",
    "    def init_net_opt(self, dm, key):\n",
    "        self._data_module = dm\n",
    "        X, y = dm.train_dataset[:128]\n",
    "        Z = jnp.ones((X.shape[0], self.m_config.enc_sizes[-1]))\n",
    "        inputs = jnp.concatenate([X, y.reshape(-1, 1)], axis=-1)\n",
    "        latent = jnp.concatenate([Z, y.reshape(-1, 1)], axis=-1)\n",
    "\n",
    "        self.encoder = make_hk_module(\n",
    "            Encoder, sizes=self.m_config.enc_sizes, \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "        self.decoder = make_hk_module(\n",
    "            Decoder, sizes=self.m_config.dec_sizes,\n",
    "            input_size=X.shape[-1], \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "\n",
    "        enc_params = self.encoder.init(\n",
    "            key, inputs, is_training=True)\n",
    "        dec_params = self.decoder.init(\n",
    "            key, latent, is_training=True)\n",
    "        opt_state = self.opt.init((enc_params, dec_params))\n",
    "        return (enc_params, dec_params), opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def encode(self, enc_params, rng_key, x, is_training=True):\n",
    "        mu_z, logvar_z = self.encoder.apply(\n",
    "            enc_params, rng_key, x, is_training=is_training)\n",
    "        return mu_z, logvar_z\n",
    "        \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def sample_latent_code(self, rng_key, mean, logvar):\n",
    "        eps = jax.random.normal(rng_key, logvar.shape)\n",
    "        return mean + eps * jnp.sqrt(logvar)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def decode(self, dec_params, rng_key, z, is_training=True):\n",
    "        mu_x = self.decoder.apply(\n",
    "            dec_params, rng_key, z, is_training=is_training)\n",
    "        return mu_x\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 6))\n",
    "    def sample_step(\n",
    "        self, rng_key, dec_params, em, ev, c, is_training=True\n",
    "    ):\n",
    "        z = self.sample_latent_code(rng_key, em, ev)\n",
    "        z = jnp.concatenate([z, c.reshape(-1, 1)], axis=-1)\n",
    "        mu_x = self.decode(dec_params, rng_key, z, is_training=is_training)\n",
    "        return mu_x\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4, 5))\n",
    "    def sample(\n",
    "        self, params, rng_key, inputs, mc_samples, is_training=True\n",
    "    ):\n",
    "        enc_params, dec_params = params\n",
    "        x, c = inputs[:, :-1], inputs[:, -1]\n",
    "        em, ev = self.encode(enc_params, rng_key, inputs)\n",
    "        keys = jax.random.split(rng_key, mc_samples)\n",
    "        \n",
    "        partial_sample_step = partial(\n",
    "            self.sample_step, dec_params=dec_params,\n",
    "            em=em, ev=ev, c=c\n",
    "        )\n",
    "        mu_x = jax.vmap(partial_sample_step)(keys)\n",
    "        # return SampleOut(em=em, ev=ev, mu_x=mu_x)\n",
    "        return (em, ev, mu_x)\n",
    "        \n",
    "    def compute_loss(self, params, rng_key, inputs, is_training=True):\n",
    "        def cf_loss(cf: Array, x: Array, y: Array):\n",
    "            assert cf.shape == x.shape, f\"cf.shape ({cf.shape}) != x.shape ({x.shape}))\"\n",
    "            # proximity loss\n",
    "            recon_err = jnp.sum(jnp.abs(cf - x), axis=1).mean()\n",
    "            # Sum to 1 over the categorical indexes of a feature\n",
    "            cat_error = self._data_module.apply_regularization(x, cf)\n",
    "            # validity loss\n",
    "            pred_prob = self.pred_fn(cf)\n",
    "            # This is same as the following:\n",
    "            # tempt_1, tempt_0 = pred_prob[y == 1], pred_prob[y == 0]\n",
    "            # validity_loss = hindge_embedding_loss(tempt_1 - (1. - tempt_1), -1, 0.165) + \\\n",
    "            #     hindge_embedding_loss(1. - 2 * tempt_0, -1, 0.165)\n",
    "            target = jnp.array([-1])\n",
    "            hindge_loss_1 = hindge_embedding_loss(\n",
    "                jax.nn.sigmoid(pred_prob) - jax.nn.sigmoid(1. - pred_prob), target, 0.165, reduction=None)\n",
    "            hindge_loss_0 = hindge_embedding_loss(\n",
    "                jax.nn.sigmoid(1. - pred_prob) - jax.nn.sigmoid(pred_prob), target, 0.165, reduction=None)\n",
    "            tempt_1 = jnp.where(y == 1, hindge_loss_1, 0).sum() / y.sum()\n",
    "            tempt_0 = jnp.where(y == 0, hindge_loss_0, 0).sum() / (y.shape[0] - y.sum())\n",
    "            validity_loss = tempt_1 + tempt_0\n",
    "\n",
    "            return recon_err + cat_error, validity_loss\n",
    "\n",
    "        em, ev, cfs = self.sample(\n",
    "            params, rng_key, inputs, self.m_config.mu_samples, \n",
    "            is_training=is_training\n",
    "        )\n",
    "        X, y = inputs[:, :-1], inputs[:, -1]\n",
    "        # kl divergence\n",
    "        kl = 0.5 * jnp.mean(em**2 + ev - jnp.log(ev) - 1, axis=1)\n",
    "        cf_loss_partial = partial(cf_loss, x=X, y=y)\n",
    "        recon_err, validity_loss = jax.vmap(cf_loss_partial)(cfs)\n",
    "        # assert recon_err.shape == (self.m_config.mu_samples,), recon_err.shape\n",
    "        # assert cat_error.shape == (self.m_config.mu_samples,), cat_error.shape\n",
    "        # assert validity_loss.shape == (self.m_config.mu_samples,), validity_loss.shape\n",
    "        recon_err = jnp.mean(recon_err)\n",
    "        validity_loss = jnp.mean(validity_loss)\n",
    "        return jnp.mean(kl + recon_err) + validity_loss\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _training_step(\n",
    "        self, \n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState, \n",
    "        rng_key: random.PRNGKey, \n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        x, _ = batch\n",
    "        y = self.pred_fn(x).round().reshape(-1, 1)\n",
    "        loss, grads = jax.value_and_grad(self.compute_loss)(\n",
    "            params, rng_key, jnp.concatenate([x, y], axis=-1))\n",
    "        update_params, opt_state = grad_update(\n",
    "            grads, params, opt_state, self.opt)\n",
    "        return update_params, opt_state, loss\n",
    "    \n",
    "    def training_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        params, opt_state, loss = self._training_step(params, opt_state, rng_key, batch)\n",
    "        self.log_dict({'train/loss': loss.item()})\n",
    "        return params, opt_state\n",
    "    \n",
    "    def validation_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAECFConfigs(VAECFModuleConfigs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAECF(BaseCFModule, BaseParametricCFModule):\n",
    "    params: Tuple[hk.Params, hk.Params] = None\n",
    "    module: VAECFModule\n",
    "    name: str = 'C-CHVAE'\n",
    "\n",
    "    def __init__(self, m_config: Dict | VAECFConfigs = None):\n",
    "        if m_config is None:\n",
    "            m_config = VAECFConfigs()\n",
    "        self.m_config = m_config\n",
    "        self.module = VAECFModule(m_config.dict())\n",
    "\n",
    "    def _is_module_trained(self) -> bool:\n",
    "        return not (self.params is None)\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        datamodule: TabularDataModule, # data module\n",
    "        t_configs: TrainingConfigs | dict = None # training configs\n",
    "    ):\n",
    "        _default_t_configs = dict(\n",
    "            n_epochs=10, batch_size=128\n",
    "        )\n",
    "        if t_configs is None: t_configs = _default_t_configs\n",
    "        params, _ = train_model(self.module, datamodule, t_configs)\n",
    "        self.params = params\n",
    "    \n",
    "    def generate_cfs(self, X: Array, pred_fn: Callable = None) -> jnp.ndarray:\n",
    "        # TODO\n",
    "        _cchvae_generate_fn_partial = partial(\n",
    "            _cchvae_generate,\n",
    "            pred_fn=pred_fn,\n",
    "            max_steps=self.m_config.max_steps,\n",
    "            n_search_samples=self.m_config.n_search_samples,\n",
    "            step_size=self.m_config.step_size,\n",
    "            cchvae_module=self.module,\n",
    "            cchvae_params=self.params,\n",
    "            apply_fn=self.data_module.apply_constraints,\n",
    "        )\n",
    "        rngs = lax.broadcast(random.PRNGKey(0), (X.shape[0], ))\n",
    "        return jax.vmap(_cchvae_generate_fn_partial)(X, rngs)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.trainer import train_model\n",
    "from relax.data import load_data\n",
    "from relax.module import PredictiveTrainingModule\n",
    "from relax.evaluate import _AuxPredFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = load_data('adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/ReLax/relax/_ckpt_manager.py:48: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  \"`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\"\n",
      "Epoch 4: 100%|██████████| 96/96 [00:00<00:00, 97.08batch/s, train/train_loss_1=0.0434]\n"
     ]
    }
   ],
   "source": [
    "m_config = dict(sizes=[50, 10, 50], lr=0.03)\n",
    "t_config = dict(n_epochs=5, batch_size=256)\n",
    "\n",
    "training_module = PredictiveTrainingModule(m_config)\n",
    "params, opt_state = train_model(\n",
    "    training_module, dm, t_config\n",
    ")\n",
    "# predict function\n",
    "# pred_fn = lambda x: training_module.forward(params, x, is_training=False)\n",
    "pred_fn = lambda x, params, rng_key: training_module.forward(\n",
    "    params, rng_key, x, is_training=False\n",
    ")\n",
    "pred_fn = _AuxPredFn(pred_fn, {'params': params, 'rng_key': random.PRNGKey(0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# vae = VAECFModule()\n",
    "# setattr(vae, 'pred_fn', pred_fn)\n",
    "# train_model(\n",
    "#     vae,\n",
    "#     dm,\n",
    "#     dict(n_epochs=10, batch_size=128)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
