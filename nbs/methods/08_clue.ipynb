{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule\n",
    "from relax.utils import *\n",
    "from relax.module import MLP, BaseTrainingModule\n",
    "from relax.data import *\n",
    "from relax.trainer import train_model, TrainingConfigs\n",
    "from jax.scipy.stats.norm import logpdf as gaussian_logpdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, sizes: List[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert sizes[-1] % 2 == 0, f\"sizes[-1] must be even, but got {sizes[-1]}\"\n",
    "        self.encoder = MLP(\n",
    "            sizes, dropout_rate=dropout, name=\"encoder_mean\")\n",
    "    \n",
    "    def __call__(self, x: Array, is_training: bool):\n",
    "        params = self.encoder(x, is_training)\n",
    "        d = params.shape[-1] // 2\n",
    "        mu, sigma = params[:, :d], params[:, d:]\n",
    "        sigma = jax.nn.softplus(sigma)\n",
    "        sigma = jnp.clip(sigma, 1e-3)\n",
    "        return mu, sigma\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sizes: List[int], \n",
    "        input_size: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder = MLP(\n",
    "            sizes, dropout_rate=dropout, name=\"Decoder\")\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def __call__(self, z: Array, is_training: bool):\n",
    "        mu_dec = self.decoder(z, is_training=is_training)\n",
    "        mu_dec = hk.Linear(self.input_size, name='mu_x')(mu_dec)\n",
    "        mu_dec = jax.nn.sigmoid(mu_dec)\n",
    "        return mu_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit\n",
    "def kl_divergence(p: Array, q: Array, eps: float = 2 ** -17) -> Array:\n",
    "    loss_pointwise = p * (jnp.log(p + eps) - jnp.log(q + eps))\n",
    "    return loss_pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEGaussCatConfigs(BaseParser):\n",
    "    lr: float = Field(0.001, description=\"Learning rate.\")\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [20, 16, 14, 12],\n",
    "        description=\"Sequence of Encoder layer sizes.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [12, 14, 16, 20],\n",
    "        description=\"Sequence of Decoder layer sizes.\"\n",
    "    )\n",
    "    dropout_rate: float = Field(\n",
    "        0.1, description=\"Dropout rate.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEGaussCat(BaseTrainingModule):\n",
    "    def __init__(self, m_configs: Dict = None):\n",
    "        if m_configs is None: m_configs = {}\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.m_config = validate_configs(m_configs, VAEGaussCatConfigs)\n",
    "        self.opt = optax.radam(self.m_config.lr)\n",
    "\n",
    "    def _update_categorical_info(self):\n",
    "        cat_arrays = self._data_module._cat_arrays\n",
    "        self._cat_info = {\n",
    "            'cat_idx': self._data_module.cat_idx,\n",
    "            # 'cat_arr': jnp.array([len(cat_arr) for cat_arr in cat_arrays]),\n",
    "            'cat_arr': [len(cat_arr) for cat_arr in cat_arrays],\n",
    "        }\n",
    "    \n",
    "    def init_net_opt(self, dm, key):\n",
    "        self._data_module = dm\n",
    "        self._update_categorical_info()\n",
    "        keys = jax.random.split(key, 3)\n",
    "        X, y = dm.train_dataset[:128]\n",
    "        Z = jnp.ones((X.shape[0], self.m_config.enc_sizes[-1] // 2))\n",
    "\n",
    "        self.encoder = make_hk_module(\n",
    "            Encoder, sizes=self.m_config.enc_sizes, \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "        self.decoder = make_hk_module(\n",
    "            Decoder, sizes=self.m_config.dec_sizes,\n",
    "            input_size=X.shape[-1], \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "\n",
    "        enc_params = self.encoder.init(\n",
    "            keys[0], X, is_training=True)\n",
    "        dec_params = self.decoder.init(\n",
    "            keys[1], Z, is_training=True)\n",
    "        opt_state = self.opt.init((enc_params, dec_params))\n",
    "\n",
    "        # set prior for training latents\n",
    "        self.prior = jrand.normal(\n",
    "            keys[2], (self.m_config.enc_sizes[-1],)\n",
    "        )\n",
    "        return (enc_params, dec_params), opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def encode(self, enc_params, rng_key, x, is_training=True):\n",
    "        mu_z, var_z = self.encoder.apply(\n",
    "            enc_params, rng_key, x, is_training=is_training)\n",
    "        return mu_z, var_z\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, ))\n",
    "    def sample_latent(self, rng_key, mean, var):\n",
    "        key, _ = jax.random.split(rng_key)\n",
    "        std = jnp.exp(0.5 * var)\n",
    "        eps = jax.random.normal(key, var.shape)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def decode(self, dec_params, rng_key, z, is_training=True,):\n",
    "        reconstruct_x = self.decoder.apply(\n",
    "            dec_params, rng_key, z, is_training=is_training)\n",
    "        return reconstruct_x        \n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 5))\n",
    "    def sample_step(\n",
    "        self, rng_key, dec_params, mean, var, is_training=True\n",
    "    ):\n",
    "        z = self.sample_latent(rng_key, mean, var)\n",
    "        mu_x = self.decode(dec_params, rng_key, z, is_training=is_training)\n",
    "        return mu_x\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4, 5))\n",
    "    def sample(\n",
    "        self, params, rng_key, x, mc_samples, is_training=True\n",
    "    ): # Shape: (mc_samples, batch_size, input_size)\n",
    "        enc_params, dec_params = params\n",
    "        mean, var = self.encode(enc_params, rng_key, x, is_training=is_training)\n",
    "        keys = jax.random.split(rng_key, mc_samples)\n",
    "        \n",
    "        partial_sample_step = partial(\n",
    "            self.sample_step, dec_params=dec_params,\n",
    "            mean=mean, var=var, is_training=is_training\n",
    "        )\n",
    "        reconstruct_x = jax.vmap(partial_sample_step)(keys)\n",
    "        return (mean, var, reconstruct_x)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, ))\n",
    "    def sample_prior(self, rng_key):\n",
    "        rng_key, key = jax.random.split(rng_key)\n",
    "        prior = jrand.normal(key, (self.m_config.enc_sizes[-1],))\n",
    "        return prior\n",
    "    \n",
    "    def compute_loss(self, params, rng_key, x, is_training=True):\n",
    "        # @partial(jax.jit, static_argnums=(2, 3))\n",
    "        def reconstruct_loss(x: Array, cf: Array, cat_idx: int, cat_arr: List[int]):\n",
    "            cont_loss = optax.l2_loss(x[:, :cat_idx], cf[:, :cat_idx])\n",
    "            cat_loss = []\n",
    "\n",
    "            def _cat_loss_f(start_end_idx):\n",
    "                start_idx, end_idx = start_end_idx\n",
    "                return optax.softmax_cross_entropy(\n",
    "                    cf[:, start_idx: end_idx], x[:, start_idx: end_idx]\n",
    "                ).reshape(-1, 1)\n",
    "            \n",
    "            # for start_end_idx in start_end_indices:\n",
    "            for i, cat in enumerate(cat_arr):\n",
    "                start_end_idx = (cat_idx + i * cat, cat_idx + (i + 1) * cat)\n",
    "                cat_loss.append(_cat_loss_f(start_end_idx))\n",
    "            cat_loss = jnp.concatenate(cat_loss, axis=-1)\n",
    "            \n",
    "            # cat_loss = jax.vmap(jit(_cat_loss_f))(start_indices, end_indices)\n",
    "            # cat_loss = jax.lax.scan(_cat_loss_f, 0., start_end_indices, len(start_end_indices))[1]\n",
    "            return jnp.concatenate([cont_loss, cat_loss], axis=-1).sum(-1)\n",
    "        \n",
    "        keys = jax.random.split(rng_key, 2)\n",
    "        mu_z, logvar_z, reconstruct_x = self.sample(\n",
    "            params, keys[0], x, mc_samples=1, is_training=is_training\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + logvar_z - jnp.power(mu_z, 2) - jnp.exp(logvar_z)).sum(-1)\n",
    "        \n",
    "        rec = reconstruct_loss(\n",
    "            x, reconstruct_x.reshape(x.shape), \n",
    "            cat_idx=self._cat_info['cat_idx'],\n",
    "            cat_arr=self._cat_info['cat_arr']\n",
    "        ).sum(-1)\n",
    "        batchwise_loss = (rec + kl_loss) / x.shape[0]\n",
    "        return batchwise_loss.mean()\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _training_step(\n",
    "        self, \n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState, \n",
    "        rng_key: random.PRNGKey, \n",
    "        batch: Tuple[Array, Array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        x, _ = batch\n",
    "        loss, grads = jax.value_and_grad(self.compute_loss)(\n",
    "            params, rng_key, x)\n",
    "        update_params, opt_state = grad_update(\n",
    "            grads, params, opt_state, self.opt)\n",
    "        return update_params, opt_state, loss\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        params, opt_state, loss = self._training_step(params, opt_state, rng_key, batch)\n",
    "        self.log_dict({'train/loss': loss.item()})\n",
    "        return params, opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def validation_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@auto_reshaping('x')\n",
    "def _clue_generate(\n",
    "    x: Array,\n",
    "    rng_key: jrand.PRNGKey,\n",
    "    pred_fn: Callable,\n",
    "    max_steps: int,\n",
    "    step_size: float,\n",
    "    vae_module: VAEGaussCat,\n",
    "    vae_params: Tuple[hk.Params, hk.Params],\n",
    "    uncertainty_weight: float,\n",
    "    aleatoric_weight: float,\n",
    "    prior_weight: float,\n",
    "    distance_weight: float,\n",
    "    validity_weight: float,\n",
    "    apply_fn: Callable\n",
    ") -> Array:\n",
    "    \n",
    "    @jit\n",
    "    def sample_latent_from_x(\n",
    "        x: Array, enc_params: hk.Params, rng_key: jrand.PRNGKey\n",
    "    ):\n",
    "        key_1, key_2 = jrand.split(rng_key)\n",
    "        mean, var = vae_module.encode(enc_params, key_1, x, is_training=False)\n",
    "        z = vae_module.sample_latent(key_2, mean, var)\n",
    "        return z\n",
    "    \n",
    "    @partial(jit, static_argnums=(2,))\n",
    "    def generate_from_z(\n",
    "        z: Array, \n",
    "        dec_params: hk.Params,\n",
    "        hard: bool = False\n",
    "    ):\n",
    "        cf = vae_module.decode(\n",
    "            dec_params, rng_key, z, is_training=False)\n",
    "        cf = apply_fn(x, cf, hard=hard)\n",
    "        return cf\n",
    "\n",
    "    @jit\n",
    "    def uncertainty_from_z(z: Array, dec_params: hk.Params):\n",
    "        cfs = generate_from_z(z, dec_params, hard=False)\n",
    "        prob = pred_fn(cfs)\n",
    "        total_uncertainty = -(prob * jnp.log(prob + 1e-10)).sum(-1)\n",
    "        return total_uncertainty, cfs, prob\n",
    "    \n",
    "    @jit\n",
    "    def compute_loss(z: Array, dec_params: hk.Params):\n",
    "        uncertainty, cfs, prob = uncertainty_from_z(z, dec_params)\n",
    "        loglik = gaussian_logpdf(z).sum(-1)\n",
    "        dist = jnp.abs(cfs - x).mean()\n",
    "        validity = binary_cross_entropy(preds=prob, labels=y_targets).mean()\n",
    "        loss = (\n",
    "            (uncertainty_weight + aleatoric_weight) * uncertainty \n",
    "            + prior_weight * loglik\n",
    "            + distance_weight * dist\n",
    "            + validity_weight * validity\n",
    "        )\n",
    "        return loss.mean()\n",
    "    \n",
    "    def step(i, z_opt_state):\n",
    "        z, opt_state = z_opt_state\n",
    "        z_grad = jax.grad(compute_loss)(z, dec_params)\n",
    "        z, opt_state = grad_update(z_grad, z, opt_state, opt)\n",
    "        return z, opt_state\n",
    "    \n",
    "    enc_params, dec_params = vae_params\n",
    "    key_1, _ = jax.random.split(rng_key)\n",
    "    z = sample_latent_from_x(x, enc_params, key_1)\n",
    "    opt = optax.adam(step_size)\n",
    "    opt_state = opt.init(z)\n",
    "    y_targets = 1 - pred_fn(x)\n",
    "\n",
    "    # Write a loop to optimize z using lax.fori_loop\n",
    "    z, opt_state = lax.fori_loop(0, max_steps, step, (z, opt_state))\n",
    "    cf = generate_from_z(z, dec_params, hard=True)\n",
    "    return cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLUEConfigs(BaseParser):\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [20, 16, 14, 12], description=\"Sequence of Encoder layer sizes.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [12, 14, 16, 20], description=\"Sequence of Decoder layer sizes.\"\n",
    "    )\n",
    "    encoded_size: int = Field(5, description=\"Encoded size\")\n",
    "    lr: float = Field(0.001, description=\"Learning rate\")\n",
    "    max_steps: int = Field(500, description=\"Max steps\")\n",
    "    step_size: float = Field(0.01, description=\"Step size\")\n",
    "    vae_n_epochs: int = Field(10, description=\"Number of epochs for VAE\")\n",
    "    vae_batch_size: int = Field(128, description=\"Batch size for VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLUE(BaseCFModule, BaseParametricCFModule):\n",
    "    params: Tuple[hk.Params, hk.Params] = None\n",
    "    module: VAEGaussCat\n",
    "    name: str = 'CLUE'\n",
    "\n",
    "    def __init__(self, m_config: Dict | CLUEConfigs = None):\n",
    "        if m_config is None: m_config = CLUEConfigs()\n",
    "        self.m_config = m_config\n",
    "        self.module = VAEGaussCat(m_config.dict())\n",
    "\n",
    "    def _is_module_trained(self) -> bool:\n",
    "        return not (self.params is None)\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        datamodule: TabularDataModule, # data module\n",
    "        t_configs: TrainingConfigs | dict = None, # training configs\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        _default_t_configs = dict(\n",
    "            n_epochs=10, batch_size=128\n",
    "        )\n",
    "        if t_configs is None: t_configs = _default_t_configs\n",
    "        params, _ = train_model(self.module, datamodule, t_configs)\n",
    "        self.params = params\n",
    "\n",
    "    def generate_cf(self, x, rng_key, pred_fn: Callable = None) -> Array:\n",
    "        return _clue_generate(\n",
    "            x, rng_key=rng_key, pred_fn=pred_fn,\n",
    "            max_steps=self.m_config.max_steps,\n",
    "            step_size=self.m_config.step_size,\n",
    "            vae_module=self.module,\n",
    "            vae_params=self.params,\n",
    "            uncertainty_weight=.0,\n",
    "            aleatoric_weight=0.0,\n",
    "            prior_weight=0.0,\n",
    "            distance_weight=.1,\n",
    "            validity_weight=1.0,\n",
    "            apply_fn=self.data_module.apply_constraints,\n",
    "        )\n",
    "    \n",
    "    def generate_cfs(self, X: Array, pred_fn: Callable = None) -> jnp.ndarray:\n",
    "        generate_cf_partial = partial(\n",
    "            self.generate_cf, pred_fn=pred_fn\n",
    "        )\n",
    "        rngs = lax.broadcast(random.PRNGKey(0), (X.shape[0], ))\n",
    "        return jax.vmap(generate_cf_partial)(X, rngs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.module import PredictiveTrainingModule\n",
    "from relax.evaluate import generate_cf_explanations, benchmark_cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = load_data('adult', data_configs=dict(sample_frac=0.1))\n",
    "dm = load_data('adult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "2023-05-02 21:15:55.497977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-02 21:15:55.498064: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-05-02 21:15:55.498070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/birk/code/ReLax/relax/_ckpt_manager.py:48: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  \"`monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\"\n",
      "Epoch 4: 100%|██████████| 96/96 [00:00<00:00, 238.47batch/s, train/train_loss_1=0.0475]\n"
     ]
    }
   ],
   "source": [
    "m_config = dict(sizes=[50, 10, 50], lr=0.03)\n",
    "t_config = dict(n_epochs=5, batch_size=256)\n",
    "\n",
    "training_module = PredictiveTrainingModule(m_config)\n",
    "params, opt_state = train_model(\n",
    "    training_module, dm, t_config\n",
    ")\n",
    "# predict function\n",
    "# pred_fn = lambda x: training_module.forward(params, x, is_training=False)\n",
    "\n",
    "pred_fn = training_module.pred_fn\n",
    "# pred_fn = lambda x, params, key: training_module.forward(\n",
    "#     params, key, x, is_training=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/mambaforge-pypy3/envs/nbdev2/lib/python3.7/site-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n",
      "Epoch 9: 100%|██████████| 191/191 [00:00<00:00, 372.22batch/s, train/loss=6.26]\n"
     ]
    }
   ],
   "source": [
    "clue = CLUE()\n",
    "clue.train(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clue_test = CLUE()\n",
    "clue_test.params = clue.params\n",
    "clue_test.module = clue.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_exp = generate_cf_explanations(\n",
    "    clue_test, dm, pred_fn, pred_fn_args=dict(\n",
    "        params=params, rng_key=random.PRNGKey(0)\n",
    "    ), t_configs=dict(\n",
    "        n_epochs=5, batch_size=256\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>validity</th>\n",
       "      <th>proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <th>CLUE</th>\n",
       "      <td>0.822626</td>\n",
       "      <td>0.847193</td>\n",
       "      <td>5.270984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 acc  validity  proximity\n",
       "adult CLUE  0.822626  0.847193   5.270984"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_cfs([cf_exp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
