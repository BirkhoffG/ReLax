{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import BaseCFModule, BaseParametricCFModule\n",
    "from relax.utils import *\n",
    "from relax.module import MLP, BaseTrainingModule\n",
    "from relax.data import *\n",
    "from relax.trainer import train_model, TrainingConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, sizes: List[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert sizes[-1] % 2 == 0, f\"sizes[-1] must be even, but got {sizes[-1]}\"\n",
    "        self.encoder = MLP(\n",
    "            sizes, dropout_rate=dropout, name=\"encoder_mean\")\n",
    "    \n",
    "    def __call__(self, x: Array, is_training: bool):\n",
    "        params = self.encoder(x, is_training)\n",
    "        d = params.shape[-1] // 2\n",
    "        mu, sigma = params[:, :d], params[:, d:]\n",
    "        sigma = jax.nn.softplus(sigma)\n",
    "        sigma = jnp.clip(sigma, 1e-3)\n",
    "        return mu, sigma\n",
    "\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sizes: List[int], \n",
    "        input_size: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder = MLP(\n",
    "            sizes, dropout_rate=dropout, name=\"Decoder\")\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def __call__(self, z: Array, is_training: bool):\n",
    "        mu_dec = self.decoder(z, is_training=is_training)\n",
    "        mu_dec = hk.Linear(self.input_size, name='mu_x')(mu_dec)\n",
    "        mu_dec = jax.nn.sigmoid(mu_dec)\n",
    "        return mu_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kl_divergence(p: Array, q: Array, eps: float = 2 ** -17) -> Array:\n",
    "    loss_pointwise = p * (jnp.log(p + eps) - jnp.log(q + eps))\n",
    "    return loss_pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEGaussCatConfigs(BaseParser):\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [20, 16, 14, 12],\n",
    "        description=\"Sequence of Encoder layer sizes.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [12, 14, 16, 20],\n",
    "        description=\"Sequence of Decoder layer sizes.\"\n",
    "    )\n",
    "    dropout_rate: float = Field(\n",
    "        0.1, description=\"Dropout rate.\"\n",
    "    )\n",
    "    lr: float = Field(\n",
    "        1e-3, description=\"Learning rate.\"\n",
    "    )\n",
    "    mu_samples: int = Field(\n",
    "        50, description=\"Number of samples for mu.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEGaussCat(BaseTrainingModule):\n",
    "    def __init__(self, m_configs: Dict = None):\n",
    "        if m_configs is None: m_configs = {}\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.m_config = validate_configs(m_configs, VAEGaussCatConfigs)\n",
    "        self.opt = optax.radam(self.m_config.lr)\n",
    "\n",
    "    def _update_categorical_info(self):\n",
    "        cat_arrays = self._data_module._cat_arrays\n",
    "        self._cat_info = {\n",
    "            'cat_idx': self._data_module.cat_idx,\n",
    "            'cat_arr': [len(cat_arr) for cat_arr in cat_arrays],\n",
    "        }\n",
    "    \n",
    "    def init_net_opt(self, dm, key):\n",
    "        self._data_module = dm\n",
    "        self._update_categorical_info()\n",
    "        keys = jax.random.split(key, 3)\n",
    "        X, y = dm.train_dataset[:128]\n",
    "        Z = jnp.ones((X.shape[0], self.m_config.enc_sizes[-1]))\n",
    "        inputs = jnp.concatenate([X, y.reshape(-1, 1)], axis=-1)\n",
    "        latent = jnp.concatenate([Z, y.reshape(-1, 1)], axis=-1)\n",
    "\n",
    "        self.encoder = make_hk_module(\n",
    "            Encoder, sizes=self.m_config.enc_sizes, \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "        self.decoder = make_hk_module(\n",
    "            Decoder, sizes=self.m_config.dec_sizes,\n",
    "            input_size=X.shape[-1], \n",
    "            dropout=self.m_config.dropout_rate\n",
    "        )\n",
    "\n",
    "        enc_params = self.encoder.init(\n",
    "            keys[0], inputs, is_training=True)\n",
    "        dec_params = self.decoder.init(\n",
    "            key[1], latent, is_training=True)\n",
    "        opt_state = self.opt.init((enc_params, dec_params))\n",
    "\n",
    "        # set prior for training latents\n",
    "        self.prior = jrand.normal(\n",
    "            keys[2], (self.m_config.enc_sizes[-1],)\n",
    "        )\n",
    "        return (enc_params, dec_params), opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def encode(self, enc_params, rng_key, x, is_training=True):\n",
    "        mu_z, var_z = self.encoder.apply(\n",
    "            enc_params, rng_key, x, is_training=is_training)\n",
    "        return mu_z, var_z\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, ))\n",
    "    def sample_latent(self, rng_key, mean, var):\n",
    "        eps = jax.random.normal(rng_key, var.shape)\n",
    "        return mean + eps * var\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4))\n",
    "    def decode(self, dec_params, rng_key, z, is_training=True):\n",
    "        reconstruct_x = self.decoder.apply(\n",
    "            dec_params, rng_key, z, is_training=is_training)\n",
    "        return reconstruct_x\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 5))\n",
    "    def sample_step(\n",
    "        self, rng_key, dec_params, mean, var, is_training=True\n",
    "    ):\n",
    "        z = self.sample_latent(rng_key, mean, var)\n",
    "        mu_x = self.decode(dec_params, rng_key, z, is_training=is_training)\n",
    "        return mu_x\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, 4, 5))\n",
    "    def sample(\n",
    "        self, params, rng_key, x, mc_samples, is_training=True\n",
    "    ): # Shape: (mc_samples, batch_size, input_size)\n",
    "        enc_params, dec_params = params\n",
    "        mean, var = self.encode(enc_params, rng_key, x, is_training=is_training)\n",
    "        keys = jax.random.split(rng_key, mc_samples)\n",
    "        \n",
    "        partial_sample_step = partial(\n",
    "            self.sample_step, dec_params=dec_params,\n",
    "            mean=mean, var=var, is_training=is_training\n",
    "        )\n",
    "        reconstruct_x = jax.vmap(partial_sample_step)(keys)\n",
    "        return (mean, var, reconstruct_x)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0, ))\n",
    "    def sample_prior(self, rng_key):\n",
    "        rng_key, key = jax.random.split(rng_key)\n",
    "        prior = jrand.normal(key, (self.m_config.enc_sizes[-1],))\n",
    "        return prior\n",
    "    \n",
    "    def compute_loss(self, params, rng_key, x, is_training=True):\n",
    "        @partial(jax.jit, static_argnums=(2, 3))\n",
    "        def reconstruct_loss(x: Array, cf: Array, cat_idx: int, cat_arr: List[int]):\n",
    "            cont_loss = optax.l2_loss(x[:, :cat_idx], cf[:, :cat_idx])\n",
    "            cat_loss = []\n",
    "            for i, cat in enumerate(cat_arr):\n",
    "                start_idx, end_idx = cat_idx + i * cat, cat_idx + (i + 1) * cat\n",
    "                cat_loss.append(\n",
    "                    optax.softmax_cross_entropy(\n",
    "                        cf[:, start_idx: end_idx], x[:, start_idx: end_idx]\n",
    "                    ).reshape(-1, 1)\n",
    "                )\n",
    "            cat_loss = jnp.concatenate(cat_loss, axis=-1)\n",
    "            return jnp.concatenate([cont_loss, cat_loss], axis=-1).sum(-1)\n",
    "        \n",
    "        keys = jax.random.split(rng_key, 2)\n",
    "        mean, var, reconstruct_x = self.sample(\n",
    "            params, keys[0], x, mc_samples=1, is_training=is_training\n",
    "        )\n",
    "        kl = kl_divergence(jnp.concatenate([mean, var], axis=-1), self.sample_prior(keys[1])).sum(-1)\n",
    "        rec = reconstruct_loss(reconstruct_x, x, **self._cat_info).sum(-1)\n",
    "        batchwise_loss = (rec + kl) / x.shape[0]\n",
    "        return batchwise_loss.mean()\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _training_step(\n",
    "        self, \n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState, \n",
    "        rng_key: random.PRNGKey, \n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        x, _ = batch\n",
    "        # y = self.pred_fn(x).round().reshape(-1, 1)\n",
    "        loss, grads = jax.value_and_grad(self.compute_loss)(\n",
    "            params, rng_key, x)\n",
    "        update_params, opt_state = grad_update(\n",
    "            grads, params, opt_state, self.opt)\n",
    "        return update_params, opt_state, loss\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        opt_state: optax.OptState,\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array]\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        params, opt_state, loss = self._training_step(params, opt_state, rng_key, batch)\n",
    "        self.log_dict({'train/loss': loss.item()})\n",
    "        return params, opt_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def validation_step(\n",
    "        self,\n",
    "        params: Tuple[hk.Params, hk.Params],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, optax.OptState]:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.7679496, 0.7679496], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optax.softmax_cross_entropy(\n",
    "    jnp.array([[0.1, 0.2, 0.7], [0.7, 0.2, 0.1]]),\n",
    "    jnp.array([[0., 0., 1.], [1., 0, 0]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7679, 0.7679])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.distributions import kl_divergence\n",
    "\n",
    "F.cross_entropy(\n",
    "    torch.tensor([[0.1, 0.2, 0.7], [0.7, 0.2, 0.1]]),\n",
    "    torch.tensor([[0., 0., 1.], [1., 0, 0]]),\n",
    "    reduction='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2.7332494, 2.7332494], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(\n",
    "    jnp.array([[0.1, 0.2, 0.7], [0.7, 0.2, 0.1]]),\n",
    "    jnp.array([[0., 0., 1.], [1., 0, 0]])\n",
    ").sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import kl_divergence, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000,  0.5000,  0.0918],\n",
       "        [24.5000,  0.5000,  0.8265]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(\n",
    "    Normal(torch.tensor([[0.1, 0.2, 0.7], [0.7, 0.2, 0.1]]), torch.tensor([0.1, 0.2, 0.7])),\n",
    "    Normal(torch.tensor([0., 0., 1.]), torch.tensor([0.1, 0.2, 0.7]))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
