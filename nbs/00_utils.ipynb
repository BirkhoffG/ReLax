{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Define utility funtions for `relax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "import nbdev\n",
    "from fastcore.basics import AttrDict\n",
    "from nbdev.showdoc import BasicMarkdownRenderer\n",
    "from inspect import isclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def validate_configs(\n",
    "    configs: dict | BaseParser,  # A configuration of the model/dataset.\n",
    "    config_cls: BaseParser,  # The desired configuration class.\n",
    ") -> BaseParser:\n",
    "    \"\"\"return a valid configuration object.\"\"\"\n",
    "\n",
    "    assert isclass(config_cls), f\"`config_cls` should be a class.\"\n",
    "    assert issubclass(config_cls, BaseParser), \\\n",
    "        f\"{config_cls} should be a subclass of `BaseParser`.\"\n",
    "    \n",
    "    if isinstance(configs, dict):\n",
    "        configs = config_cls(**configs)\n",
    "    if not isinstance(configs, config_cls):\n",
    "        raise TypeError(\n",
    "            f\"configs should be either a `dict` or an instance of {config_cls.__name__}.\")\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a configuration object (which inherent `BaseParser`) \n",
    "to manage training/model/data configurations.\n",
    "`validate_configs` ensures to return the designated configuration object.\n",
    "\n",
    "For example, we define a configuration object `LearningConfigs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningConfigs(BaseParser):\n",
    "    lr: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration can be `LearningConfigs`, or the raw data in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = dict(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validate_configs` will return a designated configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = validate_configs(configs_dict, LearningConfigs)\n",
    "assert type(configs) == LearningConfigs\n",
    "assert configs.lr == configs_dict['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from relax.module import PredictiveTrainingModuleConfigs\n",
    "from relax.methods.counternet import CounterNetTrainingModuleConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "configs = {\n",
    "    'lr': 0.1,\n",
    "    'sizes': [10, 5],\n",
    "    'lambda_1': 1.,\n",
    "    'lambda_2': 1.,\n",
    "    'lambda_3': 1.,\n",
    "}\n",
    "p_config = validate_configs(configs, PredictiveTrainingModuleConfigs)\n",
    "cf_config = validate_configs(configs, CounterNetTrainingModuleConfigs)\n",
    "\n",
    "assert isinstance(p_config, PredictiveTrainingModuleConfigs)\n",
    "assert isinstance(cf_config, CounterNetTrainingModuleConfigs)\n",
    "\n",
    "assert not isinstance(p_config, dict)\n",
    "assert not isinstance(cf_config, dict)\n",
    "\n",
    "p_config = validate_configs(p_config, PredictiveTrainingModuleConfigs)\n",
    "cf_config = validate_configs(cf_config, CounterNetTrainingModuleConfigs)\n",
    "\n",
    "assert isinstance(p_config, PredictiveTrainingModuleConfigs)\n",
    "assert isinstance(cf_config, CounterNetTrainingModuleConfigs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _docment_parser(parser: BaseParser):\n",
    "    p = parser.schema()['properties']\n",
    "    anno = parser.__annotations__\n",
    "    d = { \n",
    "        k: {\n",
    "            'anno': anno[k],\n",
    "            'default': v['default'] if 'default' in v else inspect._empty,\n",
    "            'docment': v['description'] if 'description' in v else inspect._empty,\n",
    "        } for k, v in p.items()\n",
    "    }\n",
    "\n",
    "    d = AttrDict(d)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ParserMarkdownRenderer(BasicMarkdownRenderer):\n",
    "        def __init__(self, sym, name: str | None = None, title_level: int = 3):\n",
    "            super().__init__(sym, name, title_level)\n",
    "            self.dm.dm = _docment_parser(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def show_doc(\n",
    "    sym # Symbol to document\n",
    "):\n",
    "    \"\"\"Same functionality as [nbdev.show_doc](configurator), \n",
    "    but provide additional support for `BaseParser`.\"\"\"\n",
    "    if inspect.isclass(sym) and issubclass(sym, BaseParser):\n",
    "        return nbdev.show_doc(sym, ParserMarkdownRenderer)\n",
    "    else:\n",
    "        return nbdev.show_doc(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/utils.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### show_doc\n",
       "\n",
       ">      show_doc (sym)\n",
       "\n",
       "Same functionality as [nbdev.show_doc](configurator), \n",
       "but provide additional support for `BaseParser`.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| sym | Symbol to document |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/relax/utils.py#L49){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### show_doc\n",
       "\n",
       ">      show_doc (sym)\n",
       "\n",
       "Same functionality as [nbdev.show_doc](configurator), \n",
       "but provide additional support for `BaseParser`.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| sym | Symbol to document |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(show_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cat_normalize(\n",
    "    cf: jnp.ndarray,  # Unnormalized counterfactual explanations `[n_samples, n_features]`\n",
    "    cat_arrays: List[List[str]],  # A list of a list of each categorical feature name\n",
    "    cat_idx: int,  # Index that starts categorical features\n",
    "    hard: bool = False,  # If `True`, return one-hot vectors; If `False`, return probability normalized via softmax\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Ensure generated counterfactual explanations to respect one-hot encoding constraints.\"\"\"\n",
    "    cf_cont = cf[:, :cat_idx]\n",
    "    normalized_cf = [cf_cont]\n",
    "\n",
    "    for col in cat_arrays:\n",
    "        cat_end_idx = cat_idx + len(col)\n",
    "        _cf_cat = cf[:, cat_idx:cat_end_idx]\n",
    "\n",
    "        cf_cat = lax.cond(\n",
    "            hard,\n",
    "            true_fun=lambda x: jax.nn.one_hot(jnp.argmax(x, axis=-1), len(col)),\n",
    "            false_fun=lambda x: jax.nn.softmax(x, axis=-1),\n",
    "            operand=_cf_cat,\n",
    "        )\n",
    "\n",
    "        cat_idx = cat_end_idx\n",
    "        normalized_cf.append(cf_cat)\n",
    "    return jnp.concatenate(normalized_cf, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tabular data point is encoded as \n",
    "$$x = [\\underbrace{x_{0}, x_{1}, ..., x_{m}}_{\\text{cont features}}, \n",
    "\\underbrace{x_{m+1}^{c=1},..., x_{m+p}^{c=1}}_{\\text{cat feature (1)}}, ..., \n",
    "\\underbrace{x_{k-q}^{c=i},..., x_{k}^{^{c=i}}}_{\\text{cat feature (i)}}]$$\n",
    "\n",
    "`cat_normalize` ensures the generated `cf` that satisfy the categorical constraints, \n",
    "i.e., $\\sum_j x^{c=i}_j=1, x^{c=i}_j > 0, \\forall c=[1, ..., i]$.\n",
    "\n",
    "`cat_idx` is the index of the first categorical feature. \n",
    "In the above example, `cat_idx` is `m+1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's define a valid input data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1., .9, 'dog', 'gray'],\n",
    "    [.3, .3, 'cat', 'gray'],\n",
    "    [.7, .1, 'fish', 'red'],\n",
    "    [1., .6, 'dog', 'gray'],\n",
    "    [.1, .2, 'fish', 'yellow']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the categorical features via the `OneHotEncoder` in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx = 2\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "x_cat = ohe.fit_transform(x[:, cat_idx:])\n",
    "x_cont = x[:, :cat_idx].astype(float)\n",
    "x_transformed = np.concatenate(\n",
    "    (x_cont, x_cat), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `hard=True`, the categorical features are in one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.87933743, -0.6561204 ,  0.        ,  1.        ,\n",
       "               0.        ,  1.        ,  0.        ,  0.        ]],            dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfs = np.random.randn(*x_transformed.shape)\n",
    "cfs = cat_normalize(cfs, ohe.categories_, \n",
    "    cat_idx=cat_idx, hard=True)\n",
    "cfs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `hard=False`, the categorical features are normalized via softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = np.random.randn(*x_transformed.shape)\n",
    "cfs = cat_normalize(cfs, ohe.categories_, \n",
    "    cat_idx=cat_idx, hard=False)\n",
    "n_cat_feats = len(ohe.categories_)\n",
    "\n",
    "assert (cfs[:, cat_idx:].sum(axis=1) - n_cat_feats * jnp.ones(len(cfs))).sum() < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from relax.data import TabularDataModule\n",
    "from relax.utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "data_configs = load_json('assets/configs/data_configs/adult.json')['data_configs']\n",
    "dm = TabularDataModule(data_configs)\n",
    "cf = random.normal(random.PRNGKey(0), (10, 29))\n",
    "cf_norm = cat_normalize(cf, dm.cat_encoder.categories_, dm.cat_idx, True)\n",
    "cat_feats = len(dm.cat_encoder.categories_)\n",
    "\n",
    "assert all(jnp.sum(cf_norm[:, dm.cat_idx:], axis=-1) == cat_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_model(\n",
    "    m_configs: Dict[str, Any], model: hk.Module  # model configs\n",
    ") -> hk.Transformed:\n",
    "    # example:\n",
    "    # net = make_model(PredictiveModel)\n",
    "    # params = net.init(...)\n",
    "    def model_fn(x, is_training: bool = True):\n",
    "        return model(m_configs)(x, is_training)\n",
    "\n",
    "    return hk.transform(model_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_hk_module(\n",
    "    module: hk.Module, # haiku module \n",
    "    *args, # haiku module arguments\n",
    "    **kargs, # haiku module arguments\n",
    ") -> hk.Transformed:\n",
    "\n",
    "    def model_fn(x, is_training: bool = True):\n",
    "        return module(*args, **kargs)(x, is_training)\n",
    "    \n",
    "    return hk.transform(model_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def init_net_opt(\n",
    "    net: hk.Transformed,\n",
    "    opt: optax.GradientTransformation,\n",
    "    X: jnp.DeviceArray,\n",
    "    key: random.PRNGKey,\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    X = device_put(X)\n",
    "    params = net.init(key, X, is_training=True)\n",
    "    opt_state = opt.init(params)\n",
    "    return params, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def grad_update(\n",
    "    grads: Dict[str, jnp.ndarray],\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    opt: optax.GradientTransformation,\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    upt_params = optax.apply_updates(params, updates)\n",
    "    return upt_params, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_cat_info(method):\n",
    "    def inner(cf_module, *args, **kwargs):\n",
    "        warning_msg = f\"\"\"This CFExplanationModule might not be updated with categorical information.\n",
    "You should try `{cf_module.name}.update_cat_info(dm)` before generating cfs.\n",
    "        \"\"\"\n",
    "        if cf_module.cat_idx == 0 and cf_module.cat_arrays == []:\n",
    "            warnings.warn(warning_msg, RuntimeWarning)\n",
    "        return method(cf_module, *args, **kwargs)\n",
    "\n",
    "    return inner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_json(f_name: str) -> Dict[str, Any]:  # file name\n",
    "    with open(f_name) as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def binary_cross_entropy(\n",
    "    preds: jnp.DeviceArray, # The predicted values\n",
    "    labels: jnp.DeviceArray # The ground-truth labels\n",
    ") -> jnp.DeviceArray: # Loss value\n",
    "    \"\"\"Per-sample binary cross-entropy loss function.\"\"\"\n",
    "\n",
    "    # Clip the predictions to avoid NaNs in the log\n",
    "    preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    # Compute the binary cross-entropy\n",
    "    loss = -labels * jnp.log(preds) - (1 - labels) * jnp.log(1 - preds)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sigmoid(x):\n",
    "    # https://stackoverflow.com/a/68293931\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def accuracy(y_true: jnp.ndarray, y_pred: jnp.ndarray) -> jnp.DeviceArray:\n",
    "    y_true, y_pred = map(jnp.round, (y_true, y_pred))\n",
    "    return jnp.mean(jnp.equal(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dist(x: jnp.ndarray, cf: jnp.ndarray, ord: int = 2) -> jnp.DeviceArray:\n",
    "    dist = jnp.linalg.norm(x - cf, ord=ord, axis=-1, keepdims=True)\n",
    "    return jnp.mean(vmap(jnp.sum)(dist))\n",
    "\n",
    "\n",
    "def proximity(x: jnp.ndarray, cf: jnp.ndarray) -> jnp.DeviceArray:\n",
    "    return dist(x, cf, ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "m = jnp.array([\n",
    "    [1., 2., 3., 1.],\n",
    "    [1., -1., 4., 1.],\n",
    "])\n",
    "n = jnp.array([\n",
    "    [0., -1., 3., 1.],\n",
    "    [1., 2., 4., 1.],\n",
    "])\n",
    "assert proximity(m, n).item() == 3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@dataclass\n",
    "class Config:\n",
    "    rng_reserve_size: int\n",
    "    global_seed: int\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls) -> Config:\n",
    "        return cls(rng_reserve_size=1, global_seed=42)\n",
    "\n",
    "main_config = Config.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_config() -> Config: \n",
    "    return main_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
