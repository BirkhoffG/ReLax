{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.counternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from cfnet.import_essentials import *\n",
    "from cfnet.module import MLP, BaseTrainingModule\n",
    "from cfnet.methods.base import BaseCFModule, ParametricCFModule\n",
    "from cfnet.utils import (\n",
    "    validate_configs,\n",
    "    sigmoid,\n",
    "    cat_normalize,\n",
    "    accuracy,\n",
    "    proximity,\n",
    "    make_model,\n",
    "    init_net_opt,\n",
    "    grad_update,\n",
    ")\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CounterNetModelConfigs(BaseParser):\n",
    "    \"\"\"Configurator of `CounterNetModel`.\"\"\"\n",
    "\n",
    "    enc_sizes: List[int]\n",
    "    dec_sizes: List[int]\n",
    "    exp_sizes: List[int]\n",
    "    dropout_rate: float = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "class CounterNetModel(hk.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        m_config: Dict[\n",
    "            str, Any\n",
    "        ],  # Model configs which contain configs in `CounterNetModelConfigs`.\n",
    "        name: Optional[str] = None,  # Name of the module.\n",
    "    ):\n",
    "        \"\"\"CounterNet model architecture.\"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.configs = validate_configs(m_config, CounterNetModelConfigs)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        # encoder\n",
    "        z = MLP(self.configs.enc_sizes, self.configs.dropout_rate, name=\"Encoder\")(\n",
    "            x, is_training\n",
    "        )\n",
    "\n",
    "        # prediction\n",
    "        pred = MLP(self.configs.dec_sizes, self.configs.dropout_rate, name=\"Predictor\")(\n",
    "            z, is_training\n",
    "        )\n",
    "        y_hat = hk.Linear(1, name=\"Predictor\")(pred)\n",
    "        y_hat = sigmoid(y_hat)\n",
    "\n",
    "        # explain\n",
    "        z_exp = jnp.concatenate((z, pred), axis=-1)\n",
    "        cf = MLP(self.configs.exp_sizes, self.configs.dropout_rate, name=\"Explainer\")(\n",
    "            z_exp, is_training\n",
    "        )\n",
    "        cf = hk.Linear(input_shape, name=\"Explainer\")(cf)\n",
    "        return y_hat, cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/cfnet/nets.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### CounterNetModelConfigs\n",
       "\n",
       ">      CounterNetModelConfigs (enc_sizes:List[int], dec_sizes:List[int],\n",
       ">                              exp_sizes:List[int], dropout_rate:float=0.3)\n",
       "\n",
       "Configurator of `CounterNetModel`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/birkhoffg/cfnet/tree/master/blob/master/cfnet/nets.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### CounterNetModelConfigs\n",
       "\n",
       ">      CounterNetModelConfigs (enc_sizes:List[int], dec_sizes:List[int],\n",
       ">                              exp_sizes:List[int], dropout_rate:float=0.3)\n",
       "\n",
       "Configurator of `CounterNetModel`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(CounterNetModelConfigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Training Module\n",
    "\n",
    "Define the `CounterNetTrainingModule` for training `CounterNetModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def partition_trainable_params(params: hk.Params, trainable_name: str):\n",
    "    trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "        lambda m, n, p: trainable_name in m, params\n",
    "    )\n",
    "    return trainable_params, non_trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def project_immutable_features(x, cf: jnp.DeviceArray, imutable_idx_list: List[int]):\n",
    "    cf = cf.at[:, imutable_idx_list].set(x[:, imutable_idx_list])\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetTrainingModuleConfigs(BaseParser):\n",
    "    lr: float = 0.003\n",
    "    lambda_1: float = 1.0\n",
    "    lambda_2: float = 0.2\n",
    "    lambda_3: float = 0.1\n",
    "    use_immutable: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetTrainingModule(BaseTrainingModule):\n",
    "    def __init__(self, m_configs: Dict[str, Any]):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.net = make_model(m_configs, CounterNetModel)\n",
    "        self.configs = validate_configs(m_configs, CounterNetTrainingModuleConfigs)\n",
    "        # self.configs = CounterNetTrainingModuleConfigs(**m_configs)\n",
    "        self.opt_1 = optax.adam(learning_rate=self.configs.lr)\n",
    "        self.opt_2 = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    def init_net_opt(self, data_module, key):\n",
    "        self.update_cat_info(data_module)\n",
    "        # manually init multiple opts\n",
    "        params, opt_1_state = init_net_opt(\n",
    "            self.net, self.opt_1, X=data_module.get_sample_X(), key=key\n",
    "        )\n",
    "        trainable_params, _ = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        opt_2_state = self.opt_2.init(trainable_params)\n",
    "        return params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        # first forward to get y_pred and normalized cf\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        # cf = cf_res + x\n",
    "        cf = cat_normalize(cf, self.cat_arrays, self.cat_idx, hard=not is_training)\n",
    "        # project immutable features\n",
    "        if self.configs.use_immutable:\n",
    "            cf = project_immutable_features(x, cf, self.imutable_idx_list)\n",
    "        # second forward to calulate cf_y\n",
    "        cf_y, _ = self.net.apply(params, rng_key, cf, is_training=is_training)\n",
    "        return y_pred, cf, cf_y\n",
    "\n",
    "    def predict(self, params, rng_key, x):\n",
    "        y_pred, _ = self.net.apply(params, rng_key, x, is_training=False)\n",
    "        return y_pred\n",
    "\n",
    "    def generate_cfs(self, X: chex.ArrayBatched, params, rng_key) -> chex.ArrayBatched:\n",
    "        y_pred, cfs = self.net.apply(params, rng_key, X, is_training=False)\n",
    "        # cfs = cfs + X\n",
    "        cfs = cat_normalize(cfs, self.cat_arrays, self.cat_idx, hard=True)\n",
    "        if self.configs.use_immutable:\n",
    "            cfs = project_immutable_features(X, cfs, self.imutable_idx_list)\n",
    "        return cfs\n",
    "\n",
    "    def loss_fn_1(self, y_pred, y):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(y_pred, y))\n",
    "\n",
    "    def loss_fn_2(self, cf_y, y_prime):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(cf_y, y_prime))\n",
    "\n",
    "    def loss_fn_3(self, x, cf):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(x, cf))\n",
    "\n",
    "    def pred_loss_fn(self, params, rng_key, batch, is_training: bool = True):\n",
    "        x, y = batch\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        return self.configs.lambda_1 * self.loss_fn_1(y_pred, y)\n",
    "\n",
    "    def exp_loss_fn(\n",
    "        self,\n",
    "        trainable_params,\n",
    "        non_trainable_params,\n",
    "        rng_key,\n",
    "        batch,\n",
    "        is_training: bool = True,\n",
    "    ):\n",
    "        # merge trainable and non_trainable params\n",
    "        params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        loss_2, loss_3 = self.loss_fn_2(cf_y, y_prime), self.loss_fn_3(x, cf)\n",
    "        return self.configs.lambda_2 * loss_2 + self.configs.lambda_3 * loss_3\n",
    "\n",
    "    def _predictor_step(self, params, opt_state, rng_key, batch):\n",
    "        grads = jax.grad(self.pred_loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt_1)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    def _explainer_step(self, params, opt_state, rng_key, batch):\n",
    "        trainable_params, non_trainable_params = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        grads = jax.grad(self.exp_loss_fn)(\n",
    "            trainable_params, non_trainable_params, rng_key, batch\n",
    "        )\n",
    "        upt_trainable_params, opt_state = grad_update(\n",
    "            grads, trainable_params, opt_state, self.opt_2\n",
    "        )\n",
    "        upt_params = hk.data_structures.merge(\n",
    "            upt_trainable_params, non_trainable_params\n",
    "        )\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.GradientTransformation, optax.GradientTransformation],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ):\n",
    "        opt_1_state, opt_2_state = opts_state\n",
    "        params, opt_1_state = self._predictor_step(params, opt_1_state, rng_key, batch)\n",
    "        upt_params, opt_2_state = self._explainer_step(\n",
    "            params, opt_2_state, rng_key, batch\n",
    "        )\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def _training_step_logs(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        logs = {\n",
    "            \"train/train_loss_1\": loss_1.item(),\n",
    "            \"train/train_loss_2\": loss_2.item(),\n",
    "            \"train/train_loss_3\": loss_3.item(),\n",
    "        }\n",
    "        return logs\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.OptState, optax.OptState],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, Tuple[optax.OptState, optax.OptState]]:\n",
    "        upt_params, (opt_1_state, opt_2_state) = self._training_step(\n",
    "            params, opts_state, rng_key, batch\n",
    "        )\n",
    "\n",
    "        logs = self._training_step_logs(upt_params, rng_key, batch)\n",
    "        self.log_dict(logs)\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        loss_1, loss_2, loss_3 = map(np.asarray, (loss_1, loss_2, loss_3))\n",
    "        logs = {\n",
    "            \"val/accuracy\": accuracy(y, y_pred),\n",
    "            \"val/validity\": accuracy(cf_y, y_prime),\n",
    "            \"val/proximity\": proximity(x, cf),\n",
    "            \"val/val_loss_1\": loss_1,\n",
    "            \"val/val_loss_2\": loss_2,\n",
    "            \"val/val_loss_3\": loss_3,\n",
    "            \"val/val_loss\": loss_1 + loss_2 + loss_3,\n",
    "        }\n",
    "        self.log_dict(logs)\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Explanation Module\n",
    "\n",
    "A high-level class inherents `BaseCFExplanationModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "DEFAULT_COUNTERNET_CONFIGS = {\n",
    "    \"enc_sizes\": [50,10],\n",
    "    \"dec_sizes\": [10],\n",
    "    \"exp_sizes\": [50, 50],\n",
    "    \"dropout_rate\": 0.3,    \n",
    "    'lr': 0.003,\n",
    "    \"lambda_1\": 1.0,\n",
    "    \"lambda_3\": 0.1,\n",
    "    \"lambda_2\": 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CounterNet(BaseCFModule, ParametricCFModule):\n",
    "    params: hk.Params = None\n",
    "    module: CounterNetTrainingModule\n",
    "\n",
    "    def __init__(self, m_configs: dict | CounterNetTrainingModuleConfigs = None):\n",
    "        if m_configs is None:\n",
    "            m_configs = CounterNetTrainingModuleConfigs()\n",
    "        self.module = CounterNetTrainingModule(m_configs)\n",
    "\n",
    "    def _is_module_trained(self):\n",
    "        return not (self.params is None)\n",
    "    \n",
    "    def train(self, t_configs=None):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('nbdev2')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
